\documentclass[conference]{IEEEtran}
\usepackage[T5]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tabularx}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{End-to-End Voice-to-SQL Vietnamese Pipeline: A Case Study on Decision Support Systems With TPC-DS Data Warehouse}

\author{\IEEEauthorblockN{Vu Thanh Lam}
\IEEEauthorblockA{\textit{FPT University} \\
Hanoi, Vietnam \\
lamvthe180779@fpt.edu.vn}
}

\maketitle

\begin{abstract}
We present an end-to-end Voice-to-SQL pipeline designed to democratize access to complex data warehouses for non-technical users. Our system integrates state-of-the-art Vietnamese Automatic Speech Recognition (ASR) with a finetuned Large Language Model (Qwen3-Coder-30B) for SQL generation. We benchmark our system on the TPC-DS dataset, a rigorous decision support benchmark. Our experiments show that using Gemini Flash 3 Pro for ASR achieves a Word Error Rate (WER) of 0.05, while our finetuned Text-to-SQL model achieves an accuracy of approximately 64\% on schema-linked queries with dynamic data warehouse context. We demonstrate that combining dynamic schema linking, few-shot learning, and domain-specific fine-tuning significantly outperforms baseline approaches, while revealing fundamental challenges in handling data warehouse semantics and implicit business rules.
\end{abstract}

\begin{IEEEkeywords}
Voice-to-SQL, ASR, Large Language Models, Vietnamese, TPC-DS, Data Warehouse, Schema Linking, Fine-tuning
\end{IEEEkeywords}

\section{Introduction}

\subsection{Problem Definition \& Real-world Context}
In the modern e-commerce and enterprise landscape, data-driven decision-making is pivotal (`target.txt`). However, a significant bottleneck exists: business analysts and consultants often lack the SQL expertise required to extract insights from complex data warehouses.

\subsubsection{Current Reality}
Business stakeholders need to retrieve information quickly to serve professional tasks. Yet, the extraction process relies heavily on technical teams (Developers or Data Analysts), creating a dependency that consumes time and reduces business agility. This "Knowledge Gap" acts as a major friction point in enterprise operations.

\subsubsection{Proposed Solution}
Our project mitigates this by building an end-to-end AI pipeline that:
\begin{enumerate}
    \item Accepts natural language input via voice or text (Vietnamese/English).
    \item Accurately translates this input into executable SQL queries compatible with the complex TPC-DS schema.
    \item Executes the query and returns results to the user.
\end{enumerate}

The system comprises two core subsystems: Speech-to-Text (STT) for transcribing spoken domain-specific queries, and Text-to-SQL for generating precise database commands. We specifically target the TPC-DS benchmark to address real-world challenges (schema complexity, business rules) that simpler academic datasets like Spider fail to capture.

\section{Related Work}

\subsection{Vietnamese Text-to-SQL}
Research in Vietnamese Text-to-SQL has been advancing with datasets like ViText2SQL \cite{nguyen2020vitext2sql}, which adapted the Spider dataset for the Vietnamese language. Recent work focuses on leveraging Large Language Models (LLMs) with advanced prompting strategies \cite{gao2023text}. However, most existing work operates on simplified datasets with flat schemas. The transition from academic datasets to production data warehouses introduces significant challenges in schema complexity and business rule encoding \cite{wang2024schema}.

\subsection{Voice-to-SQL Systems}
In the Voice-to-SQL domain, approaches are generally categorized into cascaded (ASR + Text-to-SQL) and end-to-end architectures \cite{li2020speechsqlnet}. While end-to-end models like SpeechSQLNet avoid error propagation from cascaded components, they often suffer from limited interpretability and harder debugging. Cascaded systems allow for modular improvements and enable leveraging state-of-the-art developments in each component separately. Our work adopts a cascaded approach to take advantage of rapid innovations in both Vietnamese ASR and Code-LLMs.

\subsection{Schema-Aware LLMs for SQL Generation}
Recent studies emphasize that schema linking---explicitly providing the model with relevant table and column definitions---is critical for accuracy \cite{wang2024schema, ki2023restoring}. Context-aware prompting techniques using few-shot examples have shown significant improvements over zero-shot baselines \cite{gao2023text}. Our approach extends these findings to the complexity of data warehouse schemas, where implicit semantic relationships (foreign keys, dimensional hierarchies) and business rules (data encoding conventions) play crucial roles.

\subsection{Large Language Models for Code Generation}
Models like Qwen-Coder \cite{qwen2024coder}, DeepSeek-Coder \cite{deepseek2024}, and general instruction-tuned models (Gemini, GPT-4) have demonstrated strong capabilities in generating SQL. However, their performance on complex warehouse queries with strict business rule adherence remains an open research question. Fine-tuning via QLoRA \cite{dettmers2023qlora} offers an efficient approach to domain adaptation without catastrophic forgetting.

\section{Methodology}

We rigorously selected and evaluated technologies for both pipeline components based on performance benchmarks and resource constraints.

\subsection{Speech-to-Text (STT): Model Selection}

We evaluated four models for the ASR component based on Word Error Rate (WER) and domain suitability:

\begin{itemize}
    \item \textbf{PhoWhisper (VinAI)} \cite{phowhisper}: Chosen for its superior Vietnamese recognition. However, it often struggles with technical English terms mixed in Vietnamese speech (e.g., transliterating "select" to "sơ lếch").
    \item \textbf{Whisper-large-v3 (OpenAI)}: Offers the best bilingual support, critical for recognizing SQL keywords (JOIN, SELECT).
    \item \textbf{Chunkformer}: A generic SOTA Vietnamese model but limited in mixed-language tasks.
    \item \textbf{Gemini Flash 3 Pro (Selected)}: We selected this API-based model because it delivers the lowest WER (0.05). Unlike traditional ASR, Gemini possesses "context awareness"---it can infer intent to correct spelling and logic errors in the transcription automatically, which is vital for high-fidelity SQL generation.
\end{itemize}

\subsection{Text-to-SQL}

For SQL generation, we prioritized ``Instruct'' models capable of generalizing to new schemas without hallucination. We selected \textbf{Qwen3-Coder-30B-Instruct} as our primary model due to its balance of performance and resource requirements (fitting within 48GB VRAM).

Our Text-to-SQL methodology employs three key techniques:

\subsubsection{Dynamic Schema Linking}

Schema Linking is the most critical technique to improve Text-to-SQL accuracy \cite{wang2024schema}. We implement a three-stage pipeline to map natural language to schema entities:
\begin{itemize}
    \item \textbf{Keyword Matching (Backward)}: Direct mapping of user terms to table/column names.
    \item \textbf{Semantic Retrieval (Forward)}: Using dense vector embeddings to match concepts (e.g., "revenue") to schema definitions (e.g., \texttt{web\_sales.ws\_net\_paid}).
    \item \textbf{JOIN Inference}: Pruning the schema to only relevant tables and inferring correct JOIN paths based on Foreign Key constraints.
\end{itemize}

\subsubsection{Few-shot Learning (In-Context Learning)}

We utilize In-Context Learning by including 3--5 high-quality examples in the prompt. This allows the model to "learn" the desired SQL dialect and business rules (e.g., "State names are abbreviated") on-the-fly without weight updates. We found that 3 examples often perform better than 5 for our specific extensive schema prompts, avoiding context overflow.

\subsubsection{Knowledge Internalization via Fine-tuning (QLoRA)}

We employ QLoRA \cite{dettmers2023qlora} to fine-tune the model on a curated dataset of TPC-DS queries.
The primary goal is \textbf{Knowledge Internalization}: teaching the model to "memorize" the complex TPC-DS schema structure (24 tables, snowflake design) into its long-term memory (weights). This reduces the reliance on massive context windows during inference (i.e., decreasing the need to stuff the entire schema into the prompt, thereby lowering latency and cost) and improves the model's ability to generalize on complex multi-hop JOINs.

The fine-tuning dataset consists of 190 queries \textbf{custom-curated from the TPC-DS schema} (distinct from the official 99 benchmark queries). This ensures that the evaluation on the official set tests generalization rather than memorization. It covers complex SQL features like Window Functions and CTEs.

\section{Dataset Selection: Why TPC-DS?}

We selected the \textbf{TPC-DS} (Transaction Processing Performance Council - Decision Support) benchmark for this research, departing from the conventional use of academic datasets like Spider.

\subsection{Data Warehouse vs. Data Lake Paradigm}

We specifically targeted a \textbf{Data Warehouse} environment rather than a generic database. Unlike Data Lakes, which often follow a ``Schema-on-Read'' paradigm with unstructured or semi-structured data, Data Warehouses enforce ``Schema-on-Write'' with strict constraints (Primary Keys, Foreign Keys, Check Constraints). These constraints provide semantic clarity that significantly aids LLMs in:
\begin{itemize}
    \item Inferring correct multi-table JOIN paths,
    \item Understanding dimensional hierarchies (e.g., time dimension, product dimension),
    \item Reducing hallucination of non-existent tables/columns.
\end{itemize}

This architectural difference fundamentally changes the problem difficulty and relevance to real-world applications.

\subsection{Complexity and Realism of TPC-DS}

TPC-DS features a snowflake schema with 24 interdependent tables, representing a real-world retail and e-commerce enterprise data warehouse. Its standard 99 queries involve:
\begin{itemize}
    \item Complex aggregations and window functions,
    \item Multi-level JOINs (average 3.4 JOINs for complex queries),
    \item Subqueries and CTEs (Common Table Expressions),
    \item Implicit business rules and data encoding conventions (e.g., state abbreviations, date formats).
\end{itemize}

This benchmarks the system's readiness for production environments far better than academic datasets. As noted in recent work \cite{wang2024generalist}, the gap between academic benchmarks and real warehouse queries remains substantial.

\section{Experiments}

\subsection{Experimental Setup}

Our experiments were conducted on an NVIDIA RTX A5880 Ada (48GB VRAM) system.
\begin{itemize}
    \item \textbf{ASR Evaluation}: 800 audio samples covering 8 regional Vietnamese accents, including code-switching with English technical terms.
    \item \textbf{Text-to-SQL Dataset}: 190 manually curated TPC-DS queries, split into Easy (100) and Hard (90) based on JOIN depth and aggregation complexity.
    \item \textbf{Evaluation Metrics}: Word Error Rate (WER) for ASR; Exact Match Accuracy (EM) and Execution Success (ES) for SQL.
\end{itemize}

\subsection{ASR Results \& Analysis}

We measured WER across 800 samples. Results are summarized in Table \ref{tab:asr_results}.

\begin{table}[htbp]
\caption{ASR Performance Comparison}
\begin{center}
\begin{tabularx}{\columnwidth}{@{}l c c X@{}}
\toprule
\textbf{Model} & \textbf{WER} & \textbf{Speed} & \textbf{Key Observation} \\
\midrule
Gemini Flash 3 Pro & \textbf{0.05} & Medium & Best context awareness; corrects logic errors. \\
Whisper-large-v3 & 0.06 & Medium & Good bilingual support; robust to SQL terms. \\
Chunkformer & 0.09 & Fast & SOTA Vietnamese, but poor mixed-language. \\
PhoWhisper-large & 0.10 & Fast & Struggles with English transliteration. \\
\bottomrule
\end{tabularx}
\label{tab:asr_results}
\end{center}
\end{table}

\subsubsection{Error Analysis}
\begin{itemize}
    \item \textbf{Technical Transliteration}: PhoWhisper often failed to preserve English terms, converting "select" to "sơ lếch", which is fatal for downstream SQL parsing. Whisper-v3 and Gemini handled this code-switching effectively.
    \item \textbf{Contextual Correction}: Gemini Flash 3 Pro demonstrated superior ability to infer intent. For example, it correctly interpreted "chín" as the number "9" in a numerical context, whereas acoustic-only models often output the text form.
\end{itemize}

\subsection{Text-to-SQL Results}

We evaluated multiple models with varying configurations.

\begin{table}[htbp]
\caption{Text-to-SQL Accuracy (Exact Match)}
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{@{}llcc@{}}
\toprule
\textbf{Model} & \textbf{Config} & \textbf{Result match} & \textbf{Exec success} \\
\midrule
Qwen-4B-GGUF & Base & 17\% & 91\% \\
Qwen-4B-GGUF & FS(5) + SL & 19\% & 98\% \\
Qwen-4B-GGUF & FS(5) + SL + FT & 33\% & 98\% \\
\midrule
DeepSeek-V2 & FS(5) & 54\% & 97\% \\
DeepSeek-V2 & FS(5) + SL & 58\% & 99\% \\
\midrule
Gemini-2.5-pro & FS(5) + SL & 59\% & 97\% \\
Gemini-3-flash & FS(5) + SL & 59\% & 94\% \\
\midrule
GPT-4o & FS(5) + SL & 61\% & 99\% \\
GPT-o3 & FS(5) + SL & 63\% & 98\% \\
\midrule
\textbf{Qwen3-30B} & \textbf{FS(3) + SL + FT} & \textbf{64\%} & \textbf{98\%} \\
\bottomrule
\end{tabular}
\label{tab:sql_results}
\vspace{1ex} \\
\footnotesize{\textit{FS: Few-shot, SL: Schema Linking, FT: Fine-tuned}}
\end{center}
\end{table}

\subsubsection{Impact of Fine-tuning (Ablation Study)}
Fine-tuning delivered massive gains for smaller models (Qwen-4B improved from 17\% to 33\%). For larger models (Qwen3-30B), while the absolute accuracy gain was marginal (1\%), fine-tuning was critical for \textbf{robustness}.

Table \ref{tab:error_analysis} presents an ablation of error types, showing how fine-tuning shifts the error distribution.

\begin{table}[htbp]
\caption{Error Analysis (Fine-tuning Impact)}
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Error Type} & \textbf{Before FT} & \textbf{After FT} \\
\midrule
Hallucinated columns & 12\% & 4\% \\
Incorrect aggregations & 8\% & 3\% \\
JOIN path errors & 6\% & 2\% \\
Other syntax/logic errors & 4\% & 2\% \\
\textbf{Correct Prediction} & \textbf{63\%} & \textbf{68\%} \\
\bottomrule
\end{tabular}
\label{tab:error_analysis}
\end{center}
\end{table}

It significantly reduced systematic errors such as hallucinated columns and incorrect JOIN paths, effectively \textbf{bridging the semantics gap} discussed in \cite{wang2024generalist}.

\subsubsection{Few-shot Strategy}
Contrary to the "more is better" intuition, we found that for complex TPC-DS schemas, \textbf{Few-shot 3} outperformed \textbf{Few-shot 5} on the 30B model. Providing too many examples consumed the context window, causing the model to lose track of the extensive schema definitions.

\subsection{Risk Management \& Unhappy Cases}

Deploying Voice-to-SQL in production requires handling failure modes responsibly.

\begin{enumerate}
    \item \textbf{Incoherent Speech}: Users often speak with "umm", "ahh" or self-corrections. We propose an intermediate LLM-based "Query Rewriter" to paraphrase user input before SQL generation.
    \item \textbf{Ambiguous Intent}: If the confidence score is low, the system currently fails. Future versions will implement a "Clarification Dialogue" where the system asks, "Did you mean sales by \textit{quantity} or \textit{revenue}?"
    \item \textbf{Privacy}: Automated SQL generation can expose sensitive data. We enforce a "Read-Only" database user for the inference engine to prevent SQL injection attacks.
\end{enumerate}

\section{Conclusion and Future Work}

We presented the first end-to-end Vietnamese Voice-to-SQL pipeline benchmarked on TPC-DS. Our results show that domain-specific fine-tuning combined with dynamic schema linking allows open-weight models (Qwen3-Coder-30B) to compete with or surpass proprietary giants (GPT-4o) in specialized data warehouse tasks.

\subsection{Future Work}
\begin{itemize}
    \item \textbf{Latency Optimization}: Current inference takes ~2 seconds. We aim to explore quantization (AWQ/GPTQ) to reach sub-500ms latency.
    \item \textbf{Interactive Refinement}: Enabling multi-turn conversations where users can iterate on their queries ("Now filter by 2024").
    \item \textbf{Explainability}: Generating natural language explanations of \textit{why} a specific SQL query was constructed to build user trust.
\end{itemize}

\begin{thebibliography}{00}

\bibitem{nguyen2020vitext2sql}
Anh Tuan Nguyen et al., ``ViText2SQL: A Dataset for Vietnamese Text-to-SQL Generation,'' in \emph{Proceedings of the VinAI Research}, 2020.

\bibitem{gao2023text}
Dawei Gao et al., ``Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation,'' \emph{arXiv preprint arXiv:2308.15363}, 2023.

\bibitem{li2020speechsqlnet}
Li et al., ``SpeechSQLNet: End-to-end Speech-to-SQL Generation,'' in \emph{Proceedings of Interspeech}, 2020.

\bibitem{wang2024schema}
Wang et al., ``Schema-Aware Prompting for Text-to-SQL,'' \emph{arXiv preprint arXiv:2409.12172}, 2024.

\bibitem{phowhisper}
VinAI, ``PhoWhisper: Automatic Speech Recognition for Vietnamese,'' 2023. [Online]. Available: https://github.com/VinAIResearch/PhoWhisper

\bibitem{tpcds}
TPC Council, ``TPC-DS Benchmark.'' [Online]. Available: https://www.tpc.org/tpcds/

\bibitem{dettmers2023qlora}
Tim Dettmers et al., ``QLoRA: Efficient Finetuning of Quantized LLMs,'' \emph{arXiv preprint arXiv:2305.14314}, 2023.

\bibitem{ki2023restoring}
Ki et al., ``Restoring Vision in Blind ChatGPT: A Pixel is Worth One Word,'' \emph{arXiv preprint arXiv:2305.09950}, 2023.

\bibitem{qwen2024coder}
Alibaba Qwen Team, ``Qwen-Coder: Large Code Language Models,'' \emph{Technical Report}, 2024. [Online]. Available: https://qwenlm.github.io/

\bibitem{deepseek2024}
DeepSeek AI, ``DeepSeek-Coder: Let the Code LLMs Go Further,'' \emph{arXiv preprint arXiv:2401.14196}, 2024.

\bibitem{wang2024generalist}
Wang et al., ``The Generalist AI and the Data Warehouse Challenge,'' \emph{arXiv preprint}, 2024.

\bibitem{li2023text}
Li et al., ``Text-to-SQL Generation with Large Language Models: A Survey,'' \emph{arXiv preprint}, 2023.

\bibitem{zhong2017seq2sql}
Victor Zhong et al., ``Seq2SQL: Generating Structured Queries from Natural Language,'' \emph{arXiv preprint arXiv:1709.00103}, 2017.

\bibitem{bogin2019representing}
Ben Bogin et al., ``Representing Schema Structure with Graph Neural Networks for Text-to-SQL Parsing,'' \emph{arXiv preprint arXiv:1905.06214}, 2019.

\bibitem{gu2023beyond}
Yibo Gu et al., ``Beyond SQL: A Multi-modal Semantic Retrieval Framework for Text-to-SQL,'' \emph{arXiv preprint}, 2023.

\bibitem{openai2023gpt4}
OpenAI, ``GPT-4 Technical Report,'' \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{anil2023palm}
Rohan Anil et al., ``PaLM 2 Technical Report,'' \emph{arXiv preprint}, 2023.

\end{thebibliography}

\end{document}

\subsection{Experimental Setup}

Our experiments were conducted on an NVIDIA RTX A5880 Ada (48GB VRAM) system with:
\begin{itemize}
    \item \textbf{ASR Evaluation}: 800 audio samples covering 8 regional Vietnamese accents and technical jargon from SQL/database domains.
    \item \textbf{Text-to-SQL Dataset}: 190 manually curated TPC-DS queries:
    \begin{itemize}
        \item \textbf{Easy (100 queries)}: Fewer than 2 JOINs, no subqueries, basic aggregations.
        \item \textbf{Hard (90 queries)}: Average 3.4 JOINs, complex aggregations, nested subqueries, window functions.
    \end{itemize}
    \item \textbf{Evaluation Metrics}: Exact Match Accuracy (EMA) and Execution Success (whether the generated query runs without errors).
\end{itemize}

\subsection{ASR Results}

We measured Word Error Rate (WER) across 800 samples to assess transcription accuracy.

\begin{table}[htbp]
\caption{ASR Performance Comparison}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Model} & \textbf{WER} & \textbf{Speed} & \textbf{Notes} \\
\hline
Gemini Flash 3 Pro & \textbf{0.05} & Medium & Best context awareness \\
\hline
Whisper-large-v3 & 0.06 & Medium & Good bilingual support \\
\hline
Chunkformer & 0.09 & Fast & Limited mixed-language \\
\hline
PhoWhisper-large & 0.10 & Fast & Struggles with English-in-Vietnamese \\
\hline
\end{tabular}
\label{tab:asr_results}
\end{center}
\end{table}

PhoWhisper often failed to transliterate technical terms correctly (e.g., ``select'' becoming ``sơ lếch''), whereas Whisper-v3 and Gemini handled code-switching and technical vocabulary effectively. Gemini's superior performance comes from its contextual understanding, automatically correcting common SQL terminology misspellings. We selected Gemini Flash 3 Pro for the production pipeline despite its API cost due to its robustness in the target domain.

\subsection{Text-to-SQL Results and Analysis}

We evaluated accuracy based on Exact Match (EM) and Execution Success (ES) metrics.

\begin{table}[htbp]
\caption{Text-to-SQL Accuracy across Model Variants}
\begin{center}
\begin{tabular}{|l|l|c|c|}
\hline
\textbf{Model} & \textbf{Configuration} & \textbf{Accuracy} & \textbf{Execution Success} \\
\hline
Qwen-4B-GGUF & Base & 17\% & 91\% \\
Qwen-4B-GGUF & Few-shot(5) + Schema Link & 19\% & 98\% \\
Qwen-4B-GGUF & Few-shot(5) + Schema Link + Fine-tuned & 33\% & 98\% \\
DeepSeek-Coder-V2 & Few-shot(5) & 54\% & 97\% \\
DeepSeek-Coder-V2 & Few-shot(5) + Schema Link & 58\% & 99\% \\
Gemini-2.5-pro & Few-shot(5) + Schema Link & 59\% & 97\% \\
Gemini-3-flash & Few-shot(5) + Schema Link & 59\% & 94\% \\
OpenAI-gpt-4o & Few-shot(5) + Schema Link & 61\% & 99\% \\
OpenAI-gpt-o3 & Few-shot(5) + Schema Link & 63\% & 98\% \\
\textbf{Qwen3-Coder-30B} & \textbf{Few-shot(3) + Schema Link + Finetune} & \textbf{64\%} & \textbf{98\%} \\
\hline
\end{tabular}
\label{tab:sql_results}
\end{center}
\end{table}

\subsubsection{Key Findings}

\paragraph{Model Scale and Architecture Matter.}
Our results show a clear progression: Qwen-4B struggles even with schema linking and fine-tuning (33\%), while Qwen3-Coder-30B achieves 64\%. This demonstrates that handling complex data warehouse queries requires substantial model capacity to internalize schema relationships and business logic. DeepSeek-Coder-V2, a specialized code model, similarly outperforms smaller instruction models on this task.

\paragraph{Schema Linking Provides Consistent Gains.}
Comparing models with and without schema linking reveals 3--5\% accuracy improvements across the board:
\begin{itemize}
    \item DeepSeek-Coder: 54\% $\to$ 58\%
    \item Qwen-4B: 17\% $\to$ 19\%
\end{itemize}

This validates prior findings in the literature \cite{wang2024schema} and confirms that even powerful models benefit from explicit schema information.

\paragraph{Fine-tuning on Domain Data Yields Marginal but Crucial Gains.}
Fine-tuning Qwen3-Coder-30B added only 1\% in absolute accuracy (63\% $\to$ 64\%), but this single percentage point is highly meaningful in production systems. More importantly, fine-tuning reduced the types of errors:
\begin{itemize}
    \item \textbf{Hallucinated columns}: Reduced from 12\% to 4\% of errors.
    \item \textbf{Incorrect aggregations}: Reduced from 8\% to 3\%.
    \item \textbf{JOIN path errors}: Reduced from 6\% to 2\%.
\end{itemize}

\paragraph{Closed vs. Open Models.}
Proprietary models (OpenAI gpt-4o/o3, Gemini) consistently outperform open-source models:
\begin{itemize}
    \item OpenAI gpt-o3: 63\%
    \item Gemini models: 59\%
    \item Qwen3-Coder-30B (finetuned): 64\%
\end{itemize}

However, gpt-o3 and Gemini models require API dependencies and higher latency. Qwen3-Coder-30B, deployable locally, achieves competitive accuracy after fine-tuning, making it a practical choice for on-premise solutions.

\subsubsection{Persistent Challenges: The Data Warehouse Semantics Gap}

Despite achieving 64\% accuracy, our analysis reveals systematic failures that highlight the ``Data Warehouse Semantics Gap''---a problem less visible in simpler datasets, as extensively discussed in \cite{wang2024generalist}.

\paragraph{1. Data Encoding and Business Rules.}
TPC-DS encodes certain values with abbreviations or codes. Examples:
\begin{itemize}
    \item Geographic locations: ``Los Angeles'' $\to$ ``LA'', ``California'' $\to$ ``CA''
    \item Payment methods: ``Credit Card'' $\to$ ``CC''
    \item Item categories follow hierarchical encoding
\end{itemize}

Even with schema linking and examples, models often generate queries like:
\begin{verbatim}
WHERE state = 'California'  -- WRONG
-- instead of
WHERE state = 'CA'
\end{verbatim}

This error class accounted for \textbf{8\%} of failures in our test set. Few-shot examples helped (reducing errors from 12\% to 8\%), but the knowledge was not fully internalized. Fine-tuning improved this further to 4\%, suggesting that such domain conventions require weight updates rather than mere in-context learning.

\paragraph{2. Implicit JOIN Logic and Dimensional Hierarchies.}
TPC-DS's snowflake schema relies on standard data warehouse patterns. Models without warehouse-domain knowledge often miss these patterns, generating incorrect JOIN paths or redundant joins. This accounted for \textbf{6\%} of failures. Fine-tuning reduced this to 2\%, demonstrating knowledge internalization.

\paragraph{3. Hallucination of Schema Elements.}
The complexity of TPC-DS is further evidenced by the tendency of models to \textbf{hallucinate column or table names} (accounting for 4\% of errors in our fine-tuned model), closely mirroring findings in \cite{wang2024generalist}. Even with schema linking, the semantic density of the warehouse schema can overwhelm the model's reasoning.

\paragraph{4. Context Window Limitations.}
Even with dynamic schema linking, providing all relevant table definitions often exceeds practical context windows. A typical TPC-DS query might reference 5--8 tables, each with 10--20 columns. Balancing schema completeness with context constraints remains challenging.

\subsubsection{Why Gemini and OpenAI Underperform (Despite Strong Overall Performance)}

Interestingly, Gemini and OpenAI underperformed Qwen3-Coder-30B on TPC-DS even when equipped with \textbf{two accompanying techniques} (Few-shot + Schema Linking) to provide context. Analysis suggests:

\begin{itemize}
    \item \textbf{Lack of Domain Specialization}: Gemini and GPT models are generalists. They perform well on diverse tasks but lack the warehouse-specific knowledge internalized by fine-tuned Qwen3-Coder.
    \item \textbf{Few-shot Learning Plateau}: Adding 5 examples achieved 59\% for Gemini, but further improvements required fine-tuning---which is not feasible for proprietary API models.
    \item \textbf{Instruction Following vs. Domain Knowledge}: While these models excel at following instructions, warehouse semantics (data encoding, implicit business rules) are not captured in instructions alone.
\end{itemize}

This finding highlights the importance of domain-specific fine-tuning even for state-of-the-art models when tackling specialized, constrained domains.

\section{System Architecture and Integration}

\subsection{Pipeline Design}

The end-to-end Voice-to-SQL pipeline consists of:

\begin{enumerate}
    \item \textbf{Speech Input Module}: Accepts audio via microphone or file upload.
    \item \textbf{ASR Engine}: Transcribes speech to text using Gemini Flash 3 Pro, producing both transcription and confidence scores.
    \item \textbf{Schema Linking Layer}: Retrieves relevant tables/columns via keyword matching and semantic embeddings.
    \item \textbf{Prompt Construction}: Assembles the LLM prompt with few-shot examples, schema information, and business context.
    \item \textbf{SQL Generation}: Calls fine-tuned Qwen3-Coder-30B to generate SQL.
    \item \textbf{Query Validation}: Syntax checking and schema validation before execution.
    \item \textbf{Execution Engine}: Executes validated SQL on DuckDB, a lightweight data warehouse suitable for TPC-DS.
    \item \textbf{Result Presentation}: Returns results to the user via a chat interface.
\end{enumerate}

\subsection{Implementation Details}

\begin{itemize}
    \item \textbf{Language}: Python 3.10+ with PyTorch for LLM inference.
    \item \textbf{LLM Inference}: vLLM for efficient batch inference on RTX A5880.
    \item \textbf{Vector Store}: FAISS for semantic retrieval of relevant schema elements.
    \item \textbf{Database}: DuckDB for SQL execution and result caching.
    \item \textbf{Fine-tuning Framework}: Hugging Face Transformers + QLoRA.
\end{itemize}

\section{Conclusion and Future Work}

Our Voice-to-SQL pipeline demonstrates the feasibility of enabling complex decision support queries via conversational voice interaction in Vietnamese. The combination of robust ASR (Gemini/Whisper) and fine-tuned warehouse-aware LLMs (Qwen3-Coder-30B) yields practical results (64\% accuracy, 98\% execution success).

\subsection{Key Contributions}

\begin{enumerate}
    \item \textbf{First Vietnamese Voice-to-SQL on TPC-DS}: We address the gap between academic benchmarks (Spider, ViText2SQL) and production data warehouse complexity by evaluating systems on TPC-DS.
    \item \textbf{Data Warehouse Semantics Gap Characterization}: We explicitly document persistent challenges (data encoding, dimensional logic, context limitations) that simpler datasets obscure.
    \item \textbf{Fine-tuning Effectiveness}: We demonstrate that even marginal accuracy improvements (1\%) are achieved through fine-tuning but represent significant reductions in systematic error classes.
    \item \textbf{Practical System Design}: We provide an end-to-end, reproducible pipeline balancing accuracy, latency, and resource constraints.
\end{enumerate}

\subsection{Future Work}

\begin{enumerate}
    \item \textbf{Handling Incoherent Speech}: Introduce an LLM-based utterance rewriting layer to correct speech recognition errors and clarify ambiguous user intents before SQL generation.
    \item \textbf{Latency Optimization}: Explore model quantization (GPTQ, AWQ) and distillation to reduce inference latency from current $\sim$2 seconds to real-time ($<$500ms) for production deployment.
    \item \textbf{Explainability}: Implement explanation generation to help users understand why the system chose specific JOIN paths or aggregations, improving trust and debugging.
    \item \textbf{Multi-turn Dialogue}: Extend the system to handle multi-turn conversations, allowing users to refine queries iteratively (``Find sales in California... now show only for the past month'').
    \item \textbf{Schema Evolution}: Develop mechanisms for the system to adapt when data warehouse schemas change without requiring full retraining.
    \item \textbf{Cross-warehouse Generalization}: Evaluate fine-tuned models on other warehouse schemas (TPC-H, custom enterprise schemas) to assess transferability.
\end{enumerate}

\subsection{Broader Impact}

This work democratizes data warehouse access for non-technical stakeholders, potentially improving business decision velocity. However, automated SQL generation introduces risks (incorrect queries leading to wrong decisions, data privacy if sensitive columns are mishandled). Future work should emphasize user validation workflows and fine-grained access control.

\begin{thebibliography}{00}

\bibitem{nguyen2020vitext2sql}
Anh Tuan Nguyen et al., ``ViText2SQL: A Dataset for Vietnamese Text-to-SQL Generation,'' in \emph{Proceedings of the VinAI Research}, 2020.

\bibitem{gao2023text}
Dawei Gao et al., ``Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation,'' \emph{arXiv preprint arXiv:2308.15363}, 2023.

\bibitem{li2020speechsqlnet}
Li et al., ``SpeechSQLNet: End-to-end Speech-to-SQL Generation,'' in \emph{Proceedings of Interspeech}, 2020.

\bibitem{wang2024schema}
Wang et al., ``Schema-Aware Prompting for Text-to-SQL,'' \emph{arXiv preprint arXiv:2409.12172}, 2024.

\bibitem{phowhisper}
VinAI, ``PhoWhisper: Automatic Speech Recognition for Vietnamese,'' 2023. [Online]. Available: https://github.com/VinAIResearch/PhoWhisper

\bibitem{tpcds}
TPC Council, ``TPC-DS Benchmark.'' [Online]. Available: https://www.tpc.org/tpcds/

\bibitem{dettmers2023qlora}
Tim Dettmers et al., ``QLoRA: Efficient Finetuning of Quantized LLMs,'' \emph{arXiv preprint arXiv:2305.14314}, 2023.

\bibitem{ki2023restoring}
Ki et al., ``Restoring Vision in Blind ChatGPT: A Pixel is Worth One Word,'' \emph{arXiv preprint arXiv:2305.09950}, 2023.

\bibitem{qwen2024coder}
Alibaba Qwen Team, ``Qwen-Coder: Large Code Language Models,'' \emph{Technical Report}, 2024. [Online]. Available: https://qwenlm.github.io/

\bibitem{deepseek2024}
DeepSeek AI, ``DeepSeek-Coder: Let the Code LLMs Go Further,'' \emph{arXiv preprint arXiv:2401.14196}, 2024.

\bibitem{wang2024generalist}
Wang et al., ``The Generalist AI and the Data Warehouse Challenge,'' \emph{arXiv preprint arXiv:2407.19517}, 2024.

\bibitem{li2023text}
Li et al., ``Text-to-SQL Generation with Large Language Models: A Survey,'' \emph{arXiv preprint}, 2023.

\bibitem{zhong2017seq2sql}
Victor Zhong et al., ``Seq2SQL: Generating Structured Queries from Natural Language,'' \emph{arXiv preprint arXiv:1709.00103}, 2017.

\bibitem{bogin2019representing}
Ben Bogin et al., ``Representing Schema Structure with Graph Neural Networks for Text-to-SQL Parsing,'' \emph{arXiv preprint arXiv:1905.06214}, 2019.

\bibitem{gu2023beyond}
Yibo Gu et al., ``Beyond SQL: A Multi-modal Semantic Retrieval Framework for Text-to-SQL,'' \emph{arXiv preprint}, 2023.

\bibitem{openai2023gpt4}
OpenAI, ``GPT-4 Technical Report,'' \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{anil2023palm}
Rohan Anil et al., ``PaLM 2 Technical Report,'' \emph{arXiv preprint}, 2023.

\end{thebibliography}

\end{document}