{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Research Benchmark: Text-to-SQL Evaluation\n",
                "\n",
                "**Purpose**: Evaluate 6 experimental conditions and generate results for paper.\n",
                "\n",
                "| Condition | Description |\n",
                "|-----------|-------------|\n",
                "| B0 | Baseline: Qwen + Full Schema |\n",
                "| B1 | +Dynamic Schema Selection |\n",
                "| B2 | +Fine-tuned (QLoRA) |\n",
                "| B3 | Full: Dynamic + Fine-tuned + Repair |\n",
                "| B4 | +Schema Enrichment (Dynamic + Content) |\n",
                "| B5 | +RAG Few-shot (Dynamic + Examples) |\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Kaggle setup\n",
                "!git clone https://github.com/VuThanhLam124/Capstone-NLUS-VDD.git 2>/dev/null || true\n",
                "%cd Capstone-NLUS-VDD\n",
                "!pip -q install -r requirements.txt\n",
                "!pip -q install sqlglot peft scikit-learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import json\n",
                "import os\n",
                "import time\n",
                "import re\n",
                "import gc\n",
                "import math\n",
                "import unicodedata\n",
                "from decimal import Decimal\n",
                "from datetime import date, datetime\n",
                "\n",
                "import duckdb\n",
                "import pandas as pd\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "from peft import PeftModel, PeftConfig\n",
                "\n",
                "try:\n",
                "    import sqlglot\n",
                "except:\n",
                "    sqlglot = None\n",
                "\n",
                "# Try importing local modules if available\n",
                "import sys\n",
                "sys.path.append(os.getcwd())\n",
                "try:\n",
                "    from research_pipeline.rag_retriever import TextToSQLRetriever\n",
                "    print(\"RAG module loaded.\")\n",
                "except ImportError:\n",
                "    TextToSQLRetriever = None\n",
                "    print(\"RAG module NOT found.\")\n",
                "\n",
                "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "USE_4BIT = torch.cuda.is_available()\n",
                "print(f\"Device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== PATHS ==========\n",
                "def find_repo_root(start: Path) -> Path:\n",
                "    for p in [start] + list(start.parents):\n",
                "        if (p / \"research_pipeline\").exists():\n",
                "            return p\n",
                "    return start\n",
                "\n",
                "REPO_ROOT = find_repo_root(Path.cwd())\n",
                "DB_PATH = REPO_ROOT / \"research_pipeline\" / \"data\" / \"ecommerce_dw.duckdb\"\n",
                "TEST_DATA_PATH = REPO_ROOT / \"research_pipeline\" / \"data\" / \"test.csv\"\n",
                "DB_CONTENT_PATH = REPO_ROOT / \"research_pipeline\" / \"data\" / \"db_content_samples.json\"\n",
                "RAG_INDEX_DIR = REPO_ROOT / \"research_pipeline\" / \"rag_index\"\n",
                "RESULTS_DIR = REPO_ROOT / \"research_pipeline\" / \"results\"\n",
                "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# ========== MODEL ==========\n",
                "BASE_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
                "ADAPTER_ID = \"Ellbendls/Qwen-3-4b-Text_to_SQL\"\n",
                "LOCAL_ADAPTER = REPO_ROOT / \"research_pipeline\" / \"adapters\" / \"qwen_lora_v1\"\n",
                "\n",
                "# ========== BENCHMARK CONFIG ==========\n",
                "MAX_SAMPLES = None\n",
                "MAX_NEW_TOKENS = 256\n",
                "NUM_BEAMS = 1\n",
                "MAX_TABLES = 8\n",
                "\n",
                "# ========== CONDITIONS TO RUN ==========\n",
                "RUN_CONDITIONS = [\"B0\", \"B1\", \"B2\", \"B3\", \"B4\", \"B5\"]\n",
                "\n",
                "print(f\"Repo: {REPO_ROOT}\")\n",
                "print(f\"Test data: {TEST_DATA_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup Database & Load Resources"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def setup_tpcds_db(db_path: Path, scale_factor: int = 1) -> None:\n",
                "    db_path.parent.mkdir(parents=True, exist_ok=True)\n",
                "    con = duckdb.connect(str(db_path))\n",
                "    try:\n",
                "        con.execute(\"INSTALL tpcds;\")\n",
                "        con.execute(\"LOAD tpcds;\")\n",
                "        tables = [r[0] for r in con.execute(\"SHOW TABLES\").fetchall()]\n",
                "        if not tables:\n",
                "            print(f\"Generating TPC-DS (sf={scale_factor})...\")\n",
                "            con.execute(f\"CALL dsdgen(sf={scale_factor});\")\n",
                "        print(f\"Database ready: {len(tables)} tables\")\n",
                "    finally:\n",
                "        con.close()\n",
                "\n",
                "if not DB_PATH.exists():\n",
                "    setup_tpcds_db(DB_PATH)\n",
                "\n",
                "con = duckdb.connect(str(DB_PATH), read_only=True)\n",
                "\n",
                "# Build schema map\n",
                "schema_map = {}\n",
                "for (table_name,) in con.execute(\"SHOW TABLES\").fetchall():\n",
                "    cols = [(r[0], r[1]) for r in con.execute(f\"DESCRIBE {table_name}\").fetchall()]\n",
                "    schema_map[table_name] = cols\n",
                "\n",
                "# Load DB Content Samples (for B4)\n",
                "db_content = {}\n",
                "if DB_CONTENT_PATH.exists():\n",
                "    with open(DB_CONTENT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
                "        db_content = json.load(f)\n",
                "    print(f\"Loaded content samples for {len(db_content)} tables\")\n",
                "\n",
                "# Load RAG Retriever (for B5)\n",
                "retriever = None\n",
                "if TextToSQLRetriever and RAG_INDEX_DIR.exists():\n",
                "    try:\n",
                "        retriever = TextToSQLRetriever.load(RAG_INDEX_DIR)\n",
                "        print(\"RAG Retriever loaded successfully\")\n",
                "    except Exception as e:\n",
                "        print(f\"Failed to load RAG Retriever: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Schema Selection Utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def strip_accents(text: str) -> str:\n",
                "    return \"\".join(ch for ch in unicodedata.normalize(\"NFD\", text) if unicodedata.category(ch) != \"Mn\")\n",
                "\n",
                "def tokenize(text: str) -> list[str]:\n",
                "    text = strip_accents(text.lower())\n",
                "    tokens = []\n",
                "    for tok in re.findall(r\"[a-z0-9_]+\", text):\n",
                "        tokens.extend(tok.split(\"_\"))\n",
                "    return [t for t in tokens if len(t) > 1]\n",
                "\n",
                "SYNONYMS = {\n",
                "    \"khach\": \"customer\", \"khachhang\": \"customer\", \"sanpham\": \"item\",\n",
                "    \"hang\": \"item\", \"danhmuc\": \"category\", \"bang\": \"state\", \"tinh\": \"state\",\n",
                "    \"cuahang\": \"store\", \"doanhthu\": \"revenue\", \"soluong\": \"quantity\",\n",
                "    \"gia\": \"price\", \"thang\": \"month\", \"nam\": \"year\", \"quy\": \"quarter\",\n",
                "}\n",
                "\n",
                "def expand_tokens(tokens: list[str]) -> set[str]:\n",
                "    expanded = set(tokens)\n",
                "    for tok in tokens:\n",
                "        if tok in SYNONYMS:\n",
                "            expanded.add(SYNONYMS[tok])\n",
                "    return expanded\n",
                "\n",
                "# Pre-compute table tokens\n",
                "table_tokens = {}\n",
                "for table, cols in schema_map.items():\n",
                "    tokens = set(tokenize(table))\n",
                "    for col, _ in cols:\n",
                "        tokens.update(tokenize(col))\n",
                "    table_tokens[table] = tokens\n",
                "\n",
                "def select_tables(question: str, max_tables: int = 8) -> list[str]:\n",
                "    \"\"\"Dynamic schema selection based on question tokens.\"\"\"\n",
                "    q_tokens = expand_tokens(tokenize(question))\n",
                "    scored = [(len(q_tokens & tokens), table) for table, tokens in table_tokens.items()]\n",
                "    scored.sort(reverse=True)\n",
                "    selected = [t for score, t in scored if score > 0][:max_tables]\n",
                "    \n",
                "    def ensure(table):\n",
                "        if table in schema_map and table not in selected:\n",
                "            selected.append(table)\n",
                "    \n",
                "    if any(t in q_tokens for t in {\"year\", \"month\", \"quarter\", \"date\"}):\n",
                "        ensure(\"date_dim\")\n",
                "    if \"customer\" in q_tokens:\n",
                "        ensure(\"customer\"); ensure(\"customer_address\")\n",
                "    if \"store\" in q_tokens:\n",
                "        ensure(\"store_sales\"); ensure(\"store\")\n",
                "    if any(t in q_tokens for t in {\"sales\", \"revenue\"}):\n",
                "        ensure(\"store_sales\")\n",
                "    \n",
                "    return selected[:max_tables]\n",
                "\n",
                "def build_schema_text(tables: list[str], with_content: bool = False) -> str:\n",
                "    \"\"\"Build schema prompt with optional content samples.\"\"\"\n",
                "    lines = []\n",
                "    for table in tables:\n",
                "        cols = schema_map.get(table, [])\n",
                "        lines.append(f\"TABLE {table} (\")\n",
                "        for col, typ in cols:\n",
                "            extra = \"\"\n",
                "            if with_content:\n",
                "                samples = db_content.get(table, {}).get(col)\n",
                "                if samples:\n",
                "                    # Limit to 3 samples to save context\n",
                "                    s_str = \", \".join(f\"'{str(s)}'\" for s in samples[:3])\n",
                "                    extra = f\" -- sample: [{s_str}]\"\n",
                "            lines.append(f\"  {col} {typ}{extra}\")\n",
                "        lines.append(\")\")\n",
                "        lines.append(\"\")\n",
                "    return \"\\n\".join(lines).strip()\n",
                "\n",
                "def build_full_schema() -> str:\n",
                "    return build_schema_text(list(schema_map.keys()))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## SQL Utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "_FORBIDDEN = re.compile(r\"\\b(INSERT|UPDATE|DELETE|DROP|ALTER|CREATE)\\b\", re.I)\n",
                "\n",
                "SYSTEM_PROMPT = (\n",
                "    \"You translate user questions into SQL for DuckDB (TPC-DS). \"\n",
                "    \"Return only SQL, no markdown, no explanations.\"\n",
                ")\n",
                "REPAIR_PROMPT = (\n",
                "    \"Fix the broken SQL for DuckDB (TPC-DS). Return only corrected SQL.\"\n",
                ")\n",
                "\n",
                "def extract_sql(text: str) -> str:\n",
                "    text = text.strip()\n",
                "    m = re.search(r\"```(?:sql)?\\s*(.*?)```\", text, re.I | re.S)\n",
                "    if m:\n",
                "        text = m.group(1).strip()\n",
                "    if \";\" in text:\n",
                "        text = text.split(\";\", 1)[0].strip()\n",
                "    return text\n",
                "\n",
                "def is_valid_sql(sql: str) -> bool:\n",
                "    s = re.sub(r\"--.*$\", \"\", sql, flags=re.M).strip()\n",
                "    if not s or _FORBIDDEN.search(s):\n",
                "        return False\n",
                "    first = re.split(r\"\\s+\", s, 1)[0].upper()\n",
                "    return first in {\"SELECT\", \"WITH\"}\n",
                "\n",
                "def normalize_sql(sql: str) -> str:\n",
                "    if sqlglot:\n",
                "        try:\n",
                "            return sqlglot.parse_one(sql, read=\"duckdb\").sql(dialect=\"duckdb\", pretty=False)\n",
                "        except:\n",
                "            pass\n",
                "    return re.sub(r\"\\s+\", \" \", sql.strip()).lower()\n",
                "\n",
                "def normalize_value(v):\n",
                "    if isinstance(v, float) and math.isnan(v):\n",
                "        return \"nan\"\n",
                "    if isinstance(v, float):\n",
                "        return round(v, 6)\n",
                "    if isinstance(v, Decimal):\n",
                "        return float(round(v, 6))\n",
                "    if isinstance(v, (datetime, date)):\n",
                "        return v.isoformat()\n",
                "    return v\n",
                "\n",
                "def normalize_rows(rows, keep_order: bool):\n",
                "    if rows is None:\n",
                "        return None\n",
                "    norm = [tuple(normalize_value(x) for x in row) for row in rows]\n",
                "    return norm if keep_order else sorted(norm)\n",
                "\n",
                "def has_order_by(sql: str) -> bool:\n",
                "    return bool(re.search(r\"\\border\\s+by\\b\", sql, re.I))\n",
                "\n",
                "def run_sql(con, sql: str):\n",
                "    try:\n",
                "        return con.execute(sql).fetchall(), None\n",
                "    except Exception as e:\n",
                "        return None, str(e)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_model(use_adapter: bool = False, adapter_path: str = None):\n",
                "    \"\"\"Load Qwen model with optional LoRA adapter.\"\"\"\n",
                "    quant_config = BitsAndBytesConfig(load_in_4bit=True) if USE_4BIT else None\n",
                "    \n",
                "    if use_adapter:\n",
                "        adapter_id = adapter_path or ADAPTER_ID\n",
                "        peft_config = PeftConfig.from_pretrained(adapter_id)\n",
                "        base_id = peft_config.base_model_name_or_path or BASE_ID\n",
                "        tokenizer = AutoTokenizer.from_pretrained(adapter_id, use_fast=True, trust_remote_code=True)\n",
                "    else:\n",
                "        base_id = BASE_ID\n",
                "        tokenizer = AutoTokenizer.from_pretrained(base_id, use_fast=True, trust_remote_code=True)\n",
                "    \n",
                "    config = AutoConfig.from_pretrained(base_id, trust_remote_code=True)\n",
                "    if len(tokenizer) != config.vocab_size:\n",
                "        config.vocab_size = len(tokenizer)\n",
                "    \n",
                "    model_kwargs = dict(\n",
                "        config=config,\n",
                "        device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
                "        trust_remote_code=True,\n",
                "        ignore_mismatched_sizes=True,\n",
                "    )\n",
                "    if quant_config:\n",
                "        model_kwargs[\"quantization_config\"] = quant_config\n",
                "    \n",
                "    model = AutoModelForCausalLM.from_pretrained(base_id, **model_kwargs)\n",
                "    \n",
                "    if len(tokenizer) != model.get_input_embeddings().weight.shape[0]:\n",
                "        model.resize_token_embeddings(len(tokenizer))\n",
                "    \n",
                "    if use_adapter:\n",
                "        model = PeftModel.from_pretrained(model, adapter_id)\n",
                "    \n",
                "    if tokenizer.pad_token is None:\n",
                "        tokenizer.pad_token = tokenizer.eos_token\n",
                "    \n",
                "    model.eval()\n",
                "    return tokenizer, model\n",
                "\n",
                "def unload_model(model):\n",
                "    del model\n",
                "    gc.collect()\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Generation Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_prompt(question: str, schema_text: str, tokenizer, examples: list = None) -> str:\n",
                "    few_shot_text = \"\"\n",
                "    if examples:\n",
                "        few_shot_text = \"HERE ARE SOME EXAMPLES:\\n\"\n",
                "        for ex in examples:\n",
                "            few_shot_text += f\"Q: {ex['question']}\\nSQL: {ex['sql']}\\n\\n\"\n",
                "    \n",
                "    user = f\"SCHEMA:\\n{schema_text}\\n\\n{few_shot_text}QUESTION:\\n{question}\\n\\nSQL:\"\n",
                "    \n",
                "    if getattr(tokenizer, \"chat_template\", None):\n",
                "        return tokenizer.apply_chat_template(\n",
                "            [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": user}],\n",
                "            tokenize=False, add_generation_prompt=True\n",
                "        )\n",
                "    return f\"{SYSTEM_PROMPT}\\n\\n{user}\"\n",
                "\n",
                "def build_repair_prompt(question: str, schema_text: str, bad_sql: str, error: str, tokenizer) -> str:\n",
                "    user = f\"SCHEMA:\\n{schema_text}\\n\\nQUESTION:\\n{question}\\n\\nBROKEN_SQL:\\n{bad_sql}\\n\\nERROR:\\n{error}\\n\\nFIXED_SQL:\"\n",
                "    if getattr(tokenizer, \"chat_template\", None):\n",
                "        return tokenizer.apply_chat_template(\n",
                "            [{\"role\": \"system\", \"content\": REPAIR_PROMPT}, {\"role\": \"user\", \"content\": user}],\n",
                "            tokenize=False, add_generation_prompt=True\n",
                "        )\n",
                "    return f\"{REPAIR_PROMPT}\\n\\n{user}\"\n",
                "\n",
                "def generate(prompt: str, tokenizer, model) -> str:\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
                "    with torch.no_grad():\n",
                "        output_ids = model.generate(\n",
                "            **inputs, max_new_tokens=MAX_NEW_TOKENS, do_sample=False, num_beams=NUM_BEAMS,\n",
                "            pad_token_id=tokenizer.eos_token_id\n",
                "        )\n",
                "    gen_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
                "    return extract_sql(tokenizer.decode(gen_ids, skip_special_tokens=True))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup Test Data & Cache"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_df = pd.read_csv(TEST_DATA_PATH)\n",
                "test_df = test_df.dropna(subset=[\"Transcription\", \"SQL Ground Truth\"])\n",
                "\n",
                "if MAX_SAMPLES:\n",
                "    test_df = test_df.head(MAX_SAMPLES)\n",
                "\n",
                "print(f\"Test samples: {len(test_df)}\")\n",
                "\n",
                "# Build Ground Truth cache\n",
                "gt_cache = {}\n",
                "for idx, row in test_df.iterrows():\n",
                "    gt_sql = row[\"SQL Ground Truth\"].strip()\n",
                "    gt_res, gt_err = run_sql(con, gt_sql)\n",
                "    gt_cache[idx] = {\n",
                "        \"sql\": gt_sql,\n",
                "        \"res\": gt_res,\n",
                "        \"err\": gt_err,\n",
                "        \"has_order\": has_order_by(gt_sql),\n",
                "        \"norm_sorted\": normalize_rows(gt_res, False) if not gt_err else None,\n",
                "        \"norm_ordered\": normalize_rows(gt_res, True) if not gt_err else None,\n",
                "    }\n",
                "\n",
                "print(f\"Ground truth cached: {len(gt_cache)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run Benchmarks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_condition(condition: str, tokenizer, model) -> pd.DataFrame:\n",
                "    \"\"\"Run benchmark for a single condition.\"\"\"\n",
                "    use_dynamic = condition in [\"B1\", \"B3\", \"B4\", \"B5\"]\n",
                "    use_content = condition == \"B4\"\n",
                "    use_rag = condition == \"B5\"\n",
                "    use_repair = condition == \"B3\"\n",
                "    \n",
                "    full_schema = build_full_schema() if not use_dynamic else None\n",
                "    results = []\n",
                "    \n",
                "    for idx, row in test_df.iterrows():\n",
                "        question = row[\"Transcription\"]\n",
                "        gt = gt_cache[idx]\n",
                "        \n",
                "        # Schema\n",
                "        if use_dynamic:\n",
                "            tables = select_tables(question, MAX_TABLES)\n",
                "            schema_text = build_schema_text(tables, with_content=use_content)\n",
                "        else:\n",
                "            tables = list(schema_map.keys())\n",
                "            schema_text = full_schema\n",
                "        \n",
                "        # Few-shot examples (RAG)\n",
                "        examples = None\n",
                "        if use_rag and retriever:\n",
                "            examples = retriever.retrieve(question, k=3)\n",
                "            \n",
                "        # Generate\n",
                "        start = time.time()\n",
                "        prompt = build_prompt(question, schema_text, tokenizer, examples)\n",
                "        gen_sql = generate(prompt, tokenizer, model)\n",
                "        gen_time = (time.time() - start) * 1000\n",
                "        \n",
                "        valid = is_valid_sql(gen_sql)\n",
                "        gen_res, gen_err = run_sql(con, gen_sql) if valid else (None, \"INVALID_SQL\")\n",
                "        \n",
                "        # Repair (B3)\n",
                "        repair_used = False\n",
                "        if use_repair and (not valid or gen_err):\n",
                "            repair_prompt = build_repair_prompt(question, schema_text, gen_sql, gen_err or \"Invalid\", tokenizer)\n",
                "            gen_sql = generate(repair_prompt, tokenizer, model)\n",
                "            valid = is_valid_sql(gen_sql)\n",
                "            gen_res, gen_err = run_sql(con, gen_sql) if valid else (None, \"INVALID_SQL\")\n",
                "            repair_used = True\n",
                "        \n",
                "        # Metric Check\n",
                "        exact = normalize_sql(gen_sql) == normalize_sql(gt[\"sql\"]) if valid and not gen_err else False\n",
                "        exec_match = False\n",
                "        if valid and not gen_err and not gt[\"err\"]:\n",
                "            keep = gt[\"has_order\"] or has_order_by(gen_sql)\n",
                "            gt_norm = gt[\"norm_ordered\"] if keep else gt[\"norm_sorted\"]\n",
                "            gen_norm = normalize_rows(gen_res, keep)\n",
                "            exec_match = gt_norm == gen_norm\n",
                "            \n",
                "        results.append({\n",
                "            \"id\": row.get(\"ID\", idx),\n",
                "            \"condition\": condition,\n",
                "            \"valid_sql\": valid,\n",
                "            \"exact_match\": exact,\n",
                "            \"exec_match\": exec_match,\n",
                "            \"gen_time_ms\": gen_time,\n",
                "            \"repair_used\": repair_used\n",
                "        })\n",
                "        \n",
                "        if (idx + 1) % 20 == 0:\n",
                "            print(f\"  [{condition}] Processed {idx + 1}/{len(test_df)}\")\n",
                "            \n",
                "    return pd.DataFrame(results)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_results = []\n",
                "\n",
                "for cond in RUN_CONDITIONS:\n",
                "    print(f\"\\n{'='*40}\\nRunning: {cond}\\n{'='*40}\")\n",
                "    \n",
                "    # Logic to choose model based on condition intent\n",
                "    # For simplicity: B0,B1 use Base; B2,B3,B4,B5 use Adapter (assuming we want to test enriched/RAG on top of fine-tuned)\n",
                "    # OR: B4, B5 on Base model to see improvement without finetuning? \n",
                "    # Let's assume user wants to apply B4, B5 on Base or Finetuned? \n",
                "    # Usually techniques are tested on Strongest Baseline (Finetuned).\n",
                "    # But if Finetuned is too good, we might not see delta. \n",
                "    # Let's standardize: B0, B1 = Base. B2, B3, B4, B5 = Finetuned.\n",
                "    \n",
                "    use_adapter = cond in [\"B2\", \"B3\", \"B4\", \"B5\"]\n",
                "    adapter_path = str(LOCAL_ADAPTER) if LOCAL_ADAPTER.exists() else ADAPTER_ID\n",
                "    \n",
                "    print(f\"Loading model (adapter={use_adapter})...\")\n",
                "    tokenizer, model = load_model(use_adapter=use_adapter, adapter_path=adapter_path if use_adapter else None)\n",
                "    \n",
                "    results_df = run_condition(cond, tokenizer, model)\n",
                "    results_df.to_csv(RESULTS_DIR / f\"{cond}_results.csv\", index=False)\n",
                "    all_results.append(results_df)\n",
                "    \n",
                "    unload_model(model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "summary_rows = []\n",
                "for df in all_results:\n",
                "    c = df[\"condition\"].iloc[0]\n",
                "    summary_rows.append({\n",
                "        \"Condition\": c,\n",
                "        \"Valid (%)\": f\"{df['valid_sql'].mean()*100:.1f}\",\n",
                "        \"Exact Match (%)\": f\"{df['exact_match'].mean()*100:.1f}\",\n",
                "        \"Exec Acc (%)\": f\"{df['exec_match'].mean()*100:.1f}\",\n",
                "        \"Latency (ms)\": f\"{df['gen_time_ms'].mean():.0f}\"\n",
                "    })\n",
                "\n",
                "sum_df = pd.DataFrame(summary_rows)\n",
                "sum_df.to_csv(RESULTS_DIR / \"summary.csv\", index=False)\n",
                "print(sum_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "con.close()\n",
                "print(\"Done!\")"
            ]
        }
    ],
    "metadata": {
        "kaggle": {
            "accelerator": "gpu",
            "dataSources": [],
            "isGpuEnabled": true,
            "isInternetEnabled": true,
            "language": "python"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}