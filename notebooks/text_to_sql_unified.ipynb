{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Text-to-SQL: Fine-tuning & Evaluation on TPC-DS\n",
                "\n",
                "This unified notebook provides:\n",
                "- **Part 1**: QLoRA fine-tuning of Qwen for Text-to-SQL\n",
                "- **Part 2**: Evaluation with execution accuracy metrics\n",
                "\n",
                "**Target environment**: Kaggle (GPU T4/P100)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone repository (Kaggle)\n",
                "!git clone https://github.com/VuThanhLam124/Capstone-NLUS-VDD.git"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%cd Capstone-NLUS-VDD"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -r requirements.txt\n",
                "!pip -q install -U \"transformers>=4.43\" \"peft>=0.10\" \"bitsandbytes>=0.43\" \"accelerate>=0.30\" \"trl>=0.12\" \"datasets>=2.19\" sqlglot"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import json\n",
                "import os\n",
                "import random\n",
                "import time\n",
                "import re\n",
                "import gc\n",
                "import math\n",
                "import unicodedata\n",
                "from decimal import Decimal\n",
                "from datetime import date, datetime\n",
                "\n",
                "import duckdb\n",
                "import pandas as pd\n",
                "import torch\n",
                "from datasets import Dataset\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    AutoConfig,\n",
                "    AutoModelForCausalLM,\n",
                "    BitsAndBytesConfig,\n",
                "    TrainingArguments,\n",
                "    set_seed,\n",
                ")\n",
                "from peft import PeftModel, PeftConfig, prepare_model_for_kbit_training\n",
                "\n",
                "try:\n",
                "    from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
                "except Exception:\n",
                "    SFTTrainer = None\n",
                "    DataCollatorForCompletionOnlyLM = None\n",
                "\n",
                "try:\n",
                "    import sqlglot\n",
                "except Exception:\n",
                "    sqlglot = None\n",
                "    print(\"sqlglot not installed: SQL normalization will be simple.\")\n",
                "\n",
                "# ========== PATH CONFIG ==========\n",
                "def find_repo_root(start: Path) -> Path:\n",
                "    for p in [start] + list(start.parents):\n",
                "        if (p / \"research_pipeline\").exists():\n",
                "            return p\n",
                "    return start\n",
                "\n",
                "REPO_ROOT = find_repo_root(Path.cwd())\n",
                "DB_PATH = REPO_ROOT / \"research_pipeline\" / \"data\" / \"ecommerce_dw.duckdb\"\n",
                "\n",
                "# Fine-tune data\n",
                "FINETUNE_DATA_PATHS = [\n",
                "    REPO_ROOT / \"research_pipeline\" / \"data\" / \"data_finetune.csv\",\n",
                "    REPO_ROOT / \"data\" / \"data_finetune.csv\",\n",
                "]\n",
                "\n",
                "# Benchmark data\n",
                "BENCHMARK_CANDIDATES = [\n",
                "    REPO_ROOT / \"research_pipeline\" / \"data\" / \"test_queries_vi_200_v2.json\",\n",
                "    REPO_ROOT / \"research_pipeline\" / \"data\" / \"test_queries_vi_200.json\",\n",
                "    REPO_ROOT / \"research_pipeline\" / \"test_queries.json\",\n",
                "]\n",
                "\n",
                "OUTPUT_DIR = REPO_ROOT / \"research_pipeline\"\n",
                "\n",
                "# ========== MODEL CONFIG ==========\n",
                "BASE_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
                "ADAPTER_ID = \"Ellbendls/Qwen-3-4b-Text_to_SQL\"\n",
                "LOCAL_ADAPTER_DIR = REPO_ROOT / \"research_pipeline\" / \"qwen_text_to_sql_lora_v2\"\n",
                "\n",
                "# ========== TRAINING CONFIG ==========\n",
                "SEED = 42\n",
                "MAX_FINETUNE_SAMPLES = None  # Set to int for quick debug\n",
                "TRAIN_SPLIT = 0.9\n",
                "MAX_TABLES = 8\n",
                "MAX_SEQ_LEN = 768\n",
                "\n",
                "BATCH_SIZE = 1\n",
                "GRAD_ACCUM = 8\n",
                "NUM_EPOCHS = 2\n",
                "LEARNING_RATE = 2e-4\n",
                "WARMUP_RATIO = 0.05\n",
                "\n",
                "# ========== EVALUATION CONFIG ==========\n",
                "MAX_EVAL_SAMPLES = 200  # Set None for full benchmark\n",
                "SAMPLE_SEED = 42\n",
                "DEFAULT_LIMIT = None\n",
                "MAX_NEW_TOKENS = 256\n",
                "NUM_BEAMS = 1\n",
                "\n",
                "REPAIR_ON_ERROR = True\n",
                "REPAIR_MAX_ATTEMPTS = 1\n",
                "\n",
                "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "USE_4BIT = torch.cuda.is_available()\n",
                "\n",
                "set_seed(SEED)\n",
                "print(f\"Device: {DEVICE}\")\n",
                "print(f\"Repo root: {REPO_ROOT}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Setup TPC-DS Database"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "AUTO_SETUP_DB = True\n",
                "SETUP_SCALE_FACTOR = 1\n",
                "FORCE_RECREATE_DB = False\n",
                "\n",
                "def setup_tpcds_db(db_path: Path, scale_factor: int = 1, force_recreate: bool = False) -> None:\n",
                "    db_path.parent.mkdir(parents=True, exist_ok=True)\n",
                "    con = duckdb.connect(str(db_path))\n",
                "    try:\n",
                "        con.execute(\"INSTALL tpcds;\")\n",
                "        con.execute(\"LOAD tpcds;\")\n",
                "\n",
                "        tables = [r[0] for r in con.execute(\"SHOW TABLES\").fetchall()]\n",
                "        if tables and not force_recreate:\n",
                "            print(f\"Found {len(tables)} tables. Skip generation.\")\n",
                "            return\n",
                "\n",
                "        if force_recreate and tables:\n",
                "            for t in tables:\n",
                "                con.execute(f\"DROP TABLE {t}\")\n",
                "\n",
                "        print(f\"Generating TPC-DS (sf={scale_factor})...\")\n",
                "        start = time.time()\n",
                "        con.execute(f\"CALL dsdgen(sf={scale_factor});\")\n",
                "        print(f\"Data generation completed in {time.time() - start:.2f}s\")\n",
                "    finally:\n",
                "        con.close()\n",
                "\n",
                "if not DB_PATH.exists():\n",
                "    if AUTO_SETUP_DB:\n",
                "        setup_tpcds_db(DB_PATH, scale_factor=SETUP_SCALE_FACTOR, force_recreate=FORCE_RECREATE_DB)\n",
                "    else:\n",
                "        raise FileNotFoundError(f\"TPC-DS DuckDB not found: {DB_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Schema Utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "con = duckdb.connect(str(DB_PATH), read_only=True)\n",
                "\n",
                "schema_map = {}\n",
                "for (table_name,) in con.execute(\"SHOW TABLES\").fetchall():\n",
                "    columns = [r[0] for r in con.execute(f\"DESCRIBE {table_name}\").fetchall()]\n",
                "    schema_map[table_name] = columns\n",
                "\n",
                "\n",
                "def strip_accents(text: str) -> str:\n",
                "    return \"\".join(\n",
                "        ch for ch in unicodedata.normalize(\"NFD\", text) if unicodedata.category(ch) != \"Mn\"\n",
                "    )\n",
                "\n",
                "\n",
                "def tokenize(text: str) -> list[str]:\n",
                "    text = strip_accents(text.lower())\n",
                "    raw_tokens = re.findall(r\"[a-z0-9_]+\", text)\n",
                "    tokens = []\n",
                "    for tok in raw_tokens:\n",
                "        tokens.extend(tok.split(\"_\"))\n",
                "    return [t for t in tokens if len(t) > 1]\n",
                "\n",
                "\n",
                "SYNONYMS = {\n",
                "    \"khach\": \"customer\",\n",
                "    \"khachhang\": \"customer\",\n",
                "    \"khach_hang\": \"customer\",\n",
                "    \"sanpham\": \"item\",\n",
                "    \"san_pham\": \"item\",\n",
                "    \"hang\": \"item\",\n",
                "    \"danhmuc\": \"category\",\n",
                "    \"danh_muc\": \"category\",\n",
                "    \"bang\": \"state\",\n",
                "    \"tinh\": \"state\",\n",
                "    \"cuahang\": \"store\",\n",
                "    \"cua_hang\": \"store\",\n",
                "    \"doanhthu\": \"revenue\",\n",
                "    \"doanh_thu\": \"revenue\",\n",
                "    \"soluong\": \"quantity\",\n",
                "    \"so_luong\": \"quantity\",\n",
                "    \"gia\": \"price\",\n",
                "    \"thang\": \"month\",\n",
                "    \"nam\": \"year\",\n",
                "    \"quy\": \"quarter\",\n",
                "}\n",
                "\n",
                "\n",
                "def expand_tokens(tokens: list[str]) -> set[str]:\n",
                "    expanded = set(tokens)\n",
                "    for tok in list(tokens):\n",
                "        mapped = SYNONYMS.get(tok)\n",
                "        if mapped:\n",
                "            expanded.add(mapped)\n",
                "    return expanded\n",
                "\n",
                "\n",
                "table_tokens = {}\n",
                "for table, cols in schema_map.items():\n",
                "    tokens = set(tokenize(table))\n",
                "    for col in cols:\n",
                "        tokens.update(tokenize(col))\n",
                "    table_tokens[table] = tokens\n",
                "\n",
                "\n",
                "def select_tables_for_question(question: str, max_tables: int = 8) -> list[str]:\n",
                "    q_tokens = expand_tokens(tokenize(question))\n",
                "    scored = []\n",
                "    for table, tokens in table_tokens.items():\n",
                "        score = len(q_tokens & tokens)\n",
                "        scored.append((score, table))\n",
                "    scored.sort(reverse=True)\n",
                "\n",
                "    selected = [t for score, t in scored if score > 0][:max_tables]\n",
                "\n",
                "    def ensure(table: str):\n",
                "        if table in schema_map and table not in selected:\n",
                "            selected.append(table)\n",
                "\n",
                "    if any(tok in q_tokens for tok in {\"year\", \"month\", \"quarter\", \"date\"}):\n",
                "        ensure(\"date_dim\")\n",
                "    if any(tok in q_tokens for tok in {\"customer\"}):\n",
                "        ensure(\"customer\")\n",
                "        ensure(\"customer_address\")\n",
                "    if \"state\" in q_tokens:\n",
                "        ensure(\"customer_address\")\n",
                "        ensure(\"store\")\n",
                "    if \"store\" in q_tokens:\n",
                "        ensure(\"store_sales\")\n",
                "        ensure(\"store\")\n",
                "    if \"web\" in q_tokens:\n",
                "        ensure(\"web_sales\")\n",
                "        ensure(\"web_site\")\n",
                "    if \"catalog\" in q_tokens:\n",
                "        ensure(\"catalog_sales\")\n",
                "        ensure(\"call_center\")\n",
                "    if \"call\" in q_tokens:\n",
                "        ensure(\"call_center\")\n",
                "    if \"inventory\" in q_tokens:\n",
                "        ensure(\"inventory\")\n",
                "    if any(tok in q_tokens for tok in {\"item\", \"product\", \"category\"}):\n",
                "        ensure(\"item\")\n",
                "    if any(tok in q_tokens for tok in {\"sales\", \"revenue\", \"quantity\", \"price\"}):\n",
                "        ensure(\"store_sales\")\n",
                "\n",
                "    return selected[: max_tables or len(selected)]\n",
                "\n",
                "\n",
                "def build_schema_snippet(question: str, max_tables: int = 8) -> tuple[list[str], str]:\n",
                "    tables = select_tables_for_question(question, max_tables=max_tables)\n",
                "    if not tables:\n",
                "        tables = list(schema_map.keys())\n",
                "\n",
                "    lines = []\n",
                "    for table in tables:\n",
                "        cols = schema_map[table]\n",
                "        lines.append(f\"TABLE {table} (\")\n",
                "        for col in cols:\n",
                "            lines.append(f\"  {col}\")\n",
                "        lines.append(\")\")\n",
                "        lines.append(\"\")\n",
                "    return tables, \"\".join(lines).strip()\n",
                "\n",
                "\n",
                "print(f\"Loaded schema for {len(schema_map)} tables.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## SQL Utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "_FORBIDDEN_SQL = re.compile(\n",
                "    r\"\\b(INSERT|UPDATE|DELETE|DROP|ALTER|CREATE|COPY|PRAGMA|ATTACH|DETACH|EXPORT|IMPORT|CALL)\\b\",\n",
                "    re.IGNORECASE,\n",
                ")\n",
                "\n",
                "SYSTEM_PROMPT = (\n",
                "    \"You translate user questions into SQL for DuckDB (TPC-DS). \"\n",
                "    \"Return only SQL, no markdown, no explanations. \"\n",
                "    \"Use only tables and columns from the schema.\"\n",
                ")\n",
                "REPAIR_PROMPT = (\n",
                "    \"You are fixing SQL for DuckDB (TPC-DS). \"\n",
                "    \"Return only corrected SQL, no markdown, no explanations.\"\n",
                ")\n",
                "\n",
                "\n",
                "def extract_sql(text: str) -> str:\n",
                "    text = text.strip()\n",
                "    m = re.search(r\"```(?:sql)?\\s*(.*?)```\", text, flags=re.IGNORECASE | re.DOTALL)\n",
                "    if m:\n",
                "        text = m.group(1).strip()\n",
                "    if text.lower().startswith(\"sql:\"):\n",
                "        text = text[4:].strip()\n",
                "    if \";\" in text:\n",
                "        text = text.split(\";\", 1)[0].strip()\n",
                "    return text\n",
                "\n",
                "\n",
                "def is_safe_select(sql: str) -> bool:\n",
                "    s = re.sub(r\"--.*?$\", \"\", sql, flags=re.MULTILINE).strip()\n",
                "    if not s:\n",
                "        return False\n",
                "    if _FORBIDDEN_SQL.search(s):\n",
                "        return False\n",
                "    first = re.split(r\"\\s+\", s, maxsplit=1)[0].upper()\n",
                "    return first in {\"SELECT\", \"WITH\"}\n",
                "\n",
                "\n",
                "def ensure_limit(sql: str, limit: int | None) -> str:\n",
                "    if limit is None:\n",
                "        return sql\n",
                "    s = sql.strip().rstrip(\";\").strip()\n",
                "    if re.search(r\"\\bLIMIT\\b\", s, flags=re.IGNORECASE):\n",
                "        return s\n",
                "    return f\"{s}\\nLIMIT {limit}\"\n",
                "\n",
                "\n",
                "def has_order_by(sql: str) -> bool:\n",
                "    return re.search(r\"\\border\\s+by\\b\", sql, flags=re.IGNORECASE) is not None\n",
                "\n",
                "\n",
                "def normalize_sql_text(sql: str) -> str:\n",
                "    if sqlglot is not None:\n",
                "        try:\n",
                "            return sqlglot.parse_one(sql, read=\"duckdb\").sql(dialect=\"duckdb\", pretty=False)\n",
                "        except Exception:\n",
                "            pass\n",
                "    return re.sub(r\"\\s+\", \" \", sql.strip()).lower()\n",
                "\n",
                "\n",
                "def normalize_value(v):\n",
                "    if isinstance(v, float):\n",
                "        if math.isnan(v):\n",
                "            return \"nan\"\n",
                "        return round(v, 6)\n",
                "    if isinstance(v, Decimal):\n",
                "        return float(round(v, 6))\n",
                "    if isinstance(v, (datetime, date)):\n",
                "        return v.isoformat()\n",
                "    return v\n",
                "\n",
                "\n",
                "def normalize_rows(rows, keep_order: bool):\n",
                "    if rows is None:\n",
                "        return None\n",
                "    norm = [tuple(normalize_value(x) for x in row) for row in rows]\n",
                "    return norm if keep_order else sorted(norm)\n",
                "\n",
                "\n",
                "def run_sql(con, sql: str):\n",
                "    try:\n",
                "        res = con.execute(sql).fetchall()\n",
                "        return res, None\n",
                "    except Exception as e:\n",
                "        return None, str(e)\n",
                "\n",
                "\n",
                "def classify_error(err: str | None) -> str | None:\n",
                "    if err is None:\n",
                "        return None\n",
                "    if \"Binder Error\" in err:\n",
                "        return \"binder\"\n",
                "    if \"Parser Error\" in err:\n",
                "        return \"parser\"\n",
                "    if \"Catalog Error\" in err:\n",
                "        return \"catalog\"\n",
                "    return \"exec_error\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# PART 1: Fine-tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "RUN_FINETUNING = True  # Set to False to skip fine-tuning and use pre-trained adapter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if RUN_FINETUNING:\n",
                "    # Load fine-tune dataset\n",
                "    def resolve_data_path(paths):\n",
                "        for p in paths:\n",
                "            if p.exists():\n",
                "                return p\n",
                "        raise FileNotFoundError(f\"No data_finetune.csv found in: {paths}\")\n",
                "\n",
                "    DATA_PATH = resolve_data_path(FINETUNE_DATA_PATHS)\n",
                "    print(\"Using data:\", DATA_PATH)\n",
                "\n",
                "    df = pd.read_csv(DATA_PATH)\n",
                "    df = df.dropna(subset=[\"Transcription\", \"SQL Ground Truth\"]).copy()\n",
                "    df[\"Transcription\"] = df[\"Transcription\"].astype(str).str.strip()\n",
                "    df[\"SQL Ground Truth\"] = df[\"SQL Ground Truth\"].astype(str).str.strip()\n",
                "\n",
                "    def normalize_sql_for_train(sql: str) -> str:\n",
                "        sql = sql.strip()\n",
                "        if not sql.endswith(\";\"):\n",
                "            sql = sql + \";\"\n",
                "        return sql\n",
                "\n",
                "    df[\"SQL Ground Truth\"] = df[\"SQL Ground Truth\"].map(normalize_sql_for_train)\n",
                "    df = df.drop_duplicates(subset=[\"SQL Ground Truth\"]).reset_index(drop=True)\n",
                "\n",
                "    if MAX_FINETUNE_SAMPLES is not None:\n",
                "        df = df.sample(n=min(MAX_FINETUNE_SAMPLES, len(df)), random_state=SEED).reset_index(drop=True)\n",
                "\n",
                "    print(\"Rows:\", len(df))\n",
                "    df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if RUN_FINETUNING:\n",
                "    # Prepare dataset for training\n",
                "    tokenizer = AutoTokenizer.from_pretrained(ADAPTER_ID, use_fast=True, trust_remote_code=True)\n",
                "    if tokenizer.pad_token is None:\n",
                "        tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "    def format_record(record) -> str:\n",
                "        question = record[\"Transcription\"]\n",
                "        sql = record[\"SQL Ground Truth\"]\n",
                "        _, schema_text = build_schema_snippet(question, max_tables=MAX_TABLES)\n",
                "        user = f\"\"\"SCHEMA:\n",
                "{schema_text}\n",
                "\n",
                "QUESTION:\n",
                "{question}\n",
                "\n",
                "SQL:\"\"\"\n",
                "\n",
                "        if getattr(tokenizer, \"chat_template\", None):\n",
                "            messages = [\n",
                "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
                "                {\"role\": \"user\", \"content\": user},\n",
                "                {\"role\": \"assistant\", \"content\": sql},\n",
                "            ]\n",
                "            return tokenizer.apply_chat_template(\n",
                "                messages,\n",
                "                tokenize=False,\n",
                "                add_generation_prompt=False,\n",
                "            )\n",
                "\n",
                "        return f\"\"\"{SYSTEM_PROMPT}\n",
                "\n",
                "{user} {sql}\"\"\"\n",
                "\n",
                "    dataset = Dataset.from_pandas(df)\n",
                "\n",
                "    def map_fn(record):\n",
                "        return {\"text\": format_record(record)}\n",
                "\n",
                "    dataset = dataset.map(map_fn, remove_columns=dataset.column_names)\n",
                "    dataset = dataset.shuffle(seed=SEED)\n",
                "\n",
                "    train_size = int(len(dataset) * TRAIN_SPLIT)\n",
                "    train_dataset = dataset.select(range(train_size))\n",
                "    eval_dataset = dataset.select(range(train_size, len(dataset)))\n",
                "\n",
                "    print(\"Train size:\", len(train_dataset))\n",
                "    print(\"Eval size:\", len(eval_dataset))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if RUN_FINETUNING:\n",
                "    # Load model with QLoRA\n",
                "    quant_config = BitsAndBytesConfig(\n",
                "        load_in_4bit=True,\n",
                "        bnb_4bit_use_double_quant=True,\n",
                "        bnb_4bit_quant_type=\"nf4\",\n",
                "        bnb_4bit_compute_dtype=torch.float16,\n",
                "    )\n",
                "\n",
                "    config = AutoConfig.from_pretrained(BASE_ID, trust_remote_code=True)\n",
                "    if len(tokenizer) != config.vocab_size:\n",
                "        print(f\"Adjust vocab_size {config.vocab_size} -> {len(tokenizer)}\")\n",
                "        config.vocab_size = len(tokenizer)\n",
                "\n",
                "    base_model = AutoModelForCausalLM.from_pretrained(\n",
                "        BASE_ID,\n",
                "        config=config,\n",
                "        quantization_config=quant_config,\n",
                "        device_map=\"auto\",\n",
                "        trust_remote_code=True,\n",
                "        ignore_mismatched_sizes=True,\n",
                "    )\n",
                "\n",
                "    if len(tokenizer) != base_model.get_input_embeddings().weight.shape[0]:\n",
                "        base_model.resize_token_embeddings(len(tokenizer))\n",
                "\n",
                "    base_model = prepare_model_for_kbit_training(base_model)\n",
                "    model = PeftModel.from_pretrained(base_model, ADAPTER_ID, is_trainable=True)\n",
                "\n",
                "    model.print_trainable_parameters()\n",
                "    model.config.use_cache = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if RUN_FINETUNING:\n",
                "    # Train\n",
                "    if SFTTrainer is None:\n",
                "        raise RuntimeError(\"TRL is required. Install trl and restart kernel.\")\n",
                "\n",
                "    training_args = TrainingArguments(\n",
                "        output_dir=str(LOCAL_ADAPTER_DIR),\n",
                "        per_device_train_batch_size=BATCH_SIZE,\n",
                "        per_device_eval_batch_size=BATCH_SIZE,\n",
                "        gradient_accumulation_steps=GRAD_ACCUM,\n",
                "        num_train_epochs=NUM_EPOCHS,\n",
                "        learning_rate=LEARNING_RATE,\n",
                "        warmup_ratio=WARMUP_RATIO,\n",
                "        fp16=True,\n",
                "        logging_steps=20,\n",
                "        evaluation_strategy=\"steps\",\n",
                "        eval_steps=200,\n",
                "        save_steps=200,\n",
                "        save_total_limit=2,\n",
                "        load_best_model_at_end=False,\n",
                "        report_to=\"none\",\n",
                "        optim=\"paged_adamw_8bit\",\n",
                "    )\n",
                "\n",
                "    response_template = \"<|im_start|>assistant\\n\" if getattr(tokenizer, \"chat_template\", None) else \"SQL:\"\n",
                "\n",
                "    data_collator = None\n",
                "    if DataCollatorForCompletionOnlyLM is not None:\n",
                "        data_collator = DataCollatorForCompletionOnlyLM(\n",
                "            response_template=response_template,\n",
                "            tokenizer=tokenizer,\n",
                "        )\n",
                "\n",
                "    trainer = SFTTrainer(\n",
                "        model=model,\n",
                "        tokenizer=tokenizer,\n",
                "        args=training_args,\n",
                "        train_dataset=train_dataset,\n",
                "        eval_dataset=eval_dataset,\n",
                "        dataset_text_field=\"text\",\n",
                "        max_seq_length=MAX_SEQ_LEN,\n",
                "        packing=False,\n",
                "        data_collator=data_collator,\n",
                "    )\n",
                "\n",
                "    train_result = trainer.train()\n",
                "    print(train_result)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if RUN_FINETUNING:\n",
                "    # Save adapter\n",
                "    LOCAL_ADAPTER_DIR.mkdir(parents=True, exist_ok=True)\n",
                "    trainer.model.save_pretrained(LOCAL_ADAPTER_DIR)\n",
                "    tokenizer.save_pretrained(LOCAL_ADAPTER_DIR)\n",
                "    print(\"Saved adapter to\", LOCAL_ADAPTER_DIR)\n",
                "\n",
                "    # Cleanup\n",
                "    del model, base_model, trainer\n",
                "    gc.collect()\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# PART 2: Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Choose adapter source\n",
                "USE_LOCAL_ADAPTER = RUN_FINETUNING and LOCAL_ADAPTER_DIR.exists()\n",
                "\n",
                "if USE_LOCAL_ADAPTER:\n",
                "    EVAL_ADAPTER_ID = str(LOCAL_ADAPTER_DIR)\n",
                "    print(f\"Using local adapter: {EVAL_ADAPTER_ID}\")\n",
                "else:\n",
                "    EVAL_ADAPTER_ID = ADAPTER_ID\n",
                "    print(f\"Using HuggingFace adapter: {EVAL_ADAPTER_ID}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_model_and_tokenizer_for_eval():\n",
                "    quant_config = BitsAndBytesConfig(load_in_4bit=True) if USE_4BIT else None\n",
                "\n",
                "    peft_config = PeftConfig.from_pretrained(EVAL_ADAPTER_ID)\n",
                "    base_id = peft_config.base_model_name_or_path or BASE_ID\n",
                "\n",
                "    tokenizer = AutoTokenizer.from_pretrained(EVAL_ADAPTER_ID, use_fast=True, trust_remote_code=True)\n",
                "    config = AutoConfig.from_pretrained(base_id, trust_remote_code=True)\n",
                "\n",
                "    if len(tokenizer) != config.vocab_size:\n",
                "        print(f\"Adjusting vocab_size: {config.vocab_size} -> {len(tokenizer)}\")\n",
                "        config.vocab_size = len(tokenizer)\n",
                "\n",
                "    model_kwargs = dict(\n",
                "        config=config,\n",
                "        device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
                "        trust_remote_code=True,\n",
                "        ignore_mismatched_sizes=True,\n",
                "    )\n",
                "    if quant_config is not None:\n",
                "        model_kwargs[\"quantization_config\"] = quant_config\n",
                "\n",
                "    model = AutoModelForCausalLM.from_pretrained(base_id, **model_kwargs)\n",
                "\n",
                "    if len(tokenizer) != model.get_input_embeddings().weight.shape[0]:\n",
                "        model.resize_token_embeddings(len(tokenizer))\n",
                "\n",
                "    model = PeftModel.from_pretrained(model, EVAL_ADAPTER_ID)\n",
                "\n",
                "    if tokenizer.pad_token is None:\n",
                "        tokenizer.pad_token = tokenizer.eos_token\n",
                "    model.eval()\n",
                "\n",
                "    return tokenizer, model\n",
                "\n",
                "\n",
                "print(\"Loading model...\")\n",
                "tokenizer, model = load_model_and_tokenizer_for_eval()\n",
                "print(\"Model loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load benchmark dataset\n",
                "benchmark_path = None\n",
                "for candidate in BENCHMARK_CANDIDATES:\n",
                "    if candidate.exists():\n",
                "        benchmark_path = candidate\n",
                "        break\n",
                "\n",
                "if benchmark_path is None:\n",
                "    raise FileNotFoundError(\"No benchmark JSON found.\")\n",
                "\n",
                "raw_items = json.loads(benchmark_path.read_text())\n",
                "items = []\n",
                "for item in raw_items:\n",
                "    question = item.get(\"text\") or item.get(\"question\")\n",
                "    sql = item.get(\"sql\")\n",
                "    if not question or not sql:\n",
                "        continue\n",
                "    items.append({\n",
                "        \"id\": item.get(\"id\", f\"q{len(items)+1}\"),\n",
                "        \"text\": question,\n",
                "        \"sql\": sql,\n",
                "    })\n",
                "\n",
                "if MAX_EVAL_SAMPLES:\n",
                "    random.seed(SAMPLE_SEED)\n",
                "    items = random.sample(items, min(MAX_EVAL_SAMPLES, len(items)))\n",
                "\n",
                "print(f\"Benchmark items: {len(items)} from {benchmark_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_prompt(question: str, schema_text: str) -> str:\n",
                "    user = f\"SCHEMA:\\n{schema_text}\\n\\nQUESTION:\\n{question}\\n\\nSQL:\"\n",
                "    if getattr(tokenizer, \"chat_template\", None):\n",
                "        return tokenizer.apply_chat_template(\n",
                "            [\n",
                "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
                "                {\"role\": \"user\", \"content\": user},\n",
                "            ],\n",
                "            tokenize=False,\n",
                "            add_generation_prompt=True,\n",
                "        )\n",
                "    return f\"{SYSTEM_PROMPT}\\n\\n{user}\"\n",
                "\n",
                "\n",
                "def build_repair_prompt(question: str, schema_text: str, bad_sql: str, error: str) -> str:\n",
                "    user = (\n",
                "        f\"SCHEMA:\\n{schema_text}\\n\\nQUESTION:\\n{question}\\n\\n\"\n",
                "        f\"BROKEN_SQL:\\n{bad_sql}\\n\\nERROR:\\n{error}\\n\\nFIXED_SQL:\"\n",
                "    )\n",
                "    if getattr(tokenizer, \"chat_template\", None):\n",
                "        return tokenizer.apply_chat_template(\n",
                "            [\n",
                "                {\"role\": \"system\", \"content\": REPAIR_PROMPT},\n",
                "                {\"role\": \"user\", \"content\": user},\n",
                "            ],\n",
                "            tokenize=False,\n",
                "            add_generation_prompt=True,\n",
                "        )\n",
                "    return f\"{REPAIR_PROMPT}\\n\\n{user}\"\n",
                "\n",
                "\n",
                "def run_generation(prompt: str) -> str:\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
                "    pad_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id\n",
                "    with torch.no_grad():\n",
                "        output_ids = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=MAX_NEW_TOKENS,\n",
                "            do_sample=False,\n",
                "            num_beams=NUM_BEAMS,\n",
                "            pad_token_id=pad_id,\n",
                "        )\n",
                "    gen_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
                "    text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
                "    return extract_sql(text)\n",
                "\n",
                "\n",
                "def generate_sql_eval(question: str, schema_text: str) -> str:\n",
                "    prompt = build_prompt(question, schema_text)\n",
                "    sql = run_generation(prompt)\n",
                "    return ensure_limit(sql, DEFAULT_LIMIT)\n",
                "\n",
                "\n",
                "def repair_sql_eval(question: str, schema_text: str, bad_sql: str, error: str) -> str | None:\n",
                "    prompt = build_repair_prompt(question, schema_text, bad_sql, error)\n",
                "    sql = run_generation(prompt)\n",
                "    return ensure_limit(sql, DEFAULT_LIMIT)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cache ground truth results\n",
                "gt_cache = {}\n",
                "for item in items:\n",
                "    qid = item[\"id\"]\n",
                "    gt_sql = ensure_limit(item[\"sql\"], DEFAULT_LIMIT)\n",
                "    gt_res, gt_err = run_sql(con, gt_sql)\n",
                "    gt_cache[qid] = {\n",
                "        \"sql\": gt_sql,\n",
                "        \"res\": gt_res,\n",
                "        \"err\": gt_err,\n",
                "        \"has_order\": has_order_by(gt_sql),\n",
                "        \"norm_sorted\": normalize_rows(gt_res, keep_order=False) if gt_err is None else None,\n",
                "        \"norm_ordered\": normalize_rows(gt_res, keep_order=True) if gt_err is None else None,\n",
                "    }\n",
                "print(f\"Ground-truth cached: {len(gt_cache)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run evaluation\n",
                "results = []\n",
                "for idx, item in enumerate(items, 1):\n",
                "    qid = item[\"id\"]\n",
                "    question = item[\"text\"]\n",
                "    gt = gt_cache[qid]\n",
                "\n",
                "    schema_tables, schema_text = build_schema_snippet(question, max_tables=MAX_TABLES)\n",
                "\n",
                "    start = time.time()\n",
                "    gen_sql = generate_sql_eval(question, schema_text)\n",
                "    gen_time = time.time() - start\n",
                "\n",
                "    repair_used = False\n",
                "    valid_sql = is_safe_select(gen_sql)\n",
                "    if valid_sql:\n",
                "        exec_start = time.time()\n",
                "        gen_res, gen_err = run_sql(con, gen_sql)\n",
                "        exec_time = time.time() - exec_start\n",
                "    else:\n",
                "        gen_res, gen_err, exec_time = None, \"INVALID_SQL\", None\n",
                "\n",
                "    # Try repair on error\n",
                "    if REPAIR_ON_ERROR and (not valid_sql or gen_err is not None) and REPAIR_MAX_ATTEMPTS > 0:\n",
                "        repair_sql_text = repair_sql_eval(question, schema_text, gen_sql, gen_err)\n",
                "        if repair_sql_text:\n",
                "            repair_used = True\n",
                "            gen_sql = repair_sql_text\n",
                "            valid_sql = is_safe_select(gen_sql)\n",
                "            if valid_sql:\n",
                "                exec_start = time.time()\n",
                "                gen_res, gen_err = run_sql(con, gen_sql)\n",
                "                exec_time = time.time() - exec_start\n",
                "            else:\n",
                "                gen_res, gen_err, exec_time = None, \"INVALID_SQL\", None\n",
                "\n",
                "    exact_match = False\n",
                "    if valid_sql and gen_err is None:\n",
                "        exact_match = normalize_sql_text(gen_sql) == normalize_sql_text(gt[\"sql\"])\n",
                "\n",
                "    exec_match = False\n",
                "    if valid_sql and gen_err is None and gt[\"err\"] is None:\n",
                "        keep_order = gt[\"has_order\"] or has_order_by(gen_sql)\n",
                "        gt_norm = gt[\"norm_ordered\"] if keep_order else gt[\"norm_sorted\"]\n",
                "        gen_norm = normalize_rows(gen_res, keep_order=keep_order)\n",
                "        exec_match = gt_norm == gen_norm\n",
                "\n",
                "    results.append({\n",
                "        \"id\": qid,\n",
                "        \"question\": question,\n",
                "        \"gt_sql\": gt[\"sql\"],\n",
                "        \"gen_sql\": gen_sql,\n",
                "        \"valid_sql\": valid_sql,\n",
                "        \"exact_match\": exact_match,\n",
                "        \"exec_match\": exec_match,\n",
                "        \"gen_error\": gen_err,\n",
                "        \"gen_error_type\": classify_error(gen_err),\n",
                "        \"gt_error\": gt[\"err\"],\n",
                "        \"gen_time_sec\": gen_time,\n",
                "        \"exec_time_sec\": exec_time,\n",
                "        \"schema_tables\": \",\".join(schema_tables),\n",
                "        \"schema_table_count\": len(schema_tables),\n",
                "        \"repair_used\": repair_used,\n",
                "    })\n",
                "\n",
                "    if idx % 10 == 0:\n",
                "        print(f\"Processed {idx}/{len(items)}\")\n",
                "\n",
                "results_df = pd.DataFrame(results)\n",
                "print(f\"\\nCompleted: {len(results_df)} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate metrics\n",
                "valid_mask = results_df[\"valid_sql\"]\n",
                "exec_success = results_df[\"gen_error\"].isna()\n",
                "exec_acc_all = results_df[\"exec_match\"].mean() if not results_df.empty else 0.0\n",
                "valid_exec_mask = valid_mask & results_df[\"gt_error\"].isna()\n",
                "exec_acc_valid = results_df.loc[valid_exec_mask, \"exec_match\"].mean() if valid_exec_mask.any() else 0.0\n",
                "exact_match_rate = results_df.loc[valid_mask, \"exact_match\"].mean() if valid_mask.any() else 0.0\n",
                "\n",
                "summary = {\n",
                "    \"adapter\": EVAL_ADAPTER_ID,\n",
                "    \"total_samples\": len(results_df),\n",
                "    \"valid_sql_rate\": float(valid_mask.mean()) if not results_df.empty else 0.0,\n",
                "    \"exec_success_rate\": float(exec_success.mean()) if not results_df.empty else 0.0,\n",
                "    \"exec_accuracy_all\": exec_acc_all,\n",
                "    \"exec_accuracy_valid\": exec_acc_valid,\n",
                "    \"exact_match_rate\": exact_match_rate,\n",
                "    \"avg_gen_time_sec\": float(results_df[\"gen_time_sec\"].mean()) if not results_df.empty else 0.0,\n",
                "    \"repair_used_count\": int(results_df[\"repair_used\"].sum()),\n",
                "}\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"EVALUATION SUMMARY\")\n",
                "print(\"=\"*50)\n",
                "for k, v in summary.items():\n",
                "    if isinstance(v, float):\n",
                "        print(f\"{k}: {v:.4f}\")\n",
                "    else:\n",
                "        print(f\"{k}: {v}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results\n",
                "output_csv = OUTPUT_DIR / \"benchmark_text_to_sql_results.csv\"\n",
                "results_df.to_csv(output_csv, index=False)\n",
                "print(f\"Results saved to: {output_csv}\")\n",
                "\n",
                "# Show sample results\n",
                "results_df[[\"id\", \"question\", \"valid_sql\", \"exec_match\", \"gen_error\"]].head(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cleanup\n",
                "del model\n",
                "gc.collect()\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.empty_cache()\n",
                "con.close()\n",
                "print(\"Done!\")"
            ]
        }
    ],
    "metadata": {
        "kaggle": {
            "accelerator": "gpu",
            "dataSources": [],
            "dockerImageVersionId": 31236,
            "isGpuEnabled": true,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}