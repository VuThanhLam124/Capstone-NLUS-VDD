{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPC-DS Text-to-SQL Execution Benchmark\n",
    "\n",
    "This notebook extends the text-to-SQL pipeline and evaluates execution accuracy on a single TPC-DS benchmark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed (Kaggle), run once:\n",
    "# !pip -q install -r ../requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "from decimal import Decimal\n",
    "from datetime import date, datetime\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"research_pipeline\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "DB_PATH = REPO_ROOT / \"research_pipeline\" / \"data\" / \"ecommerce_dw.duckdb\"\n",
    "PRIMARY_BENCHMARK = REPO_ROOT / \"research_pipeline\" / \"data\" / \"test_queries_vi_200.json\"\n",
    "FALLBACK_BENCHMARK = REPO_ROOT / \"research_pipeline\" / \"test_queries.json\"\n",
    "OUTPUT_DIR = REPO_ROOT / \"research_pipeline\"\n",
    "OUTPUT_CSV_ALL = OUTPUT_DIR / \"benchmark_text_to_sql_all.csv\"\n",
    "\n",
    "MODEL_CHOICES = {\n",
    "    \"qwen_3_4b_text_to_sql\": {\n",
    "        \"type\": \"causal\",\n",
    "        \"id\": \"Ellbendls/Qwen-3-4b-Text_to_SQL\",\n",
    "    },\n",
    "    \"t5_small_awesome\": {\n",
    "        \"type\": \"seq2seq\",\n",
    "        \"id\": \"cssupport/t5-small-awesome-text-to-sql\",\n",
    "    },\n",
    "    \"llama3_1_8b_lora\": {\n",
    "        \"type\": \"lora_causal\",\n",
    "        \"adapter_id\": \"philschmid/code-llama-3-1-8b-text-to-sql-lora\",\n",
    "        \"base_id\": None,\n",
    "    },\n",
    "}\n",
    "MODEL_ORDER = [\"qwen_3_4b_text_to_sql\", \"t5_small_awesome\", \"llama3_1_8b_lora\"]\n",
    "RUN_ALL_MODELS = True\n",
    "MODEL_CHOICE = \"qwen_3_4b_text_to_sql\"\n",
    "CONTINUE_ON_ERROR = True\n",
    "\n",
    "MAX_SAMPLES = 50  # set None to run full benchmark\n",
    "SAMPLE_SEED = 42\n",
    "DEFAULT_LIMIT = None  # set to an int to force LIMIT on both GT and generated SQL\n",
    "MAX_TABLES = None  # set to an int to shorten schema prompt\n",
    "MAX_NEW_TOKENS = 256\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "USE_4BIT = torch.cuda.is_available()\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO_SETUP_DB = False\n",
    "if not DB_PATH.exists():\n",
    "    if AUTO_SETUP_DB:\n",
    "        import subprocess\n",
    "        import sys\n",
    "        setup_script = REPO_ROOT / \"research_pipeline\" / \"setup_dw.py\"\n",
    "        if not setup_script.exists():\n",
    "            raise FileNotFoundError(f\"Missing setup script: {setup_script}\")\n",
    "        subprocess.check_call([sys.executable, str(setup_script)])\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"TPC-DS DuckDB not found: {DB_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70993a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(str(DB_PATH), read_only=True)\n",
    "\n",
    "def duckdb_schema_prompt(con, *, table_schema: str = \"main\", max_tables: int | None = None) -> str:\n",
    "    rows = con.execute(\n",
    "        \"\"\"\n",
    "        SELECT table_name, column_name, data_type, ordinal_position\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = ?\n",
    "        ORDER BY table_name, ordinal_position\n",
    "        \"\"\"\n",
    "        [table_schema],\n",
    "    ).fetchall()\n",
    "\n",
    "    tables: dict[str, list[tuple[str, str]]] = {}\n",
    "    for table_name, column_name, data_type, _ in rows:\n",
    "        tables.setdefault(str(table_name), []).append((str(column_name), str(data_type)))\n",
    "\n",
    "    table_names = sorted(tables.keys())\n",
    "    if max_tables is not None:\n",
    "        table_names = table_names[:max_tables]\n",
    "\n",
    "    lines: list[str] = []\n",
    "    for t in table_names:\n",
    "        lines.append(f\"TABLE {t} (\")\n",
    "        for col, typ in tables[t]:\n",
    "            lines.append(f\"  {col} {typ}\")\n",
    "        lines.append(\")\")\n",
    "        lines.append(\"\")\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "schema_text = duckdb_schema_prompt(con, max_tables=MAX_TABLES)\n",
    "print(f\"Schema tables: {schema_text.count('TABLE ')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e035eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRIMARY_BENCHMARK.exists():\n",
    "    benchmark_path = PRIMARY_BENCHMARK\n",
    "elif FALLBACK_BENCHMARK.exists():\n",
    "    benchmark_path = FALLBACK_BENCHMARK\n",
    "else:\n",
    "    raise FileNotFoundError(\"No benchmark JSON found.\")\n",
    "\n",
    "raw_items = json.loads(benchmark_path.read_text())\n",
    "items = []\n",
    "for item in raw_items:\n",
    "    question = item.get(\"text\") or item.get(\"question\")\n",
    "    sql = item.get(\"sql\")\n",
    "    if not question or not sql:\n",
    "        continue\n",
    "    items.append({\n",
    "        \"id\": item.get(\"id\", f\"q{len(items)+1}\"),\n",
    "        \"text\": question,\n",
    "        \"sql\": sql,\n",
    "    })\n",
    "\n",
    "if MAX_SAMPLES:\n",
    "    random.seed(SAMPLE_SEED)\n",
    "    items = random.sample(items, min(MAX_SAMPLES, len(items)))\n",
    "\n",
    "print(f\"Benchmark items: {len(items)} from {benchmark_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab35b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(spec: dict):\n",
    "    model_type = spec[\"type\"]\n",
    "    if model_type == \"seq2seq\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(spec[\"id\"], use_fast=True, trust_remote_code=True)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            spec[\"id\"],\n",
    "            device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    "            torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model_kind = \"seq2seq\"\n",
    "        model_id = spec[\"id\"]\n",
    "    elif model_type == \"lora_causal\":\n",
    "        from peft import PeftConfig, PeftModel\n",
    "        adapter_id = spec[\"adapter_id\"]\n",
    "        peft_config = PeftConfig.from_pretrained(adapter_id)\n",
    "        base_id = spec.get(\"base_id\") or peft_config.base_model_name_or_path\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_id, use_fast=True, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_id,\n",
    "            device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    "            torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "            load_in_4bit=USE_4BIT,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(model, adapter_id)\n",
    "        model_kind = \"causal\"\n",
    "        model_id = f\"{base_id} + {adapter_id}\"\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(spec[\"id\"], use_fast=True, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            spec[\"id\"],\n",
    "            device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    "            torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "            load_in_4bit=USE_4BIT,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model_kind = \"causal\"\n",
    "        model_id = spec[\"id\"]\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.eval()\n",
    "    return tokenizer, model, model_kind, model_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39643f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_FORBIDDEN_SQL = re.compile(\n",
    "    r\"\\b(INSERT|UPDATE|DELETE|DROP|ALTER|CREATE|COPY|PRAGMA|ATTACH|DETACH|EXPORT|IMPORT|CALL)\\b\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "def extract_sql(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    m = re.search(r\"```(?:sql)?\\s*(.*?)```\", text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    if m:\n",
    "        text = m.group(1).strip()\n",
    "    if text.lower().startswith(\"sql:\"):\n",
    "        text = text[4:].strip()\n",
    "    if \";\" in text:\n",
    "        text = text.split(\";\", 1)[0].strip()\n",
    "    return text\n",
    "\n",
    "def is_safe_select(sql: str) -> bool:\n",
    "    s = re.sub(r\"--.*?$\", \"\", sql, flags=re.MULTILINE).strip()\n",
    "    if not s:\n",
    "        return False\n",
    "    if _FORBIDDEN_SQL.search(s):\n",
    "        return False\n",
    "    first = re.split(r\"\\s+\", s, maxsplit=1)[0].upper()\n",
    "    return first in {\"SELECT\", \"WITH\"}\n",
    "\n",
    "def ensure_limit(sql: str, limit: int | None) -> str:\n",
    "    if limit is None:\n",
    "        return sql\n",
    "    s = sql.strip().rstrip(\";\").strip()\n",
    "    if re.search(r\"\\bLIMIT\\b\", s, flags=re.IGNORECASE):\n",
    "        return s\n",
    "    return f\"{s}\\nLIMIT {limit}\"\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You translate user questions into SQL for DuckDB (TPC-DS). \"\n",
    "    \"Return only SQL, no markdown, no explanations. \"\n",
    "    \"Use only tables and columns from the schema.\"\n",
    ")\n",
    "\n",
    "def build_prompt(question: str, schema_text: str, tokenizer, model_kind: str) -> str:\n",
    "    if model_kind == \"seq2seq\":\n",
    "        return f\"translate to SQL:\\n{question}\\n\\nSCHEMA:\\n{schema_text}\\n\\nSQL:\"\n",
    "    user = f\"SCHEMA:\\n{schema_text}\\n\\nQUESTION:\\n{question}\\n\\nSQL:\"\n",
    "    if getattr(tokenizer, \"chat_template\", None):\n",
    "        return tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": user},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    return f\"{SYSTEM_PROMPT}\\n\\n{user}\"\n",
    "\n",
    "def generate_sql(question: str, schema_text: str, tokenizer, model, model_kind: str) -> str:\n",
    "    prompt = build_prompt(question, schema_text, tokenizer, model_kind)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    pad_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            pad_token_id=pad_id,\n",
    "        )\n",
    "    if model_kind == \"seq2seq\":\n",
    "        text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    else:\n",
    "        gen_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "    sql = extract_sql(text)\n",
    "    sql = ensure_limit(sql, DEFAULT_LIMIT)\n",
    "    return sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d314c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_value(v):\n",
    "    if isinstance(v, float):\n",
    "        return round(v, 6)\n",
    "    if isinstance(v, Decimal):\n",
    "        return float(round(v, 6))\n",
    "    if isinstance(v, (datetime, date)):\n",
    "        return v.isoformat()\n",
    "    return v\n",
    "\n",
    "def normalize_rows(rows):\n",
    "    if rows is None:\n",
    "        return None\n",
    "    norm = [tuple(normalize_value(x) for x in row) for row in rows]\n",
    "    return sorted(norm)\n",
    "\n",
    "def run_sql(con, sql: str):\n",
    "    try:\n",
    "        res = con.execute(sql).fetchall()\n",
    "        return res, None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d8073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark_for_model(model_choice: str):\n",
    "    spec = MODEL_CHOICES[model_choice]\n",
    "    print(f\"\\nLoading model choice: {model_choice}\")\n",
    "    tokenizer, model, model_kind, model_id = load_model_and_tokenizer(spec)\n",
    "\n",
    "    results = []\n",
    "    for idx, item in enumerate(items, 1):\n",
    "        qid = item[\"id\"]\n",
    "        question = item[\"text\"]\n",
    "        gt_sql = ensure_limit(item[\"sql\"], DEFAULT_LIMIT)\n",
    "\n",
    "        gt_res, gt_err = run_sql(con, gt_sql)\n",
    "\n",
    "        start = time.time()\n",
    "        gen_sql = generate_sql(question, schema_text, tokenizer, model, model_kind)\n",
    "        gen_time = time.time() - start\n",
    "\n",
    "        if not is_safe_select(gen_sql):\n",
    "            results.append({\n",
    "                \"id\": qid,\n",
    "                \"question\": question,\n",
    "                \"gt_sql\": gt_sql,\n",
    "                \"gen_sql\": gen_sql,\n",
    "                \"exec_match\": False,\n",
    "                \"gen_error\": \"INVALID_SQL\",\n",
    "                \"gt_error\": gt_err,\n",
    "                \"gen_time_sec\": gen_time,\n",
    "                \"model_choice\": model_choice,\n",
    "                \"model_id\": model_id,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        gen_res, gen_err = run_sql(con, gen_sql)\n",
    "\n",
    "        exec_match = False\n",
    "        if gt_err is None and gen_err is None:\n",
    "            exec_match = normalize_rows(gt_res) == normalize_rows(gen_res)\n",
    "\n",
    "        results.append({\n",
    "            \"id\": qid,\n",
    "            \"question\": question,\n",
    "            \"gt_sql\": gt_sql,\n",
    "            \"gen_sql\": gen_sql,\n",
    "            \"exec_match\": exec_match,\n",
    "            \"gen_error\": gen_err,\n",
    "            \"gt_error\": gt_err,\n",
    "            \"gen_time_sec\": gen_time,\n",
    "            \"model_choice\": model_choice,\n",
    "            \"model_id\": model_id,\n",
    "        })\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"Processed {idx}/{len(items)}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb62db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_choices = MODEL_ORDER if RUN_ALL_MODELS else [MODEL_CHOICE]\n",
    "all_results = []\n",
    "summary_rows = []\n",
    "\n",
    "for choice in model_choices:\n",
    "    try:\n",
    "        results_df = run_benchmark_for_model(choice)\n",
    "    except Exception as e:\n",
    "        print(f\"Model {choice} failed: {e}\")\n",
    "        if CONTINUE_ON_ERROR:\n",
    "            continue\n",
    "        raise\n",
    "\n",
    "    out_path = OUTPUT_DIR / f\"benchmark_text_to_sql_{choice}.csv\"\n",
    "    results_df.to_csv(out_path, index=False)\n",
    "    all_results.append(results_df)\n",
    "\n",
    "    valid_mask = results_df[\"gen_error\"].isna() & results_df[\"gt_error\"].isna()\n",
    "    exec_acc_all = results_df[\"exec_match\"].mean() if not results_df.empty else 0.0\n",
    "    exec_acc_valid = results_df.loc[valid_mask, \"exec_match\"].mean() if valid_mask.any() else 0.0\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"model_choice\": choice,\n",
    "        \"total\": len(results_df),\n",
    "        \"exec_acc_all\": exec_acc_all,\n",
    "        \"exec_acc_valid\": exec_acc_valid,\n",
    "        \"invalid_sql\": int((results_df['gen_error'] == 'INVALID_SQL').sum()),\n",
    "        \"gen_exec_errors\": int(results_df['gen_error'].notna().sum()),\n",
    "        \"gt_exec_errors\": int(results_df['gt_error'].notna().sum()),\n",
    "        \"output_csv\": str(out_path),\n",
    "    })\n",
    "\n",
    "if not all_results:\n",
    "    raise RuntimeError(\"No model results produced.\")\n",
    "\n",
    "combined_df = pd.concat(all_results, ignore_index=True)\n",
    "combined_df.to_csv(OUTPUT_CSV_ALL, index=False)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "print(\"\\nSummary\")\n",
    "print(summary_df)\n",
    "print(f\"Combined results saved to: {OUTPUT_CSV_ALL}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
