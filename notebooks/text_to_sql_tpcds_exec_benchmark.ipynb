{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d5ef55de",
      "metadata": {},
      "source": [
        "# TPC-DS Text-to-SQL Execution Benchmark (Qwen Baseline)\n",
        "\n",
        "This notebook benchmarks text-to-SQL models on TPC-DS and reports execution accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1253442c",
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/VuThanhLam124/Capstone-NLUS-VDD.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c01030ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "cd Capstone-NLUS-VDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e7a66e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip -q install sqlglot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eb7a961",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import gc\n",
        "import math\n",
        "import unicodedata\n",
        "from decimal import Decimal\n",
        "from datetime import date, datetime\n",
        "\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    for p in [start] + list(start.parents):\n",
        "        if (p / \"research_pipeline\").exists():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "REPO_ROOT = find_repo_root(Path.cwd())\n",
        "DB_PATH = REPO_ROOT / \"research_pipeline\" / \"data\" / \"ecommerce_dw.duckdb\"\n",
        "BENCHMARK_CANDIDATES = [\n",
        "    REPO_ROOT / \"research_pipeline\" / \"data\" / \"test_queries_vi_200_v2.json\",\n",
        "    REPO_ROOT / \"research_pipeline\" / \"data\" / \"test_queries_vi_200.json\",\n",
        "    REPO_ROOT / \"research_pipeline\" / \"test_queries.json\",\n",
        "]\n",
        "OUTPUT_DIR = REPO_ROOT / \"research_pipeline\"\n",
        "RUN_ID = None  # set like \"run1\" or time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "MODEL_CHOICES = {\n",
        "    \"qwen_3_4b_text_to_sql\": {\n",
        "        \"type\": \"lora_causal\",\n",
        "        \"adapter_id\": \"Ellbendls/Qwen-3-4b-Text_to_SQL\",\n",
        "        \"base_id\": \"Qwen/Qwen3-4B-Instruct-2507\",\n",
        "        \"tokenizer_id\": \"Ellbendls/Qwen-3-4b-Text_to_SQL\",\n",
        "        \"allow_vocab_shrink\": True,\n",
        "    },\n",
        "}\n",
        "\n",
        "MODEL_ORDER = [\"qwen_3_4b_text_to_sql\"]\n",
        "RUN_ALL_MODELS = False\n",
        "MODEL_CHOICE = \"qwen_3_4b_text_to_sql\"\n",
        "CONTINUE_ON_ERROR = True\n",
        "\n",
        "MAX_SAMPLES = 200  # set None to run full benchmark\n",
        "SAMPLE_SEED = 42\n",
        "DEFAULT_LIMIT = None  # set to an int to force LIMIT on both GT and generated SQL\n",
        "MAX_TABLES = 8\n",
        "MAX_NEW_TOKENS = 256\n",
        "NUM_BEAMS = 1\n",
        "\n",
        "REPAIR_ON_ERROR = True\n",
        "REPAIR_MAX_ATTEMPTS = 1\n",
        "REPAIR_ONLY_FOR = {\"qwen_3_4b_text_to_sql\"}\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "USE_4BIT = torch.cuda.is_available()\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "\n",
        "def make_output_path(stem: str) -> Path:\n",
        "    suffix = f\"_{RUN_ID}\" if RUN_ID else \"\"\n",
        "    return OUTPUT_DIR / f\"{stem}{suffix}.csv\"\n",
        "\n",
        "OUTPUT_CSV_ALL = make_output_path(\"benchmark_text_to_sql_all\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2ac896a",
      "metadata": {},
      "outputs": [],
      "source": [
        "AUTO_SETUP_DB = True\n",
        "SETUP_SCALE_FACTOR = 1\n",
        "FORCE_RECREATE_DB = False\n",
        "\n",
        "def setup_tpcds_db(db_path: Path, scale_factor: int = 1, force_recreate: bool = False) -> None:\n",
        "    db_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    con = duckdb.connect(str(db_path))\n",
        "    try:\n",
        "        con.execute(\"INSTALL tpcds;\")\n",
        "        con.execute(\"LOAD tpcds;\")\n",
        "\n",
        "        tables = [r[0] for r in con.execute(\"SHOW TABLES\").fetchall()]\n",
        "        if tables and not force_recreate:\n",
        "            print(f\"Found {len(tables)} tables. Skip generation.\")\n",
        "            return\n",
        "\n",
        "        if force_recreate and tables:\n",
        "            for t in tables:\n",
        "                con.execute(f\"DROP TABLE {t}\")\n",
        "\n",
        "        print(f\"Generating TPC-DS (sf={scale_factor})...\")\n",
        "        start = time.time()\n",
        "        con.execute(f\"CALL dsdgen(sf={scale_factor});\")\n",
        "        print(f\"Data generation completed in {time.time() - start:.2f}s\")\n",
        "    finally:\n",
        "        con.close()\n",
        "\n",
        "if not DB_PATH.exists():\n",
        "    if AUTO_SETUP_DB:\n",
        "        setup_tpcds_db(DB_PATH, scale_factor=SETUP_SCALE_FACTOR, force_recreate=FORCE_RECREATE_DB)\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"TPC-DS DuckDB not found: {DB_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a20e73b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "con = duckdb.connect(str(DB_PATH), read_only=True)\n",
        "\n",
        "schema_map = {}\n",
        "for (table_name,) in con.execute(\"SHOW TABLES\").fetchall():\n",
        "    columns = [r[0] for r in con.execute(f\"DESCRIBE {table_name}\").fetchall()]\n",
        "    schema_map[table_name] = columns\n",
        "\n",
        "\n",
        "def strip_accents(text: str) -> str:\n",
        "    return \"\".join(\n",
        "        ch for ch in unicodedata.normalize(\"NFD\", text) if unicodedata.category(ch) != \"Mn\"\n",
        "    )\n",
        "\n",
        "\n",
        "def tokenize(text: str) -> list[str]:\n",
        "    text = strip_accents(text.lower())\n",
        "    raw_tokens = re.findall(r\"[a-z0-9_]+\", text)\n",
        "    tokens = []\n",
        "    for tok in raw_tokens:\n",
        "        tokens.extend(tok.split(\"_\"))\n",
        "    return [t for t in tokens if len(t) > 1]\n",
        "\n",
        "\n",
        "SYNONYMS = {\n",
        "    \"khach\": \"customer\",\n",
        "    \"khachhang\": \"customer\",\n",
        "    \"khach_hang\": \"customer\",\n",
        "    \"khachhangs\": \"customer\",\n",
        "    \"sanpham\": \"item\",\n",
        "    \"san_pham\": \"item\",\n",
        "    \"hang\": \"item\",\n",
        "    \"danhmuc\": \"category\",\n",
        "    \"danh_muc\": \"category\",\n",
        "    \"bang\": \"state\",\n",
        "    \"tinh\": \"state\",\n",
        "    \"cuahang\": \"store\",\n",
        "    \"cua_hang\": \"store\",\n",
        "    \"doanhthu\": \"revenue\",\n",
        "    \"doanh_thu\": \"revenue\",\n",
        "    \"soluong\": \"quantity\",\n",
        "    \"so_luong\": \"quantity\",\n",
        "    \"gia\": \"price\",\n",
        "    \"thang\": \"month\",\n",
        "    \"nam\": \"year\",\n",
        "    \"quy\": \"quarter\",\n",
        "}\n",
        "\n",
        "\n",
        "def expand_tokens(tokens: list[str]) -> set[str]:\n",
        "    expanded = set(tokens)\n",
        "    for tok in list(tokens):\n",
        "        mapped = SYNONYMS.get(tok)\n",
        "        if mapped:\n",
        "            expanded.add(mapped)\n",
        "    return expanded\n",
        "\n",
        "\n",
        "table_tokens = {}\n",
        "for table, cols in schema_map.items():\n",
        "    tokens = set(tokenize(table))\n",
        "    for col in cols:\n",
        "        tokens.update(tokenize(col))\n",
        "    table_tokens[table] = tokens\n",
        "\n",
        "\n",
        "def select_tables_for_question(question: str, max_tables: int = 8) -> list[str]:\n",
        "    q_tokens = expand_tokens(tokenize(question))\n",
        "    scored = []\n",
        "    for table, tokens in table_tokens.items():\n",
        "        score = len(q_tokens & tokens)\n",
        "        scored.append((score, table))\n",
        "    scored.sort(reverse=True)\n",
        "\n",
        "    selected = [t for score, t in scored if score > 0][:max_tables]\n",
        "\n",
        "    def ensure(table: str):\n",
        "        if table in schema_map and table not in selected:\n",
        "            selected.append(table)\n",
        "\n",
        "    if any(tok in q_tokens for tok in {\"year\", \"month\", \"quarter\", \"date\"}):\n",
        "        ensure(\"date_dim\")\n",
        "    if any(tok in q_tokens for tok in {\"customer\"}):\n",
        "        ensure(\"customer\")\n",
        "        ensure(\"customer_address\")\n",
        "    if \"state\" in q_tokens:\n",
        "        ensure(\"customer_address\")\n",
        "        ensure(\"store\")\n",
        "    if \"store\" in q_tokens:\n",
        "        ensure(\"store_sales\")\n",
        "        ensure(\"store\")\n",
        "    if \"web\" in q_tokens:\n",
        "        ensure(\"web_sales\")\n",
        "        ensure(\"web_site\")\n",
        "    if \"catalog\" in q_tokens:\n",
        "        ensure(\"catalog_sales\")\n",
        "        ensure(\"call_center\")\n",
        "    if \"call\" in q_tokens:\n",
        "        ensure(\"call_center\")\n",
        "    if \"inventory\" in q_tokens:\n",
        "        ensure(\"inventory\")\n",
        "    if any(tok in q_tokens for tok in {\"item\", \"product\", \"category\"}):\n",
        "        ensure(\"item\")\n",
        "    if any(tok in q_tokens for tok in {\"sales\", \"revenue\", \"quantity\", \"price\"}):\n",
        "        ensure(\"store_sales\")\n",
        "\n",
        "    return selected[: max_tables or len(selected)]\n",
        "\n",
        "\n",
        "def build_schema_snippet(question: str, max_tables: int = 8) -> tuple[list[str], str]:\n",
        "    tables = select_tables_for_question(question, max_tables=max_tables)\n",
        "    if not tables:\n",
        "        tables = list(schema_map.keys())\n",
        "\n",
        "    lines = []\n",
        "    for table in tables:\n",
        "        cols = schema_map[table]\n",
        "        lines.append(f\"TABLE {table} (\")\n",
        "        for col in cols:\n",
        "            lines.append(f\"  {col}\")\n",
        "        lines.append(\")\")\n",
        "        lines.append(\"\")\n",
        "    return tables, \"\".join(lines).strip()\n",
        "\n",
        "print(f\"Loaded schema for {len(schema_map)} tables.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef8e4f2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "benchmark_path = None\n",
        "for candidate in BENCHMARK_CANDIDATES:\n",
        "    if candidate.exists():\n",
        "        benchmark_path = candidate\n",
        "        break\n",
        "\n",
        "if benchmark_path is None:\n",
        "    raise FileNotFoundError(\"No benchmark JSON found.\")\n",
        "\n",
        "raw_items = json.loads(benchmark_path.read_text())\n",
        "items = []\n",
        "for item in raw_items:\n",
        "    question = item.get(\"text\") or item.get(\"question\")\n",
        "    sql = item.get(\"sql\")\n",
        "    if not question or not sql:\n",
        "        continue\n",
        "    items.append({\n",
        "        \"id\": item.get(\"id\", f\"q{len(items)+1}\"),\n",
        "        \"text\": question,\n",
        "        \"sql\": sql,\n",
        "    })\n",
        "\n",
        "if MAX_SAMPLES:\n",
        "    random.seed(SAMPLE_SEED)\n",
        "    items = random.sample(items, min(MAX_SAMPLES, len(items)))\n",
        "\n",
        "print(f\"Benchmark items: {len(items)} from {benchmark_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6938b0d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import sqlglot\n",
        "except Exception:\n",
        "    sqlglot = None\n",
        "    print(\"sqlglot not installed: SQL normalization will be simple.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d61a6921",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model_and_tokenizer(spec: dict):\n",
        "    model_type = spec[\"type\"]\n",
        "    quant_config = BitsAndBytesConfig(load_in_4bit=True) if USE_4BIT else None\n",
        "\n",
        "    def resolve_tokenizer_and_config(model_id: str, tokenizer_id: str | None = None, allow_shrink: bool = False):\n",
        "        tokenizer_id = tokenizer_id or model_id\n",
        "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_id, use_fast=True, trust_remote_code=True)\n",
        "        config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
        "        if len(tokenizer) != config.vocab_size and (len(tokenizer) > config.vocab_size or allow_shrink):\n",
        "            print(f\"Adjusting vocab_size for {model_id}: {config.vocab_size} -> {len(tokenizer)}\")\n",
        "            config.vocab_size = len(tokenizer)\n",
        "        return tokenizer, config\n",
        "\n",
        "    if model_type == \"seq2seq\":\n",
        "        tokenizer, config = resolve_tokenizer_and_config(\n",
        "            spec[\"id\"], spec.get(\"tokenizer_id\"), spec.get(\"allow_vocab_shrink\", False)\n",
        "        )\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            spec[\"id\"],\n",
        "            config=config,\n",
        "            device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
        "            dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "        model_kind = \"seq2seq\"\n",
        "        model_id = spec[\"id\"]\n",
        "    elif model_type == \"lora_causal\":\n",
        "        from peft import PeftConfig, PeftModel\n",
        "        adapter_id = spec[\"adapter_id\"]\n",
        "        peft_config = PeftConfig.from_pretrained(adapter_id)\n",
        "        base_id = spec.get(\"base_id\") or peft_config.base_model_name_or_path\n",
        "        tokenizer, config = resolve_tokenizer_and_config(\n",
        "            base_id, spec.get(\"tokenizer_id\"), spec.get(\"allow_vocab_shrink\", False)\n",
        "        )\n",
        "        model_kwargs = dict(\n",
        "            config=config,\n",
        "            device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
        "            dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
        "            trust_remote_code=True,\n",
        "            ignore_mismatched_sizes=True,\n",
        "        )\n",
        "        if quant_config is not None:\n",
        "            model_kwargs[\"quantization_config\"] = quant_config\n",
        "        model = AutoModelForCausalLM.from_pretrained(base_id, **model_kwargs)\n",
        "        if spec.get(\"allow_vocab_shrink\", False) and len(tokenizer) != model.get_input_embeddings().weight.shape[0]:\n",
        "            model.resize_token_embeddings(len(tokenizer))\n",
        "        model = PeftModel.from_pretrained(model, adapter_id)\n",
        "        model_kind = \"causal\"\n",
        "        model_id = f\"{base_id} + {adapter_id}\"\n",
        "    else:\n",
        "        tokenizer, config = resolve_tokenizer_and_config(\n",
        "            spec[\"id\"], spec.get(\"tokenizer_id\"), spec.get(\"allow_vocab_shrink\", False)\n",
        "        )\n",
        "        model_kwargs = dict(\n",
        "            config=config,\n",
        "            device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
        "            dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
        "            trust_remote_code=True,\n",
        "            ignore_mismatched_sizes=True,\n",
        "        )\n",
        "        if quant_config is not None:\n",
        "            model_kwargs[\"quantization_config\"] = quant_config\n",
        "        model = AutoModelForCausalLM.from_pretrained(spec[\"id\"], **model_kwargs)\n",
        "        model_kind = \"causal\"\n",
        "        model_id = spec[\"id\"]\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.eval()\n",
        "    return tokenizer, model, model_kind, model_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "045a1eba",
      "metadata": {},
      "outputs": [],
      "source": [
        "_FORBIDDEN_SQL = re.compile(\n",
        "    r\"\\b(INSERT|UPDATE|DELETE|DROP|ALTER|CREATE|COPY|PRAGMA|ATTACH|DETACH|EXPORT|IMPORT|CALL)\\b\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You translate user questions into SQL for DuckDB (TPC-DS). \"\n",
        "    \"Return only SQL, no markdown, no explanations. \"\n",
        "    \"Use only tables and columns from the schema.\"\n",
        ")\n",
        "REPAIR_PROMPT = (\n",
        "    \"You are fixing SQL for DuckDB (TPC-DS). \"\n",
        "    \"Return only corrected SQL, no markdown, no explanations.\"\n",
        ")\n",
        "SEQ2SEQ_PROMPT_TEMPLATE = \"translate to SQL:\\n{question}\\n\\nSCHEMA:\\n{schema}\\n\\nSQL:\"\n",
        "\n",
        "def extract_sql(text: str) -> str:\n",
        "    text = text.strip()\n",
        "    m = re.search(r\"```(?:sql)?\\s*(.*?)```\", text, flags=re.IGNORECASE | re.DOTALL)\n",
        "    if m:\n",
        "        text = m.group(1).strip()\n",
        "    if text.lower().startswith(\"sql:\"):\n",
        "        text = text[4:].strip()\n",
        "    if \";\" in text:\n",
        "        text = text.split(\";\", 1)[0].strip()\n",
        "    return text\n",
        "\n",
        "def is_safe_select(sql: str) -> bool:\n",
        "    s = re.sub(r\"--.*?$\", \"\", sql, flags=re.MULTILINE).strip()\n",
        "    if not s:\n",
        "        return False\n",
        "    if _FORBIDDEN_SQL.search(s):\n",
        "        return False\n",
        "    first = re.split(r\"\\s+\", s, maxsplit=1)[0].upper()\n",
        "    return first in {\"SELECT\", \"WITH\"}\n",
        "\n",
        "def ensure_limit(sql: str, limit: int | None) -> str:\n",
        "    if limit is None:\n",
        "        return sql\n",
        "    s = sql.strip().rstrip(\";\").strip()\n",
        "    if re.search(r\"\\bLIMIT\\b\", s, flags=re.IGNORECASE):\n",
        "        return s\n",
        "    return f\"{s}\\nLIMIT {limit}\"\n",
        "\n",
        "def has_order_by(sql: str) -> bool:\n",
        "    return re.search(r\"\\border\\s+by\\b\", sql, flags=re.IGNORECASE) is not None\n",
        "\n",
        "def normalize_sql(sql: str) -> str:\n",
        "    if sqlglot is not None:\n",
        "        try:\n",
        "            return sqlglot.parse_one(sql, read=\"duckdb\").sql(dialect=\"duckdb\", pretty=False)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return re.sub(r\"\\s+\", \" \", sql.strip()).lower()\n",
        "\n",
        "def build_prompt(question: str, schema_text: str, tokenizer, model_kind: str) -> str:\n",
        "    if model_kind == \"seq2seq\":\n",
        "        return SEQ2SEQ_PROMPT_TEMPLATE.format(question=question, schema=schema_text)\n",
        "    user = f\"SCHEMA:\\n{schema_text}\\n\\nQUESTION:\\n{question}\\n\\nSQL:\"\n",
        "    if getattr(tokenizer, \"chat_template\", None):\n",
        "        return tokenizer.apply_chat_template(\n",
        "            [\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": user},\n",
        "            ],\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "        )\n",
        "    return f\"{SYSTEM_PROMPT}\\n\\n{user}\"\n",
        "\n",
        "def build_repair_prompt(question: str, schema_text: str, bad_sql: str, error: str, tokenizer) -> str:\n",
        "    user = (\n",
        "        f\"SCHEMA:\\n{schema_text}\\n\\nQUESTION:\\n{question}\\n\\n\"\n",
        "        f\"BROKEN_SQL:\\n{bad_sql}\\n\\nERROR:\\n{error}\\n\\nFIXED_SQL:\"\n",
        "    )\n",
        "    if getattr(tokenizer, \"chat_template\", None):\n",
        "        return tokenizer.apply_chat_template(\n",
        "            [\n",
        "                {\"role\": \"system\", \"content\": REPAIR_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": user},\n",
        "            ],\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "        )\n",
        "    return f\"{REPAIR_PROMPT}\\n\\n{user}\"\n",
        "\n",
        "def run_generation(prompt: str, tokenizer, model, model_kind: str) -> str:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "    pad_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            do_sample=False,\n",
        "            num_beams=NUM_BEAMS,\n",
        "            pad_token_id=pad_id,\n",
        "        )\n",
        "    if model_kind == \"seq2seq\":\n",
        "        text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    else:\n",
        "        gen_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
        "        text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
        "    return extract_sql(text)\n",
        "\n",
        "def generate_sql(question: str, schema_text: str, tokenizer, model, model_kind: str) -> str:\n",
        "    prompt = build_prompt(question, schema_text, tokenizer, model_kind)\n",
        "    sql = run_generation(prompt, tokenizer, model, model_kind)\n",
        "    return ensure_limit(sql, DEFAULT_LIMIT)\n",
        "\n",
        "def repair_sql(question: str, schema_text: str, bad_sql: str, error: str, tokenizer, model, model_kind: str) -> str | None:\n",
        "    if model_kind == \"seq2seq\":\n",
        "        return None\n",
        "    prompt = build_repair_prompt(question, schema_text, bad_sql, error, tokenizer)\n",
        "    sql = run_generation(prompt, tokenizer, model, model_kind)\n",
        "    return ensure_limit(sql, DEFAULT_LIMIT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d77884c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_value(v):\n",
        "    if isinstance(v, float):\n",
        "        if math.isnan(v):\n",
        "            return \"nan\"\n",
        "        return round(v, 6)\n",
        "    if isinstance(v, Decimal):\n",
        "        return float(round(v, 6))\n",
        "    if isinstance(v, (datetime, date)):\n",
        "        return v.isoformat()\n",
        "    return v\n",
        "\n",
        "def normalize_rows(rows, keep_order: bool):\n",
        "    if rows is None:\n",
        "        return None\n",
        "    norm = [tuple(normalize_value(x) for x in row) for row in rows]\n",
        "    return norm if keep_order else sorted(norm)\n",
        "\n",
        "def run_sql(con, sql: str):\n",
        "    try:\n",
        "        res = con.execute(sql).fetchall()\n",
        "        return res, None\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n",
        "\n",
        "def classify_error(err: str | None) -> str | None:\n",
        "    if err is None:\n",
        "        return None\n",
        "    if \"Binder Error\" in err:\n",
        "        return \"binder\"\n",
        "    if \"Parser Error\" in err:\n",
        "        return \"parser\"\n",
        "    if \"Catalog Error\" in err:\n",
        "        return \"catalog\"\n",
        "    return \"exec_error\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23be27ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "gt_cache = {}\n",
        "for item in items:\n",
        "    qid = item[\"id\"]\n",
        "    gt_sql = ensure_limit(item[\"sql\"], DEFAULT_LIMIT)\n",
        "    gt_res, gt_err = run_sql(con, gt_sql)\n",
        "    gt_cache[qid] = {\n",
        "        \"sql\": gt_sql,\n",
        "        \"res\": gt_res,\n",
        "        \"err\": gt_err,\n",
        "        \"has_order\": has_order_by(gt_sql),\n",
        "        \"norm_sorted\": normalize_rows(gt_res, keep_order=False) if gt_err is None else None,\n",
        "        \"norm_ordered\": normalize_rows(gt_res, keep_order=True) if gt_err is None else None,\n",
        "    }\n",
        "print(f\"Ground-truth cached: {len(gt_cache)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee4e2254",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_benchmark_for_model(model_choice: str):\n",
        "    spec = MODEL_CHOICES[model_choice]\n",
        "    print(f\"Loading model choice: {model_choice}\")\n",
        "    tokenizer, model, model_kind, model_id = load_model_and_tokenizer(spec)\n",
        "\n",
        "    results = []\n",
        "    for idx, item in enumerate(items, 1):\n",
        "        qid = item[\"id\"]\n",
        "        question = item[\"text\"]\n",
        "        gt = gt_cache[qid]\n",
        "\n",
        "        schema_tables, schema_text = build_schema_snippet(question, max_tables=MAX_TABLES)\n",
        "\n",
        "        start = time.time()\n",
        "        gen_sql = generate_sql(question, schema_text, tokenizer, model, model_kind)\n",
        "        gen_time = time.time() - start\n",
        "\n",
        "        repair_used = False\n",
        "        valid_sql = is_safe_select(gen_sql)\n",
        "        if valid_sql:\n",
        "            exec_start = time.time()\n",
        "            gen_res, gen_err = run_sql(con, gen_sql)\n",
        "            exec_time = time.time() - exec_start\n",
        "        else:\n",
        "            gen_res, gen_err, exec_time = None, \"INVALID_SQL\", None\n",
        "\n",
        "        if (\n",
        "            REPAIR_ON_ERROR\n",
        "            and model_choice in REPAIR_ONLY_FOR\n",
        "            and (not valid_sql or gen_err is not None)\n",
        "            and REPAIR_MAX_ATTEMPTS > 0\n",
        "        ):\n",
        "            repair_sql_text = repair_sql(question, schema_text, gen_sql, gen_err, tokenizer, model, model_kind)\n",
        "            if repair_sql_text:\n",
        "                repair_used = True\n",
        "                gen_sql = repair_sql_text\n",
        "                valid_sql = is_safe_select(gen_sql)\n",
        "                if valid_sql:\n",
        "                    exec_start = time.time()\n",
        "                    gen_res, gen_err = run_sql(con, gen_sql)\n",
        "                    exec_time = time.time() - exec_start\n",
        "                else:\n",
        "                    gen_res, gen_err, exec_time = None, \"INVALID_SQL\", None\n",
        "\n",
        "        exact_match = False\n",
        "        if valid_sql and gen_err is None:\n",
        "            exact_match = normalize_sql(gen_sql) == normalize_sql(gt[\"sql\"])\n",
        "\n",
        "        exec_match = False\n",
        "        if valid_sql and gen_err is None and gt[\"err\"] is None:\n",
        "            keep_order = gt[\"has_order\"] or has_order_by(gen_sql)\n",
        "            gt_norm = gt[\"norm_ordered\"] if keep_order else gt[\"norm_sorted\"]\n",
        "            gen_norm = normalize_rows(gen_res, keep_order=keep_order)\n",
        "            exec_match = gt_norm == gen_norm\n",
        "\n",
        "        results.append({\n",
        "            \"id\": qid,\n",
        "            \"question\": question,\n",
        "            \"gt_sql\": gt[\"sql\"],\n",
        "            \"gen_sql\": gen_sql,\n",
        "            \"valid_sql\": valid_sql,\n",
        "            \"exact_match\": exact_match,\n",
        "            \"exec_match\": exec_match,\n",
        "            \"gen_error\": gen_err,\n",
        "            \"gen_error_type\": classify_error(gen_err),\n",
        "            \"gt_error\": gt[\"err\"],\n",
        "            \"gen_time_sec\": gen_time,\n",
        "            \"exec_time_sec\": exec_time,\n",
        "            \"model_choice\": model_choice,\n",
        "            \"model_id\": model_id,\n",
        "            \"schema_tables\": \",\".join(schema_tables),\n",
        "            \"schema_table_count\": len(schema_tables),\n",
        "            \"repair_used\": repair_used,\n",
        "        })\n",
        "\n",
        "        if idx % 10 == 0:\n",
        "            print(f\"Processed {idx}/{len(items)}\")\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    del model\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a65a56e",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_choices = MODEL_ORDER if RUN_ALL_MODELS else [MODEL_CHOICE]\n",
        "all_results = []\n",
        "summary_rows = []\n",
        "\n",
        "for choice in model_choices:\n",
        "    try:\n",
        "        results_df = run_benchmark_for_model(choice)\n",
        "    except Exception as e:\n",
        "        print(f\"Model {choice} failed: {e}\")\n",
        "        if CONTINUE_ON_ERROR:\n",
        "            continue\n",
        "        raise\n",
        "\n",
        "    out_path = make_output_path(f\"benchmark_text_to_sql_{choice}\")\n",
        "    results_df.to_csv(out_path, index=False)\n",
        "    all_results.append(results_df)\n",
        "\n",
        "    valid_mask = results_df[\"valid_sql\"]\n",
        "    exec_success = results_df[\"gen_error\"].isna()\n",
        "    exec_acc_all = results_df[\"exec_match\"].mean() if not results_df.empty else 0.0\n",
        "    valid_exec_mask = valid_mask & results_df[\"gt_error\"].isna()\n",
        "    exec_acc_valid = results_df.loc[valid_exec_mask, \"exec_match\"].mean() if valid_exec_mask.any() else 0.0\n",
        "    exact_match_rate = results_df.loc[valid_mask, \"exact_match\"].mean() if valid_mask.any() else 0.0\n",
        "\n",
        "    summary_rows.append({\n",
        "        \"model_choice\": choice,\n",
        "        \"model_id\": results_df[\"model_id\"].iloc[0] if not results_df.empty else None,\n",
        "        \"total\": len(results_df),\n",
        "        \"valid_sql_rate\": float(valid_mask.mean()) if not results_df.empty else 0.0,\n",
        "        \"exec_success_rate\": float(exec_success.mean()) if not results_df.empty else 0.0,\n",
        "        \"exec_acc_all\": exec_acc_all,\n",
        "        \"exec_acc_valid\": exec_acc_valid,\n",
        "        \"exact_match_rate\": exact_match_rate,\n",
        "        \"avg_gen_time_sec\": float(results_df[\"gen_time_sec\"].mean()) if not results_df.empty else 0.0,\n",
        "        \"avg_exec_time_sec\": float(results_df[\"exec_time_sec\"].dropna().mean()) if results_df[\"exec_time_sec\"].notna().any() else 0.0,\n",
        "        \"invalid_sql\": int((results_df[\"gen_error\"] == \"INVALID_SQL\").sum()),\n",
        "        \"gen_exec_errors\": int(results_df[\"gen_error\"].notna().sum()),\n",
        "        \"gt_exec_errors\": int(results_df[\"gt_error\"].notna().sum()),\n",
        "        \"output_csv\": str(out_path),\n",
        "    })\n",
        "\n",
        "if not all_results:\n",
        "    raise RuntimeError(\"No model results produced.\")\n",
        "\n",
        "combined_df = pd.concat(all_results, ignore_index=True)\n",
        "combined_df.to_csv(OUTPUT_CSV_ALL, index=False)\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "print(\"Summary\")\n",
        "print(summary_df)\n",
        "print(f\"Combined results saved to: {OUTPUT_CSV_ALL}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36a23d00",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "thesis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}