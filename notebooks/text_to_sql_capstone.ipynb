{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a3192d16-62ac-4273-aecb-b29dc37abe69","cell_type":"markdown","source":"# TPC-DS Text-to-SQL Execution Benchmark\n\nThis notebook evaluates text-to-SQL models on a single TPC-DS benchmark and reports execution accuracy.\n","metadata":{}},{"id":"c2897a1a-84b2-498e-82ba-dded9eb46d1e","cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\nprint(HF_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:04:23.708506Z","iopub.execute_input":"2025-12-27T07:04:23.709334Z","iopub.status.idle":"2025-12-27T07:04:23.876184Z","shell.execute_reply.started":"2025-12-27T07:04:23.709297Z","shell.execute_reply":"2025-12-27T07:04:23.875516Z"}},"outputs":[{"name":"stdout","text":"hf_TOKEN_REDACTED\n","output_type":"stream"}],"execution_count":1},{"id":"fdd9d5dc-c515-4acc-a1d4-677b1b76fd62","cell_type":"code","source":"!git clone https://github.com/VuThanhLam124/Capstone-NLUS-VDD.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:04:23.877436Z","iopub.execute_input":"2025-12-27T07:04:23.877663Z","iopub.status.idle":"2025-12-27T07:04:24.647503Z","shell.execute_reply.started":"2025-12-27T07:04:23.877641Z","shell.execute_reply":"2025-12-27T07:04:24.646813Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'Capstone-NLUS-VDD'...\nremote: Enumerating objects: 93, done.\u001b[K\nremote: Counting objects: 100% (93/93), done.\u001b[K\nremote: Compressing objects: 100% (66/66), done.\u001b[K\nremote: Total 93 (delta 35), reused 78 (delta 23), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (93/93), 341.28 KiB | 11.01 MiB/s, done.\nResolving deltas: 100% (35/35), done.\n","output_type":"stream"}],"execution_count":2},{"id":"6e9143d7","cell_type":"code","source":"cd Capstone-NLUS-VDD","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:04:24.648571Z","iopub.execute_input":"2025-12-27T07:04:24.648857Z","iopub.status.idle":"2025-12-27T07:04:24.654177Z","shell.execute_reply.started":"2025-12-27T07:04:24.648824Z","shell.execute_reply":"2025-12-27T07:04:24.653480Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/Capstone-NLUS-VDD\n","output_type":"stream"}],"execution_count":3},{"id":"e87f77ff","cell_type":"code","source":"!pip install -r requirements.txt\n!pip -q install sqlglot","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:04:24.654991Z","iopub.execute_input":"2025-12-27T07:04:24.655229Z","iopub.status.idle":"2025-12-27T07:04:42.729396Z","shell.execute_reply.started":"2025-12-27T07:04:24.655196Z","shell.execute_reply":"2025-12-27T07:04:42.728423Z"}},"outputs":[{"name":"stdout","text":"Collecting duckdb==1.1.3 (from -r requirements.txt (line 1))\n  Downloading duckdb-1.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (762 bytes)\nCollecting openai-whisper (from -r requirements.txt (line 2))\n  Downloading openai_whisper-20250625.tar.gz (803 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting jiwer (from -r requirements.txt (line 3))\n  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting bitsandbytes (from -r requirements.txt (line 4))\n  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (4.57.1)\nRequirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (0.17.1)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.11.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (1.15.3)\nRequirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (0.13.1)\nRequirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (0.11.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (2.2.2)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (0.9.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (0.2.1)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (2.8.0+cu126)\nCollecting edge-tts (from -r requirements.txt (line 15))\n  Downloading edge_tts-7.2.7-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (0.36.0)\nRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 17)) (1.6.0)\nRequirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 18)) (22.0.0)\nRequirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (10.8.0)\nRequirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (0.60.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (2.0.2)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (0.12.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (2.8.0+cu126)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (4.67.1)\nRequirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (3.4.0)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer->-r requirements.txt (line 3)) (8.3.1)\nCollecting rapidfuzz>=3.9.7 (from jiwer->-r requirements.txt (line 3))\n  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes->-r requirements.txt (line 4)) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (3.20.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (2.32.5)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (0.6.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft->-r requirements.txt (line 6)) (5.9.5)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile->-r requirements.txt (line 9)) (2.0.0)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 10)) (3.0.1)\nRequirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 10)) (1.6.1)\nRequirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 10)) (1.5.3)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 10)) (4.4.2)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 10)) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 10)) (1.0.0)\nRequirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 10)) (4.15.0)\nRequirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 10)) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 10)) (1.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 11)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 11)) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 11)) (2025.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (1.11.1.6)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from edge-tts->-r requirements.txt (line 15)) (3.13.2)\nRequirement already satisfied: certifi>=2023.11.17 in /usr/local/lib/python3.12/dist-packages (from edge-tts->-r requirements.txt (line 15)) (2025.11.12)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->-r requirements.txt (line 16)) (1.2.1rc0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts->-r requirements.txt (line 15)) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts->-r requirements.txt (line 15)) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts->-r requirements.txt (line 15)) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts->-r requirements.txt (line 15)) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts->-r requirements.txt (line 15)) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts->-r requirements.txt (line 15)) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts->-r requirements.txt (line 15)) (1.22.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile->-r requirements.txt (line 9)) (2.23)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper->-r requirements.txt (line 2)) (0.43.0)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa->-r requirements.txt (line 10)) (4.5.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 11)) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 5)) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 5)) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 5)) (2.6.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa->-r requirements.txt (line 10)) (3.6.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper->-r requirements.txt (line 2)) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper->-r requirements.txt (line 2)) (3.0.3)\nDownloading duckdb-1.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading jiwer-4.0.0-py3-none-any.whl (23 kB)\nDownloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading edge_tts-7.2.7-py3-none-any.whl (30 kB)\nDownloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: openai-whisper\n  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=c60a8ef4cc24e84c5417fdc34e327695243ed8e2e4c223f10e2709347313d9a7\n  Stored in directory: /root/.cache/pip/wheels/61/d2/20/09ec9bef734d126cba375b15898010b6cc28578d8afdde5869\nSuccessfully built openai-whisper\nInstalling collected packages: rapidfuzz, duckdb, jiwer, edge-tts, openai-whisper, bitsandbytes\n  Attempting uninstall: duckdb\n    Found existing installation: duckdb 1.3.2\n    Uninstalling duckdb-1.3.2:\n      Successfully uninstalled duckdb-1.3.2\nSuccessfully installed bitsandbytes-0.49.0 duckdb-1.1.3 edge-tts-7.2.7 jiwer-4.0.0 openai-whisper-20250625 rapidfuzz-3.14.3\n","output_type":"stream"}],"execution_count":4},{"id":"e6cc9070","cell_type":"code","source":"from pathlib import Path\nimport json\nimport os\nimport random\nimport time\nimport re\nimport gc\nimport math\nfrom decimal import Decimal\nfrom datetime import date, datetime\n\nimport duckdb\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoConfig, BitsAndBytesConfig\n\n\ndef find_repo_root(start: Path) -> Path:\n    for p in [start] + list(start.parents):\n        if (p / \"research_pipeline\").exists():\n            return p\n    return start\n\nREPO_ROOT = find_repo_root(Path.cwd())\nDB_PATH = REPO_ROOT / \"research_pipeline\" / \"data\" / \"ecommerce_dw.duckdb\"\nPRIMARY_BENCHMARK = REPO_ROOT / \"research_pipeline\" / \"data\" / \"test_queries_vi_200.json\"\nFALLBACK_BENCHMARK = REPO_ROOT / \"research_pipeline\" / \"test_queries.json\"\nOUTPUT_DIR = REPO_ROOT / \"research_pipeline\"\nRUN_ID = None  # set like \"run1\" or time.strftime(\"%Y%m%d_%H%M%S\")\n\nMODEL_CHOICES = {\n    \"qwen_3_4b_text_to_sql\": {\n        \"type\": \"lora_causal\",\n        \"adapter_id\": \"Ellbendls/Qwen-3-4b-Text_to_SQL\",\n        \"base_id\": \"Qwen/Qwen3-4B-Instruct-2507\",\n        \"tokenizer_id\": \"Ellbendls/Qwen-3-4b-Text_to_SQL\",\n        \"allow_vocab_shrink\": True,\n    },\n    \"t5_small_awesome\": {\n        \"type\": \"seq2seq\",\n        \"id\": \"cssupport/t5-small-awesome-text-to-sql\",\n        \"tokenizer_id\": \"cssupport/t5-small-awesome-text-to-sql\",\n    },\n    \"llama3_1_8b_lora\": {\n        \"type\": \"lora_causal\",\n        \"adapter_id\": \"philschmid/code-llama-3-1-8b-text-to-sql-lora\",\n        \"base_id\": \"meta-llama/Meta-Llama-3.1-8B\",\n        \"tokenizer_id\": \"meta-llama/Meta-Llama-3.1-8B\",\n    },\n}\nMODEL_ORDER = [\"qwen_3_4b_text_to_sql\", \"t5_small_awesome\", \"llama3_1_8b_lora\"]\nRUN_ALL_MODELS = True\nMODEL_CHOICE = \"qwen_3_4b_text_to_sql\"\nCONTINUE_ON_ERROR = True\n\nMAX_SAMPLES = 50  # set None to run full benchmark\nSAMPLE_SEED = 42\nDEFAULT_LIMIT = None  # set to an int to force LIMIT on both GT and generated SQL\nMAX_TABLES = None  # set to an int to shorten schema prompt\nMAX_NEW_TOKENS = 256\nNUM_BEAMS = 1\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nUSE_4BIT = torch.cuda.is_available()\nprint(f\"Using device: {DEVICE}\")\n\ndef make_output_path(stem: str) -> Path:\n    suffix = f\"_{RUN_ID}\" if RUN_ID else \"\"\n    return OUTPUT_DIR / f\"{stem}{suffix}.csv\"\n\nOUTPUT_CSV_ALL = make_output_path(\"benchmark_text_to_sql_all\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:04:42.731861Z","iopub.execute_input":"2025-12-27T07:04:42.732162Z","iopub.status.idle":"2025-12-27T07:04:54.010283Z","shell.execute_reply.started":"2025-12-27T07:04:42.732136Z","shell.execute_reply":"2025-12-27T07:04:54.009397Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":5},{"id":"45f6cab2","cell_type":"code","source":"AUTO_SETUP_DB = True\nSETUP_SCALE_FACTOR = 1\nFORCE_RECREATE_DB = False\n\ndef setup_tpcds_db(db_path: Path, scale_factor: int = 1, force_recreate: bool = False) -> None:\n    db_path.parent.mkdir(parents=True, exist_ok=True)\n    con = duckdb.connect(str(db_path))\n    try:\n        con.execute(\"INSTALL tpcds;\")\n        con.execute(\"LOAD tpcds;\")\n\n        tables = [r[0] for r in con.execute(\"SHOW TABLES\").fetchall()]\n        if tables and not force_recreate:\n            print(f\"Found {len(tables)} tables. Skip generation.\")\n            return\n\n        if force_recreate and tables:\n            for t in tables:\n                con.execute(f\"DROP TABLE {t}\")\n\n        print(f\"Generating TPC-DS (sf={scale_factor})...\")\n        start = time.time()\n        con.execute(f\"CALL dsdgen(sf={scale_factor});\")\n        print(f\"Data generation completed in {time.time() - start:.2f}s\")\n    finally:\n        con.close()\n\nif not DB_PATH.exists():\n    if AUTO_SETUP_DB:\n        setup_tpcds_db(DB_PATH, scale_factor=SETUP_SCALE_FACTOR, force_recreate=FORCE_RECREATE_DB)\n    else:\n        raise FileNotFoundError(f\"TPC-DS DuckDB not found: {DB_PATH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:04:54.011260Z","iopub.execute_input":"2025-12-27T07:04:54.011768Z","iopub.status.idle":"2025-12-27T07:05:23.866283Z","shell.execute_reply.started":"2025-12-27T07:04:54.011739Z","shell.execute_reply":"2025-12-27T07:05:23.865467Z"}},"outputs":[{"name":"stdout","text":"Generating TPC-DS (sf=1)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29411f15c323451aa33c5d2b81c9bba1"}},"metadata":{}},{"name":"stdout","text":"Data generation completed in 29.41s\n","output_type":"stream"}],"execution_count":6},{"id":"0b2f69a3","cell_type":"code","source":"con = duckdb.connect(str(DB_PATH), read_only=True)\n\ndef duckdb_schema_prompt(con, *, table_schema: str = \"main\", max_tables: int | None = None) -> str:\n    rows = con.execute(\n        \"\"\"\n        SELECT table_name, column_name, data_type, ordinal_position\n        FROM information_schema.columns\n        WHERE table_schema = ?\n        ORDER BY table_name, ordinal_position\n        \"\"\",\n        [table_schema],\n    ).fetchall()\n\n    tables: dict[str, list[tuple[str, str]]] = {}\n    for table_name, column_name, data_type, _ in rows:\n        tables.setdefault(str(table_name), []).append((str(column_name), str(data_type)))\n\n    table_names = sorted(tables.keys())\n    if max_tables is not None:\n        table_names = table_names[:max_tables]\n\n    lines: list[str] = []\n    for t in table_names:\n        lines.append(f\"TABLE {t} (\")\n        for col, typ in tables[t]:\n            lines.append(f\"  {col} {typ}\")\n        lines.append(\")\")\n        lines.append(\"\")\n    return \"\".join(lines).strip()\n\nschema_text = duckdb_schema_prompt(con, max_tables=MAX_TABLES)\nprint(f\"Schema tables: {schema_text.count('TABLE ')}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:05:23.867406Z","iopub.execute_input":"2025-12-27T07:05:23.867999Z","iopub.status.idle":"2025-12-27T07:05:23.920300Z","shell.execute_reply.started":"2025-12-27T07:05:23.867962Z","shell.execute_reply":"2025-12-27T07:05:23.919543Z"}},"outputs":[{"name":"stdout","text":"Schema tables: 24\n","output_type":"stream"}],"execution_count":7},{"id":"542e7eee","cell_type":"code","source":"if PRIMARY_BENCHMARK.exists():\n    benchmark_path = PRIMARY_BENCHMARK\nelif FALLBACK_BENCHMARK.exists():\n    benchmark_path = FALLBACK_BENCHMARK\nelse:\n    raise FileNotFoundError(\"No benchmark JSON found.\")\n\nraw_items = json.loads(benchmark_path.read_text())\nitems = []\nfor item in raw_items:\n    question = item.get(\"text\") or item.get(\"question\")\n    sql = item.get(\"sql\")\n    if not question or not sql:\n        continue\n    items.append({\n        \"id\": item.get(\"id\", f\"q{len(items)+1}\"),\n        \"text\": question,\n        \"sql\": sql,\n    })\n\nif MAX_SAMPLES:\n    random.seed(SAMPLE_SEED)\n    items = random.sample(items, min(MAX_SAMPLES, len(items)))\n\nprint(f\"Benchmark items: {len(items)} from {benchmark_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:05:23.921316Z","iopub.execute_input":"2025-12-27T07:05:23.921742Z","iopub.status.idle":"2025-12-27T07:05:23.928459Z","shell.execute_reply.started":"2025-12-27T07:05:23.921705Z","shell.execute_reply":"2025-12-27T07:05:23.927702Z"}},"outputs":[{"name":"stdout","text":"Benchmark items: 5 from /kaggle/working/Capstone-NLUS-VDD/research_pipeline/test_queries.json\n","output_type":"stream"}],"execution_count":8},{"id":"c23e611a","cell_type":"code","source":"try:\n    import sqlglot\nexcept Exception:\n    sqlglot = None\n    print(\"sqlglot not installed: SQL normalization will be simple.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:05:23.929423Z","iopub.execute_input":"2025-12-27T07:05:23.929803Z","iopub.status.idle":"2025-12-27T07:05:24.361322Z","shell.execute_reply.started":"2025-12-27T07:05:23.929771Z","shell.execute_reply":"2025-12-27T07:05:24.360691Z"}},"outputs":[],"execution_count":9},{"id":"c590d27e","cell_type":"code","source":"def load_model_and_tokenizer(spec: dict):\n    model_type = spec[\"type\"]\n    quant_config = BitsAndBytesConfig(load_in_4bit=True) if USE_4BIT else None\n\n    def resolve_tokenizer_and_config(model_id: str, tokenizer_id: str | None = None, allow_shrink: bool = False):\n        tokenizer_id = tokenizer_id or model_id\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_id, use_fast=True, trust_remote_code=True)\n        config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n        if len(tokenizer) != config.vocab_size and (len(tokenizer) > config.vocab_size or allow_shrink):\n            print(f\"Extending vocab_size for {model_id}: {config.vocab_size} -> {len(tokenizer)}\")\n            config.vocab_size = len(tokenizer)\n        return tokenizer, config\n\n    if model_type == \"seq2seq\":\n        tokenizer, config = resolve_tokenizer_and_config(spec[\"id\"], spec.get(\"tokenizer_id\", spec.get(\"allow_vocab_shrink\", False)), spec.get(\"allow_vocab_shrink\", False))\n        model = AutoModelForSeq2SeqLM.from_pretrained(\n            spec[\"id\"],\n            config=config,\n            device_map=\"auto\" if DEVICE == \"cuda\" else None,\n            dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n            trust_remote_code=True,\n        )\n        model_kind = \"seq2seq\"\n        model_id = spec[\"id\"]\n    elif model_type == \"lora_causal\":\n        from peft import PeftConfig, PeftModel\n        adapter_id = spec[\"adapter_id\"]\n        peft_config = PeftConfig.from_pretrained(adapter_id)\n        base_id = spec.get(\"base_id\") or peft_config.base_model_name_or_path\n        if base_id and \"meta-llama\" in base_id and not os.environ.get(\"HF_TOKEN\"):\n            raise RuntimeError(f\"HF_TOKEN not set for gated base model: {base_id}. Set HF_TOKEN or disable this model.\")\n        tokenizer, config = resolve_tokenizer_and_config(base_id, spec.get(\"tokenizer_id\", spec.get(\"allow_vocab_shrink\", False)), spec.get(\"allow_vocab_shrink\", False))\n        model_kwargs = dict(\n            config=config,\n            device_map=\"auto\" if DEVICE == \"cuda\" else None,\n            dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n            trust_remote_code=True,\n            ignore_mismatched_sizes=True,\n        )\n        if quant_config is not None:\n            model_kwargs[\"quantization_config\"] = quant_config\n        model = AutoModelForCausalLM.from_pretrained(base_id, **model_kwargs)\n        model = PeftModel.from_pretrained(model, adapter_id)\n        model_kind = \"causal\"\n        model_id = f\"{base_id} + {adapter_id}\"\n    else:\n        tokenizer, config = resolve_tokenizer_and_config(spec[\"id\"], spec.get(\"tokenizer_id\", spec.get(\"allow_vocab_shrink\", False)), spec.get(\"allow_vocab_shrink\", False))\n        model_kwargs = dict(\n            config=config,\n            device_map=\"auto\" if DEVICE == \"cuda\" else None,\n            dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n            trust_remote_code=True,\n            ignore_mismatched_sizes=True,\n        )\n        if quant_config is not None:\n            model_kwargs[\"quantization_config\"] = quant_config\n        model = AutoModelForCausalLM.from_pretrained(spec[\"id\"], **model_kwargs)\n        model_kind = \"causal\"\n        model_id = spec[\"id\"]\n\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    model.eval()\n    return tokenizer, model, model_kind, model_id\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:05:24.362339Z","iopub.execute_input":"2025-12-27T07:05:24.363088Z","iopub.status.idle":"2025-12-27T07:05:24.374649Z","shell.execute_reply.started":"2025-12-27T07:05:24.363049Z","shell.execute_reply":"2025-12-27T07:05:24.373890Z"}},"outputs":[],"execution_count":10},{"id":"62033330","cell_type":"code","source":"_FORBIDDEN_SQL = re.compile(\n    r\"\\b(INSERT|UPDATE|DELETE|DROP|ALTER|CREATE|COPY|PRAGMA|ATTACH|DETACH|EXPORT|IMPORT|CALL)\\b\",\n    re.IGNORECASE,\n)\n\nSYSTEM_PROMPT = (\n    \"You translate user questions into SQL for DuckDB (TPC-DS). \"\n    \"Return only SQL, no markdown, no explanations. \"\n    \"Use only tables and columns from the schema.\"\n)\nSEQ2SEQ_PROMPT_TEMPLATE = \"translate to SQL:\\n{question}\\n\\nSCHEMA:\\n{schema}\\n\\nSQL:\"\n\ndef extract_sql(text: str) -> str:\n    text = text.strip()\n    m = re.search(r\"```(?:sql)?\\s*(.*?)```\", text, flags=re.IGNORECASE | re.DOTALL)\n    if m:\n        text = m.group(1).strip()\n    if text.lower().startswith(\"sql:\"):\n        text = text[4:].strip()\n    if \";\" in text:\n        text = text.split(\";\", 1)[0].strip()\n    return text\n\ndef is_safe_select(sql: str) -> bool:\n    s = re.sub(r\"--.*?$\", \"\", sql, flags=re.MULTILINE).strip()\n    if not s:\n        return False\n    if _FORBIDDEN_SQL.search(s):\n        return False\n    first = re.split(r\"\\s+\", s, maxsplit=1)[0].upper()\n    return first in {\"SELECT\", \"WITH\"}\n\ndef ensure_limit(sql: str, limit: int | None) -> str:\n    if limit is None:\n        return sql\n    s = sql.strip().rstrip(\";\").strip()\n    if re.search(r\"\\bLIMIT\\b\", s, flags=re.IGNORECASE):\n        return s\n    return f\"{s}\\nLIMIT {limit}\"\n\ndef has_order_by(sql: str) -> bool:\n    return re.search(r\"\\border\\s+by\\b\", sql, flags=re.IGNORECASE) is not None\n\ndef normalize_sql(sql: str) -> str:\n    if sqlglot is not None:\n        try:\n            return sqlglot.parse_one(sql, read=\"duckdb\").sql(dialect=\"duckdb\", pretty=False)\n        except Exception:\n            pass\n    return re.sub(r\"\\s+\", \" \", sql.strip()).lower()\n\ndef build_prompt(question: str, schema_text: str, tokenizer, model_kind: str) -> str:\n    if model_kind == \"seq2seq\":\n        return SEQ2SEQ_PROMPT_TEMPLATE.format(question=question, schema=schema_text)\n    user = f\"SCHEMA:\\n{schema_text}\\n\\nQUESTION:\\n{question}\\n\\nSQL:\"\n    if getattr(tokenizer, \"chat_template\", None):\n        return tokenizer.apply_chat_template(\n            [\n                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": user},\n            ],\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n    return f\"{SYSTEM_PROMPT}\\n\\n{user}\"\n\ndef generate_sql(question: str, schema_text: str, tokenizer, model, model_kind: str) -> str:\n    prompt = build_prompt(question, schema_text, tokenizer, model_kind)\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n    pad_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id\n    with torch.no_grad():\n        output_ids = model.generate(\n            **inputs,\n            max_new_tokens=MAX_NEW_TOKENS,\n            do_sample=False,\n            num_beams=NUM_BEAMS,\n            pad_token_id=pad_id,\n        )\n    if model_kind == \"seq2seq\":\n        text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    else:\n        gen_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n        text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n    sql = extract_sql(text)\n    sql = ensure_limit(sql, DEFAULT_LIMIT)\n    return sql\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:05:24.375762Z","iopub.execute_input":"2025-12-27T07:05:24.376075Z","iopub.status.idle":"2025-12-27T07:05:24.392687Z","shell.execute_reply.started":"2025-12-27T07:05:24.376050Z","shell.execute_reply":"2025-12-27T07:05:24.391980Z"}},"outputs":[],"execution_count":11},{"id":"d7c725df-7a1e-46a6-b2c4-c62e2a4eaac2","cell_type":"code","source":"def normalize_value(v):\n    if isinstance(v, float):\n        if math.isnan(v):\n            return \"nan\"\n        return round(v, 6)\n    if isinstance(v, Decimal):\n        return float(round(v, 6))\n    if isinstance(v, (datetime, date)):\n        return v.isoformat()\n    return v\n\ndef normalize_rows(rows, keep_order: bool):\n    if rows is None:\n        return None\n    norm = [tuple(normalize_value(x) for x in row) for row in rows]\n    return norm if keep_order else sorted(norm)\n\ndef run_sql(con, sql: str):\n    try:\n        res = con.execute(sql).fetchall()\n        return res, None\n    except Exception as e:\n        return None, str(e)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:05:24.394337Z","iopub.execute_input":"2025-12-27T07:05:24.394574Z","iopub.status.idle":"2025-12-27T07:05:24.407808Z","shell.execute_reply.started":"2025-12-27T07:05:24.394550Z","shell.execute_reply":"2025-12-27T07:05:24.407211Z"}},"outputs":[],"execution_count":12},{"id":"b175949c-a01c-4c56-8873-c76c2d893e10","cell_type":"code","source":"gt_cache = {}\nfor item in items:\n    qid = item[\"id\"]\n    gt_sql = ensure_limit(item[\"sql\"], DEFAULT_LIMIT)\n    gt_res, gt_err = run_sql(con, gt_sql)\n    gt_cache[qid] = {\n        \"sql\": gt_sql,\n        \"res\": gt_res,\n        \"err\": gt_err,\n        \"has_order\": has_order_by(gt_sql),\n        \"norm_sorted\": normalize_rows(gt_res, keep_order=False) if gt_err is None else None,\n        \"norm_ordered\": normalize_rows(gt_res, keep_order=True) if gt_err is None else None,\n    }\nprint(f\"Ground-truth cached: {len(gt_cache)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:05:24.408837Z","iopub.execute_input":"2025-12-27T07:05:24.409301Z","iopub.status.idle":"2025-12-27T07:05:24.500759Z","shell.execute_reply.started":"2025-12-27T07:05:24.409277Z","shell.execute_reply":"2025-12-27T07:05:24.500032Z"}},"outputs":[{"name":"stdout","text":"Ground-truth cached: 5\n","output_type":"stream"}],"execution_count":13},{"id":"99b73cf8-a9f4-4da2-b54f-165025bd645a","cell_type":"code","source":"def run_benchmark_for_model(model_choice: str):\n    spec = MODEL_CHOICES[model_choice]\n    print(f\"Loading model choice: {model_choice}\")\n    tokenizer, model, model_kind, model_id = load_model_and_tokenizer(spec)\n\n    results = []\n    for idx, item in enumerate(items, 1):\n        qid = item[\"id\"]\n        question = item[\"text\"]\n        gt = gt_cache[qid]\n\n        start = time.time()\n        gen_sql = generate_sql(question, schema_text, tokenizer, model, model_kind)\n        gen_time = time.time() - start\n\n        valid_sql = is_safe_select(gen_sql)\n        if valid_sql:\n            exec_start = time.time()\n            gen_res, gen_err = run_sql(con, gen_sql)\n            exec_time = time.time() - exec_start\n        else:\n            gen_res, gen_err, exec_time = None, \"INVALID_SQL\", None\n\n        exact_match = False\n        if valid_sql and gen_err is None:\n            exact_match = normalize_sql(gen_sql) == normalize_sql(gt[\"sql\"])\n\n        exec_match = False\n        if valid_sql and gen_err is None and gt[\"err\"] is None:\n            keep_order = gt[\"has_order\"] or has_order_by(gen_sql)\n            gt_norm = gt[\"norm_ordered\"] if keep_order else gt[\"norm_sorted\"]\n            gen_norm = normalize_rows(gen_res, keep_order=keep_order)\n            exec_match = gt_norm == gen_norm\n\n        results.append({\n            \"id\": qid,\n            \"question\": question,\n            \"gt_sql\": gt[\"sql\"],\n            \"gen_sql\": gen_sql,\n            \"valid_sql\": valid_sql,\n            \"exact_match\": exact_match,\n            \"exec_match\": exec_match,\n            \"gen_error\": gen_err,\n            \"gt_error\": gt[\"err\"],\n            \"gen_time_sec\": gen_time,\n            \"exec_time_sec\": exec_time,\n            \"model_choice\": model_choice,\n            \"model_id\": model_id,\n        })\n\n        if idx % 10 == 0:\n            print(f\"Processed {idx}/{len(items)}\")\n\n    results_df = pd.DataFrame(results)\n\n    del model\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    return results_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:05:24.503212Z","iopub.execute_input":"2025-12-27T07:05:24.503494Z","iopub.status.idle":"2025-12-27T07:05:24.511454Z","shell.execute_reply.started":"2025-12-27T07:05:24.503469Z","shell.execute_reply":"2025-12-27T07:05:24.510640Z"}},"outputs":[],"execution_count":14},{"id":"a66e3c30-8072-4676-9c28-7d65ba260cf0","cell_type":"code","source":"model_choices = MODEL_ORDER if RUN_ALL_MODELS else [MODEL_CHOICE]\nall_results = []\nsummary_rows = []\n\nfor choice in model_choices:\n    try:\n        results_df = run_benchmark_for_model(choice)\n    except Exception as e:\n        print(f\"Model {choice} failed: {e}\")\n        if CONTINUE_ON_ERROR:\n            continue\n        raise\n\n    out_path = make_output_path(f\"benchmark_text_to_sql_{choice}\")\n    results_df.to_csv(out_path, index=False)\n    all_results.append(results_df)\n\n    valid_mask = results_df[\"valid_sql\"]\n    exec_success = results_df[\"gen_error\"].isna()\n    exec_acc_all = results_df[\"exec_match\"].mean() if not results_df.empty else 0.0\n    valid_exec_mask = valid_mask & results_df[\"gt_error\"].isna()\n    exec_acc_valid = results_df.loc[valid_exec_mask, \"exec_match\"].mean() if valid_exec_mask.any() else 0.0\n    exact_match_rate = results_df.loc[valid_mask, \"exact_match\"].mean() if valid_mask.any() else 0.0\n\n    summary_rows.append({\n        \"model_choice\": choice,\n        \"model_id\": results_df[\"model_id\"].iloc[0] if not results_df.empty else None,\n        \"total\": len(results_df),\n        \"valid_sql_rate\": float(valid_mask.mean()) if not results_df.empty else 0.0,\n        \"exec_success_rate\": float(exec_success.mean()) if not results_df.empty else 0.0,\n        \"exec_acc_all\": exec_acc_all,\n        \"exec_acc_valid\": exec_acc_valid,\n        \"exact_match_rate\": exact_match_rate,\n        \"avg_gen_time_sec\": float(results_df[\"gen_time_sec\"].mean()) if not results_df.empty else 0.0,\n        \"avg_exec_time_sec\": float(results_df[\"exec_time_sec\"].dropna().mean()) if results_df[\"exec_time_sec\"].notna().any() else 0.0,\n        \"invalid_sql\": int((results_df[\"gen_error\"] == \"INVALID_SQL\").sum()),\n        \"gen_exec_errors\": int(results_df[\"gen_error\"].notna().sum()),\n        \"gt_exec_errors\": int(results_df[\"gt_error\"].notna().sum()),\n        \"output_csv\": str(out_path),\n    })\n\nif not all_results:\n    raise RuntimeError(\"No model results produced.\")\n\ncombined_df = pd.concat(all_results, ignore_index=True)\ncombined_df.to_csv(OUTPUT_CSV_ALL, index=False)\n\nsummary_df = pd.DataFrame(summary_rows)\nprint(\"Summary\")\nprint(summary_df)\nprint(f\"Combined results saved to: {OUTPUT_CSV_ALL}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:05:24.512231Z","iopub.execute_input":"2025-12-27T07:05:24.512454Z","iopub.status.idle":"2025-12-27T07:08:08.301302Z","shell.execute_reply.started":"2025-12-27T07:05:24.512433Z","shell.execute_reply":"2025-12-27T07:08:08.300556Z"}},"outputs":[{"name":"stdout","text":"Loading model choice: qwen_3_4b_text_to_sql\n","output_type":"stream"},{"name":"stderr","text":"2025-12-27 07:05:26.724598: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766819126.889665      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766819126.943130      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766819127.345128      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766819127.345171      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766819127.345174      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766819127.345177      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/939 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45ced2d3488e4ba19f3dee51eb5be796"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b234c7a0aac48128b5481e62ace7548"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"499babd9cae3412b8efbcc733c2871af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b1756b3983a459ca36389141f73f282"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ba971b8134e4e80873724bb94603740"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbb2874fa8e9425abb08b59da55a41dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/419 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e9a520982454f22b02a0a51453e11a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja:   0%|          | 0.00/196 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e3118e6bdcd4e85b15b1d3d07c67dd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae57f0c2c6fd4d3cace2c1ba55c9ae95"}},"metadata":{}},{"name":"stdout","text":"Extending vocab_size for Qwen/Qwen3-4B-Instruct-2507: 151936 -> 151669\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a366f1aef323459c960267f109baab6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9e43ffd385e481d80437ff674216f16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76a9af9a85354cce87776374ed5494d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7a5dfd30f8d4a9c8505ec505a90bd03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c62aa3017617433c8df400da7f322db0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3b7103edd5543698b48f9fd6344869d"}},"metadata":{}},{"name":"stderr","text":"Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at Qwen/Qwen3-4B-Instruct-2507 and are newly initialized because the shapes did not match:\n- model.embed_tokens.weight: found shape torch.Size([151936, 2560]) in the checkpoint and torch.Size([151669, 2560]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"084cbe1fd69548b182619afebcffa02f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/3.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"288d3e6828794f3584429b57c42b207e"}},"metadata":{}},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Loading model choice: t5_small_awesome\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd979b6b3bc74d3da271f20258fdc2e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a10f61bf9e24fc49dbfcfabbcbe30ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59a5f3a4dfcb4a4f90bbc6fcfeb7187f"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bf14ff1e47040a39d924237df294fe8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f918904e048f496f85586c0a095ef467"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4300f6530684e148dbab8caec9a0e98"}},"metadata":{}},{"name":"stdout","text":"Loading model choice: llama3_1_8b_lora\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/733 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c22da483369b4fbbb83563dc03c35dc8"}},"metadata":{}},{"name":"stdout","text":"Model llama3_1_8b_lora failed: HF_TOKEN not set for gated base model: meta-llama/Meta-Llama-3.1-8B. Set HF_TOKEN or disable this model.\nSummary\n            model_choice                                           model_id  \\\n0  qwen_3_4b_text_to_sql  Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...   \n1       t5_small_awesome             cssupport/t5-small-awesome-text-to-sql   \n\n   total  valid_sql_rate  exec_success_rate  exec_acc_all  exec_acc_valid  \\\n0      5             1.0                0.6           0.6             0.6   \n1      5             1.0                0.2           0.0             0.0   \n\n   exact_match_rate  avg_gen_time_sec  avg_exec_time_sec  invalid_sql  \\\n0               0.0         12.599303           0.012015            0   \n1               0.0          1.889598           0.000993            0   \n\n   gen_exec_errors  gt_exec_errors  \\\n0                2               0   \n1                4               0   \n\n                                          output_csv  \n0  /kaggle/working/Capstone-NLUS-VDD/research_pip...  \n1  /kaggle/working/Capstone-NLUS-VDD/research_pip...  \nCombined results saved to: /kaggle/working/Capstone-NLUS-VDD/research_pipeline/benchmark_text_to_sql_all.csv\n","output_type":"stream"}],"execution_count":15},{"id":"6930cc00-ba37-4fd9-93a9-5bf020e7c97b","cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv(\"/kaggle/working/Capstone-NLUS-VDD/research_pipeline/benchmark_text_to_sql_all.csv\")\ndata.head(20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:08:53.395303Z","iopub.execute_input":"2025-12-27T07:08:53.396067Z","iopub.status.idle":"2025-12-27T07:08:53.421018Z","shell.execute_reply.started":"2025-12-27T07:08:53.396037Z","shell.execute_reply":"2025-12-27T07:08:53.420417Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"   id                                           question  \\\n0  q1  Find the top 10 electronics items with a price...   \n1  q5  Who are the top 3 customers who spent the most...   \n2  q3  Calculate the total quantity sold and total ne...   \n3  q2  Which are the top 5 states with the highest nu...   \n4  q4  Show me the average sales price for each categ...   \n5  q1  Find the top 10 electronics items with a price...   \n6  q5  Who are the top 3 customers who spent the most...   \n7  q3  Calculate the total quantity sold and total ne...   \n8  q2  Which are the top 5 states with the highest nu...   \n9  q4  Show me the average sales price for each categ...   \n\n                                              gt_sql  \\\n0  SELECT i_item_id, i_item_desc, i_current_price...   \n1  SELECT c_last_name, c_first_name, sum(ss_net_p...   \n2  SELECT sum(ss_quantity) as total_quantity, sum...   \n3  SELECT ca_state, COUNT(*) as customer_count FR...   \n4  SELECT i_category, AVG(ss_sales_price) as avg_...   \n5  SELECT i_item_id, i_item_desc, i_current_price...   \n6  SELECT c_last_name, c_first_name, sum(ss_net_p...   \n7  SELECT sum(ss_quantity) as total_quantity, sum...   \n8  SELECT ca_state, COUNT(*) as customer_count FR...   \n9  SELECT i_category, AVG(ss_sales_price) as avg_...   \n\n                                             gen_sql  valid_sql  exact_match  \\\n0  SELECT i_item_sk, i_item_name, SUM(cs_ext_sale...       True        False   \n1  SELECT c_customer_sk, SUM(ss_net_paid) as tota...       True        False   \n2  SELECT SUM(ss_quantity), SUM(ss_net_profit) FR...       True        False   \n3  SELECT s_state, COUNT(*) as num_customers FROM...       True        False   \n4  SELECT i.i_category, AVG(ss_sales_price) as av...       True        False   \n5  SELECT cc_name, MAX(cc_tax_percentage) FROM ca...       True        False   \n6  SELECT cc_call_center_name, MAX(cc_start_date_...       True        False   \n7  SELECT cc_county, SUM(null) FROM catalog_retur...       True        False   \n8  SELECT cc_county, MAX(cc_county) FROM call_cen...       True        False   \n9  SELECT cc_county, AVG(sales) FROM catalog_retu...       True        False   \n\n   exec_match                                          gen_error  gt_error  \\\n0       False  Binder Error: Referenced column \"i_item_name\" ...       NaN   \n1        True                                                NaN       NaN   \n2        True                                                NaN       NaN   \n3       False  Binder Error: Referenced column \"s_state\" not ...       NaN   \n4        True                                                NaN       NaN   \n5       False       Parser Error: syntax error at or near \"FROM\"       NaN   \n6       False         Parser Error: syntax error at end of input       NaN   \n7       False  Catalog Error: Table with name cc_call_center_...       NaN   \n8       False                                                NaN       NaN   \n9       False  Binder Error: Referenced column \"cc_county\" no...       NaN   \n\n   gen_time_sec  exec_time_sec           model_choice  \\\n0     15.591726       0.001120  qwen_3_4b_text_to_sql   \n1     14.406594       0.006500  qwen_3_4b_text_to_sql   \n2     10.953556       0.002932  qwen_3_4b_text_to_sql   \n3     10.569335       0.000695  qwen_3_4b_text_to_sql   \n4     11.475305       0.048827  qwen_3_4b_text_to_sql   \n5      2.734055       0.000275       t5_small_awesome   \n6      3.076737       0.000239       t5_small_awesome   \n7      2.733557       0.000778       t5_small_awesome   \n8      0.532614       0.003034       t5_small_awesome   \n9      0.371027       0.000637       t5_small_awesome   \n\n                                            model_id  \n0  Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...  \n1  Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...  \n2  Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...  \n3  Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...  \n4  Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...  \n5             cssupport/t5-small-awesome-text-to-sql  \n6             cssupport/t5-small-awesome-text-to-sql  \n7             cssupport/t5-small-awesome-text-to-sql  \n8             cssupport/t5-small-awesome-text-to-sql  \n9             cssupport/t5-small-awesome-text-to-sql  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>question</th>\n      <th>gt_sql</th>\n      <th>gen_sql</th>\n      <th>valid_sql</th>\n      <th>exact_match</th>\n      <th>exec_match</th>\n      <th>gen_error</th>\n      <th>gt_error</th>\n      <th>gen_time_sec</th>\n      <th>exec_time_sec</th>\n      <th>model_choice</th>\n      <th>model_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>q1</td>\n      <td>Find the top 10 electronics items with a price...</td>\n      <td>SELECT i_item_id, i_item_desc, i_current_price...</td>\n      <td>SELECT i_item_sk, i_item_name, SUM(cs_ext_sale...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Binder Error: Referenced column \"i_item_name\" ...</td>\n      <td>NaN</td>\n      <td>15.591726</td>\n      <td>0.001120</td>\n      <td>qwen_3_4b_text_to_sql</td>\n      <td>Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>q5</td>\n      <td>Who are the top 3 customers who spent the most...</td>\n      <td>SELECT c_last_name, c_first_name, sum(ss_net_p...</td>\n      <td>SELECT c_customer_sk, SUM(ss_net_paid) as tota...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>14.406594</td>\n      <td>0.006500</td>\n      <td>qwen_3_4b_text_to_sql</td>\n      <td>Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>q3</td>\n      <td>Calculate the total quantity sold and total ne...</td>\n      <td>SELECT sum(ss_quantity) as total_quantity, sum...</td>\n      <td>SELECT SUM(ss_quantity), SUM(ss_net_profit) FR...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10.953556</td>\n      <td>0.002932</td>\n      <td>qwen_3_4b_text_to_sql</td>\n      <td>Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>q2</td>\n      <td>Which are the top 5 states with the highest nu...</td>\n      <td>SELECT ca_state, COUNT(*) as customer_count FR...</td>\n      <td>SELECT s_state, COUNT(*) as num_customers FROM...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Binder Error: Referenced column \"s_state\" not ...</td>\n      <td>NaN</td>\n      <td>10.569335</td>\n      <td>0.000695</td>\n      <td>qwen_3_4b_text_to_sql</td>\n      <td>Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>q4</td>\n      <td>Show me the average sales price for each categ...</td>\n      <td>SELECT i_category, AVG(ss_sales_price) as avg_...</td>\n      <td>SELECT i.i_category, AVG(ss_sales_price) as av...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11.475305</td>\n      <td>0.048827</td>\n      <td>qwen_3_4b_text_to_sql</td>\n      <td>Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>q1</td>\n      <td>Find the top 10 electronics items with a price...</td>\n      <td>SELECT i_item_id, i_item_desc, i_current_price...</td>\n      <td>SELECT cc_name, MAX(cc_tax_percentage) FROM ca...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Parser Error: syntax error at or near \"FROM\"</td>\n      <td>NaN</td>\n      <td>2.734055</td>\n      <td>0.000275</td>\n      <td>t5_small_awesome</td>\n      <td>cssupport/t5-small-awesome-text-to-sql</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>q5</td>\n      <td>Who are the top 3 customers who spent the most...</td>\n      <td>SELECT c_last_name, c_first_name, sum(ss_net_p...</td>\n      <td>SELECT cc_call_center_name, MAX(cc_start_date_...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Parser Error: syntax error at end of input</td>\n      <td>NaN</td>\n      <td>3.076737</td>\n      <td>0.000239</td>\n      <td>t5_small_awesome</td>\n      <td>cssupport/t5-small-awesome-text-to-sql</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>q3</td>\n      <td>Calculate the total quantity sold and total ne...</td>\n      <td>SELECT sum(ss_quantity) as total_quantity, sum...</td>\n      <td>SELECT cc_county, SUM(null) FROM catalog_retur...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Catalog Error: Table with name cc_call_center_...</td>\n      <td>NaN</td>\n      <td>2.733557</td>\n      <td>0.000778</td>\n      <td>t5_small_awesome</td>\n      <td>cssupport/t5-small-awesome-text-to-sql</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>q2</td>\n      <td>Which are the top 5 states with the highest nu...</td>\n      <td>SELECT ca_state, COUNT(*) as customer_count FR...</td>\n      <td>SELECT cc_county, MAX(cc_county) FROM call_cen...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.532614</td>\n      <td>0.003034</td>\n      <td>t5_small_awesome</td>\n      <td>cssupport/t5-small-awesome-text-to-sql</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>q4</td>\n      <td>Show me the average sales price for each categ...</td>\n      <td>SELECT i_category, AVG(ss_sales_price) as avg_...</td>\n      <td>SELECT cc_county, AVG(sales) FROM catalog_retu...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Binder Error: Referenced column \"cc_county\" no...</td>\n      <td>NaN</td>\n      <td>0.371027</td>\n      <td>0.000637</td>\n      <td>t5_small_awesome</td>\n      <td>cssupport/t5-small-awesome-text-to-sql</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":18},{"id":"03075360-0b79-4fb0-9a88-e75c5f0a4560","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}