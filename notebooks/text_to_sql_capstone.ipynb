{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3192d16-62ac-4273-aecb-b29dc37abe69",
   "metadata": {},
   "source": [
    "# TPC-DS Text-to-SQL Execution Benchmark\n",
    "\n",
    "This notebook evaluates text-to-SQL models on a single TPC-DS benchmark and reports execution accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd9d5dc-c515-4acc-a1d4-677b1b76fd62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T07:04:23.877663Z",
     "iopub.status.busy": "2025-12-27T07:04:23.877436Z",
     "iopub.status.idle": "2025-12-27T07:04:24.647503Z",
     "shell.execute_reply": "2025-12-27T07:04:24.646813Z",
     "shell.execute_reply.started": "2025-12-27T07:04:23.877641Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Capstone-NLUS-VDD'...\n",
      "remote: Enumerating objects: 93, done.\u001b[K\n",
      "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
      "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
      "remote: Total 93 (delta 35), reused 78 (delta 23), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (93/93), 341.28 KiB | 11.01 MiB/s, done.\n",
      "Resolving deltas: 100% (35/35), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/VuThanhLam124/Capstone-NLUS-VDD.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e9143d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T07:04:24.648857Z",
     "iopub.status.busy": "2025-12-27T07:04:24.648571Z",
     "iopub.status.idle": "2025-12-27T07:04:24.654177Z",
     "shell.execute_reply": "2025-12-27T07:04:24.653480Z",
     "shell.execute_reply.started": "2025-12-27T07:04:24.648824Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Capstone-NLUS-VDD\n"
     ]
    }
   ],
   "source": [
    "cd Capstone-NLUS-VDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e87f77ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T07:04:24.655229Z",
     "iopub.status.busy": "2025-12-27T07:04:24.654991Z",
     "iopub.status.idle": "2025-12-27T07:04:42.729396Z",
     "shell.execute_reply": "2025-12-27T07:04:42.728423Z",
     "shell.execute_reply.started": "2025-12-27T07:04:24.655196Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting duckdb==1.1.3 (from -r requirements.txt (line 1))\n",
      "  Downloading duckdb-1.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (762 bytes)\n",
      "Collecting openai-whisper (from -r requirements.txt (line 2))\n",
      "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting jiwer (from -r requirements.txt (line 3))\n",
      "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting bitsandbytes (from -r requirements.txt (line 4))\n",
      "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (4.57.1)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (0.17.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.11.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (1.15.3)\n",
      "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (0.13.1)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (0.11.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (2.2.2)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (0.9.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (0.2.1)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (2.8.0+cu126)\n",
      "Collecting edge-tts (from -r requirements.txt (line 15))\n",
      "  Downloading edge_tts-7.2.7-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (0.36.0)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 17)) (1.6.0)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 18)) (22.0.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (10.8.0)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (0.60.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (2.0.2)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (0.12.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (2.8.0+cu126)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer->-r requirements.txt (line 3)) (8.3.1)\n",
      "Collecting rapidfuzz>=3.9.7 (from jiwer->-r requirements.txt (line 3))\n",
      "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (3.20.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (0.6.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft->-r requirements.txt (line 6)) (5.9.5)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile->-r requirements.txt (line 9)) (2.0.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 10)) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 10)) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 10)) (1.5.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 10)) (4.4.2)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 10)) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 10)) (4.15.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 10)) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 10)) (1.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 11)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 11)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 11)) (2025.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (1.11.1.6)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from edge-tts->-r requirements.txt (line 15)) (3.13.2)\n",
      "Requirement already satisfied: certifi>=2023.11.17 in /usr/local/lib/python3.12/dist-packages (from edge-tts->-r requirements.txt (line 15)) (2025.11.12)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->-r requirements.txt (line 16)) (1.2.1rc0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts->-r requirements.txt (line 15)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts->-r requirements.txt (line 15)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts->-r requirements.txt (line 15)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts->-r requirements.txt (line 15)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts->-r requirements.txt (line 15)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts->-r requirements.txt (line 15)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts->-r requirements.txt (line 15)) (1.22.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile->-r requirements.txt (line 9)) (2.23)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper->-r requirements.txt (line 2)) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa->-r requirements.txt (line 10)) (4.5.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 11)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 5)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 5)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 5)) (2.6.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa->-r requirements.txt (line 10)) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper->-r requirements.txt (line 2)) (3.0.3)\n",
      "Downloading duckdb-1.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
      "Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading edge_tts-7.2.7-py3-none-any.whl (30 kB)\n",
      "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=c60a8ef4cc24e84c5417fdc34e327695243ed8e2e4c223f10e2709347313d9a7\n",
      "  Stored in directory: /root/.cache/pip/wheels/61/d2/20/09ec9bef734d126cba375b15898010b6cc28578d8afdde5869\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: rapidfuzz, duckdb, jiwer, edge-tts, openai-whisper, bitsandbytes\n",
      "  Attempting uninstall: duckdb\n",
      "    Found existing installation: duckdb 1.3.2\n",
      "    Uninstalling duckdb-1.3.2:\n",
      "      Successfully uninstalled duckdb-1.3.2\n",
      "Successfully installed bitsandbytes-0.49.0 duckdb-1.1.3 edge-tts-7.2.7 jiwer-4.0.0 openai-whisper-20250625 rapidfuzz-3.14.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip -q install sqlglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6cc9070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T07:04:42.732162Z",
     "iopub.status.busy": "2025-12-27T07:04:42.731861Z",
     "iopub.status.idle": "2025-12-27T07:04:54.010283Z",
     "shell.execute_reply": "2025-12-27T07:04:54.009397Z",
     "shell.execute_reply.started": "2025-12-27T07:04:42.732136Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import math\n",
    "from decimal import Decimal\n",
    "from datetime import date, datetime\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoConfig, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"research_pipeline\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "DB_PATH = REPO_ROOT / \"research_pipeline\" / \"data\" / \"ecommerce_dw.duckdb\"\n",
    "PRIMARY_BENCHMARK = REPO_ROOT / \"research_pipeline\" / \"data\" / \"test_queries_vi_200.json\"\n",
    "FALLBACK_BENCHMARK = REPO_ROOT / \"research_pipeline\" / \"test_queries.json\"\n",
    "OUTPUT_DIR = REPO_ROOT / \"research_pipeline\"\n",
    "RUN_ID = None  # set like \"run1\" or time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "MODEL_CHOICES = {\n",
    "    \"qwen_3_4b_text_to_sql\": {\n",
    "        \"type\": \"lora_causal\",\n",
    "        \"adapter_id\": \"Ellbendls/Qwen-3-4b-Text_to_SQL\",\n",
    "        \"base_id\": \"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "        \"tokenizer_id\": \"Ellbendls/Qwen-3-4b-Text_to_SQL\",\n",
    "        \"allow_vocab_shrink\": True,\n",
    "    },\n",
    "    \"t5_small_awesome\": {\n",
    "        \"type\": \"seq2seq\",\n",
    "        \"id\": \"cssupport/t5-small-awesome-text-to-sql\",\n",
    "        \"tokenizer_id\": \"cssupport/t5-small-awesome-text-to-sql\",\n",
    "    },\n",
    "    \"llama3_1_8b_lora\": {\n",
    "        \"type\": \"lora_causal\",\n",
    "        \"adapter_id\": \"philschmid/code-llama-3-1-8b-text-to-sql-lora\",\n",
    "        \"base_id\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
    "        \"tokenizer_id\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
    "    },\n",
    "}\n",
    "MODEL_ORDER = [\"qwen_3_4b_text_to_sql\", \"t5_small_awesome\", \"llama3_1_8b_lora\"]\n",
    "RUN_ALL_MODELS = True\n",
    "MODEL_CHOICE = \"qwen_3_4b_text_to_sql\"\n",
    "CONTINUE_ON_ERROR = True\n",
    "\n",
    "MAX_SAMPLES = 50  # set None to run full benchmark\n",
    "SAMPLE_SEED = 42\n",
    "DEFAULT_LIMIT = None  # set to an int to force LIMIT on both GT and generated SQL\n",
    "MAX_TABLES = None  # set to an int to shorten schema prompt\n",
    "MAX_NEW_TOKENS = 256\n",
    "NUM_BEAMS = 1\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "USE_4BIT = torch.cuda.is_available()\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "def make_output_path(stem: str) -> Path:\n",
    "    suffix = f\"_{RUN_ID}\" if RUN_ID else \"\"\n",
    "    return OUTPUT_DIR / f\"{stem}{suffix}.csv\"\n",
    "\n",
    "OUTPUT_CSV_ALL = make_output_path(\"benchmark_text_to_sql_all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45f6cab2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T07:04:54.011768Z",
     "iopub.status.busy": "2025-12-27T07:04:54.011260Z",
     "iopub.status.idle": "2025-12-27T07:05:23.866283Z",
     "shell.execute_reply": "2025-12-27T07:05:23.865467Z",
     "shell.execute_reply.started": "2025-12-27T07:04:54.011739Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating TPC-DS (sf=1)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29411f15c323451aa33c5d2b81c9bba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generation completed in 29.41s\n"
     ]
    }
   ],
   "source": [
    "AUTO_SETUP_DB = True\n",
    "SETUP_SCALE_FACTOR = 1\n",
    "FORCE_RECREATE_DB = False\n",
    "\n",
    "def setup_tpcds_db(db_path: Path, scale_factor: int = 1, force_recreate: bool = False) -> None:\n",
    "    db_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    con = duckdb.connect(str(db_path))\n",
    "    try:\n",
    "        con.execute(\"INSTALL tpcds;\")\n",
    "        con.execute(\"LOAD tpcds;\")\n",
    "\n",
    "        tables = [r[0] for r in con.execute(\"SHOW TABLES\").fetchall()]\n",
    "        if tables and not force_recreate:\n",
    "            print(f\"Found {len(tables)} tables. Skip generation.\")\n",
    "            return\n",
    "\n",
    "        if force_recreate and tables:\n",
    "            for t in tables:\n",
    "                con.execute(f\"DROP TABLE {t}\")\n",
    "\n",
    "        print(f\"Generating TPC-DS (sf={scale_factor})...\")\n",
    "        start = time.time()\n",
    "        con.execute(f\"CALL dsdgen(sf={scale_factor});\")\n",
    "        print(f\"Data generation completed in {time.time() - start:.2f}s\")\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "if not DB_PATH.exists():\n",
    "    if AUTO_SETUP_DB:\n",
    "        setup_tpcds_db(DB_PATH, scale_factor=SETUP_SCALE_FACTOR, force_recreate=FORCE_RECREATE_DB)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"TPC-DS DuckDB not found: {DB_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b2f69a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T07:05:23.867999Z",
     "iopub.status.busy": "2025-12-27T07:05:23.867406Z",
     "iopub.status.idle": "2025-12-27T07:05:23.920300Z",
     "shell.execute_reply": "2025-12-27T07:05:23.919543Z",
     "shell.execute_reply.started": "2025-12-27T07:05:23.867962Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema tables: 24\n"
     ]
    }
   ],
   "source": [
    "con = duckdb.connect(str(DB_PATH), read_only=True)\n",
    "\n",
    "def duckdb_schema_prompt(con, *, table_schema: str = \"main\", max_tables: int | None = None) -> str:\n",
    "    rows = con.execute(\n",
    "        \"\"\"\n",
    "        SELECT table_name, column_name, data_type, ordinal_position\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = ?\n",
    "        ORDER BY table_name, ordinal_position\n",
    "        \"\"\",\n",
    "        [table_schema],\n",
    "    ).fetchall()\n",
    "\n",
    "    tables: dict[str, list[tuple[str, str]]] = {}\n",
    "    for table_name, column_name, data_type, _ in rows:\n",
    "        tables.setdefault(str(table_name), []).append((str(column_name), str(data_type)))\n",
    "\n",
    "    table_names = sorted(tables.keys())\n",
    "    if max_tables is not None:\n",
    "        table_names = table_names[:max_tables]\n",
    "\n",
    "    lines: list[str] = []\n",
    "    for t in table_names:\n",
    "        lines.append(f\"TABLE {t} (\")\n",
    "        for col, typ in tables[t]:\n",
    "            lines.append(f\"  {col} {typ}\")\n",
    "        lines.append(\")\")\n",
    "        lines.append(\"\")\n",
    "    return \"\".join(lines).strip()\n",
    "\n",
    "schema_text = duckdb_schema_prompt(con, max_tables=MAX_TABLES)\n",
    "print(f\"Schema tables: {schema_text.count('TABLE ')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "542e7eee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T07:05:23.921742Z",
     "iopub.status.busy": "2025-12-27T07:05:23.921316Z",
     "iopub.status.idle": "2025-12-27T07:05:23.928459Z",
     "shell.execute_reply": "2025-12-27T07:05:23.927702Z",
     "shell.execute_reply.started": "2025-12-27T07:05:23.921705Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark items: 5 from /kaggle/working/Capstone-NLUS-VDD/research_pipeline/test_queries.json\n"
     ]
    }
   ],
   "source": [
    "if PRIMARY_BENCHMARK.exists():\n",
    "    benchmark_path = PRIMARY_BENCHMARK\n",
    "elif FALLBACK_BENCHMARK.exists():\n",
    "    benchmark_path = FALLBACK_BENCHMARK\n",
    "else:\n",
    "    raise FileNotFoundError(\"No benchmark JSON found.\")\n",
    "\n",
    "raw_items = json.loads(benchmark_path.read_text())\n",
    "items = []\n",
    "for item in raw_items:\n",
    "    question = item.get(\"text\") or item.get(\"question\")\n",
    "    sql = item.get(\"sql\")\n",
    "    if not question or not sql:\n",
    "        continue\n",
    "    items.append({\n",
    "        \"id\": item.get(\"id\", f\"q{len(items)+1}\"),\n",
    "        \"text\": question,\n",
    "        \"sql\": sql,\n",
    "    })\n",
    "\n",
    "if MAX_SAMPLES:\n",
    "    random.seed(SAMPLE_SEED)\n",
    "    items = random.sample(items, min(MAX_SAMPLES, len(items)))\n",
    "\n",
    "print(f\"Benchmark items: {len(items)} from {benchmark_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c23e611a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T07:05:23.929803Z",
     "iopub.status.busy": "2025-12-27T07:05:23.929423Z",
     "iopub.status.idle": "2025-12-27T07:05:24.361322Z",
     "shell.execute_reply": "2025-12-27T07:05:24.360691Z",
     "shell.execute_reply.started": "2025-12-27T07:05:23.929771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import sqlglot\n",
    "except Exception:\n",
    "    sqlglot = None\n",
    "    print(\"sqlglot not installed: SQL normalization will be simple.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c590d27e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T07:05:24.363088Z",
     "iopub.status.busy": "2025-12-27T07:05:24.362339Z",
     "iopub.status.idle": "2025-12-27T07:05:24.374649Z",
     "shell.execute_reply": "2025-12-27T07:05:24.373890Z",
     "shell.execute_reply.started": "2025-12-27T07:05:24.363049Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(spec: dict):\n",
    "    model_type = spec[\"type\"]\n",
    "    quant_config = BitsAndBytesConfig(load_in_4bit=True) if USE_4BIT else None\n",
    "\n",
    "    def resolve_tokenizer_and_config(model_id: str, tokenizer_id: str | None = None, allow_shrink: bool = False):\n",
    "        tokenizer_id = tokenizer_id or model_id\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_id, use_fast=True, trust_remote_code=True)\n",
    "        config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "        if len(tokenizer) != config.vocab_size and (len(tokenizer) > config.vocab_size or allow_shrink):\n",
    "            print(f\"Extending vocab_size for {model_id}: {config.vocab_size} -> {len(tokenizer)}\")\n",
    "            config.vocab_size = len(tokenizer)\n",
    "        return tokenizer, config\n",
    "\n",
    "    if model_type == \"seq2seq\":\n",
    "        tokenizer, config = resolve_tokenizer_and_config(spec[\"id\"], spec.get(\"tokenizer_id\", spec.get(\"allow_vocab_shrink\", False)), spec.get(\"allow_vocab_shrink\", False))\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            spec[\"id\"],\n",
    "            config=config,\n",
    "            device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    "            dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model_kind = \"seq2seq\"\n",
    "        model_id = spec[\"id\"]\n",
    "    elif model_type == \"lora_causal\":\n",
    "        from peft import PeftConfig, PeftModel\n",
    "        adapter_id = spec[\"adapter_id\"]\n",
    "        peft_config = PeftConfig.from_pretrained(adapter_id)\n",
    "        base_id = spec.get(\"base_id\") or peft_config.base_model_name_or_path\n",
    "        if base_id and \"meta-llama\" in base_id and not os.environ.get(\"HF_TOKEN\"):\n",
    "            raise RuntimeError(f\"HF_TOKEN not set for gated base model: {base_id}. Set HF_TOKEN or disable this model.\")\n",
    "        tokenizer, config = resolve_tokenizer_and_config(base_id, spec.get(\"tokenizer_id\", spec.get(\"allow_vocab_shrink\", False)), spec.get(\"allow_vocab_shrink\", False))\n",
    "        model_kwargs = dict(\n",
    "            config=config,\n",
    "            device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    "            dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "            trust_remote_code=True,\n",
    "            ignore_mismatched_sizes=True,\n",
    "        )\n",
    "        if quant_config is not None:\n",
    "            model_kwargs[\"quantization_config\"] = quant_config\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_id, **model_kwargs)\n",
    "        model = PeftModel.from_pretrained(model, adapter_id)\n",
    "        model_kind = \"causal\"\n",
    "        model_id = f\"{base_id} + {adapter_id}\"\n",
    "    else:\n",
    "        tokenizer, config = resolve_tokenizer_and_config(spec[\"id\"], spec.get(\"tokenizer_id\", spec.get(\"allow_vocab_shrink\", False)), spec.get(\"allow_vocab_shrink\", False))\n",
    "        model_kwargs = dict(\n",
    "            config=config,\n",
    "            device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    "            dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "            trust_remote_code=True,\n",
    "            ignore_mismatched_sizes=True,\n",
    "        )\n",
    "        if quant_config is not None:\n",
    "            model_kwargs[\"quantization_config\"] = quant_config\n",
    "        model = AutoModelForCausalLM.from_pretrained(spec[\"id\"], **model_kwargs)\n",
    "        model_kind = \"causal\"\n",
    "        model_id = spec[\"id\"]\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.eval()\n",
    "    return tokenizer, model, model_kind, model_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62033330",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T07:05:24.376075Z",
     "iopub.status.busy": "2025-12-27T07:05:24.375762Z",
     "iopub.status.idle": "2025-12-27T07:05:24.392687Z",
     "shell.execute_reply": "2025-12-27T07:05:24.391980Z",
     "shell.execute_reply.started": "2025-12-27T07:05:24.376050Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "_FORBIDDEN_SQL = re.compile(\n",
    "    r\"\\b(INSERT|UPDATE|DELETE|DROP|ALTER|CREATE|COPY|PRAGMA|ATTACH|DETACH|EXPORT|IMPORT|CALL)\\b\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You translate user questions into SQL for DuckDB (TPC-DS). \"\n",
    "    \"Return only SQL, no markdown, no explanations. \"\n",
    "    \"Use only tables and columns from the schema.\"\n",
    ")\n",
    "SEQ2SEQ_PROMPT_TEMPLATE = \"translate to SQL:\\n{question}\\n\\nSCHEMA:\\n{schema}\\n\\nSQL:\"\n",
    "\n",
    "def extract_sql(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    m = re.search(r\"```(?:sql)?\\s*(.*?)```\", text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    if m:\n",
    "        text = m.group(1).strip()\n",
    "    if text.lower().startswith(\"sql:\"):\n",
    "        text = text[4:].strip()\n",
    "    if \";\" in text:\n",
    "        text = text.split(\";\", 1)[0].strip()\n",
    "    return text\n",
    "\n",
    "def is_safe_select(sql: str) -> bool:\n",
    "    s = re.sub(r\"--.*?$\", \"\", sql, flags=re.MULTILINE).strip()\n",
    "    if not s:\n",
    "        return False\n",
    "    if _FORBIDDEN_SQL.search(s):\n",
    "        return False\n",
    "    first = re.split(r\"\\s+\", s, maxsplit=1)[0].upper()\n",
    "    return first in {\"SELECT\", \"WITH\"}\n",
    "\n",
    "def ensure_limit(sql: str, limit: int | None) -> str:\n",
    "    if limit is None:\n",
    "        return sql\n",
    "    s = sql.strip().rstrip(\";\").strip()\n",
    "    if re.search(r\"\\bLIMIT\\b\", s, flags=re.IGNORECASE):\n",
    "        return s\n",
    "    return f\"{s}\\nLIMIT {limit}\"\n",
    "\n",
    "def has_order_by(sql: str) -> bool:\n",
    "    return re.search(r\"\\border\\s+by\\b\", sql, flags=re.IGNORECASE) is not None\n",
    "\n",
    "def normalize_sql(sql: str) -> str:\n",
    "    if sqlglot is not None:\n",
    "        try:\n",
    "            return sqlglot.parse_one(sql, read=\"duckdb\").sql(dialect=\"duckdb\", pretty=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return re.sub(r\"\\s+\", \" \", sql.strip()).lower()\n",
    "\n",
    "def build_prompt(question: str, schema_text: str, tokenizer, model_kind: str) -> str:\n",
    "    if model_kind == \"seq2seq\":\n",
    "        return SEQ2SEQ_PROMPT_TEMPLATE.format(question=question, schema=schema_text)\n",
    "    user = f\"SCHEMA:\\n{schema_text}\\n\\nQUESTION:\\n{question}\\n\\nSQL:\"\n",
    "    if getattr(tokenizer, \"chat_template\", None):\n",
    "        return tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": user},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    return f\"{SYSTEM_PROMPT}\\n\\n{user}\"\n",
    "\n",
    "def generate_sql(question: str, schema_text: str, tokenizer, model, model_kind: str) -> str:\n",
    "    prompt = build_prompt(question, schema_text, tokenizer, model_kind)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    pad_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=False,\n",
    "            num_beams=NUM_BEAMS,\n",
    "            pad_token_id=pad_id,\n",
    "        )\n",
    "    if model_kind == \"seq2seq\":\n",
    "        text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    else:\n",
    "        gen_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "    sql = extract_sql(text)\n",
    "    sql = ensure_limit(sql, DEFAULT_LIMIT)\n",
    "    return sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7c725df-7a1e-46a6-b2c4-c62e2a4eaac2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T07:05:24.394574Z",
     "iopub.status.busy": "2025-12-27T07:05:24.394337Z",
     "iopub.status.idle": "2025-12-27T07:05:24.407808Z",
     "shell.execute_reply": "2025-12-27T07:05:24.407211Z",
     "shell.execute_reply.started": "2025-12-27T07:05:24.394550Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_value(v):\n",
    "    if isinstance(v, float):\n",
    "        if math.isnan(v):\n",
    "            return \"nan\"\n",
    "        return round(v, 6)\n",
    "    if isinstance(v, Decimal):\n",
    "        return float(round(v, 6))\n",
    "    if isinstance(v, (datetime, date)):\n",
    "        return v.isoformat()\n",
    "    return v\n",
    "\n",
    "def normalize_rows(rows, keep_order: bool):\n",
    "    if rows is None:\n",
    "        return None\n",
    "    norm = [tuple(normalize_value(x) for x in row) for row in rows]\n",
    "    return norm if keep_order else sorted(norm)\n",
    "\n",
    "def run_sql(con, sql: str):\n",
    "    try:\n",
    "        res = con.execute(sql).fetchall()\n",
    "        return res, None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b175949c-a01c-4c56-8873-c76c2d893e10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T07:05:24.409301Z",
     "iopub.status.busy": "2025-12-27T07:05:24.408837Z",
     "iopub.status.idle": "2025-12-27T07:05:24.500759Z",
     "shell.execute_reply": "2025-12-27T07:05:24.500032Z",
     "shell.execute_reply.started": "2025-12-27T07:05:24.409277Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground-truth cached: 5\n"
     ]
    }
   ],
   "source": [
    "gt_cache = {}\n",
    "for item in items:\n",
    "    qid = item[\"id\"]\n",
    "    gt_sql = ensure_limit(item[\"sql\"], DEFAULT_LIMIT)\n",
    "    gt_res, gt_err = run_sql(con, gt_sql)\n",
    "    gt_cache[qid] = {\n",
    "        \"sql\": gt_sql,\n",
    "        \"res\": gt_res,\n",
    "        \"err\": gt_err,\n",
    "        \"has_order\": has_order_by(gt_sql),\n",
    "        \"norm_sorted\": normalize_rows(gt_res, keep_order=False) if gt_err is None else None,\n",
    "        \"norm_ordered\": normalize_rows(gt_res, keep_order=True) if gt_err is None else None,\n",
    "    }\n",
    "print(f\"Ground-truth cached: {len(gt_cache)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99b73cf8-a9f4-4da2-b54f-165025bd645a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T07:05:24.503494Z",
     "iopub.status.busy": "2025-12-27T07:05:24.503212Z",
     "iopub.status.idle": "2025-12-27T07:05:24.511454Z",
     "shell.execute_reply": "2025-12-27T07:05:24.510640Z",
     "shell.execute_reply.started": "2025-12-27T07:05:24.503469Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_benchmark_for_model(model_choice: str):\n",
    "    spec = MODEL_CHOICES[model_choice]\n",
    "    print(f\"Loading model choice: {model_choice}\")\n",
    "    tokenizer, model, model_kind, model_id = load_model_and_tokenizer(spec)\n",
    "\n",
    "    results = []\n",
    "    for idx, item in enumerate(items, 1):\n",
    "        qid = item[\"id\"]\n",
    "        question = item[\"text\"]\n",
    "        gt = gt_cache[qid]\n",
    "\n",
    "        start = time.time()\n",
    "        gen_sql = generate_sql(question, schema_text, tokenizer, model, model_kind)\n",
    "        gen_time = time.time() - start\n",
    "\n",
    "        valid_sql = is_safe_select(gen_sql)\n",
    "        if valid_sql:\n",
    "            exec_start = time.time()\n",
    "            gen_res, gen_err = run_sql(con, gen_sql)\n",
    "            exec_time = time.time() - exec_start\n",
    "        else:\n",
    "            gen_res, gen_err, exec_time = None, \"INVALID_SQL\", None\n",
    "\n",
    "        exact_match = False\n",
    "        if valid_sql and gen_err is None:\n",
    "            exact_match = normalize_sql(gen_sql) == normalize_sql(gt[\"sql\"])\n",
    "\n",
    "        exec_match = False\n",
    "        if valid_sql and gen_err is None and gt[\"err\"] is None:\n",
    "            keep_order = gt[\"has_order\"] or has_order_by(gen_sql)\n",
    "            gt_norm = gt[\"norm_ordered\"] if keep_order else gt[\"norm_sorted\"]\n",
    "            gen_norm = normalize_rows(gen_res, keep_order=keep_order)\n",
    "            exec_match = gt_norm == gen_norm\n",
    "\n",
    "        results.append({\n",
    "            \"id\": qid,\n",
    "            \"question\": question,\n",
    "            \"gt_sql\": gt[\"sql\"],\n",
    "            \"gen_sql\": gen_sql,\n",
    "            \"valid_sql\": valid_sql,\n",
    "            \"exact_match\": exact_match,\n",
    "            \"exec_match\": exec_match,\n",
    "            \"gen_error\": gen_err,\n",
    "            \"gt_error\": gt[\"err\"],\n",
    "            \"gen_time_sec\": gen_time,\n",
    "            \"exec_time_sec\": exec_time,\n",
    "            \"model_choice\": model_choice,\n",
    "            \"model_id\": model_id,\n",
    "        })\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"Processed {idx}/{len(items)}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a66e3c30-8072-4676-9c28-7d65ba260cf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T07:05:24.512454Z",
     "iopub.status.busy": "2025-12-27T07:05:24.512231Z",
     "iopub.status.idle": "2025-12-27T07:08:08.301302Z",
     "shell.execute_reply": "2025-12-27T07:08:08.300556Z",
     "shell.execute_reply.started": "2025-12-27T07:05:24.512433Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model choice: qwen_3_4b_text_to_sql\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 07:05:26.724598: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1766819126.889665      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1766819126.943130      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1766819127.345128      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766819127.345171      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766819127.345174      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766819127.345177      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ced2d3488e4ba19f3dee51eb5be796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/939 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b234c7a0aac48128b5481e62ace7548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499babd9cae3412b8efbcc733c2871af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1756b3983a459ca36389141f73f282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba971b8134e4e80873724bb94603740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb2874fa8e9425abb08b59da55a41dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9a520982454f22b02a0a51453e11a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/419 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3118e6bdcd4e85b15b1d3d07c67dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/196 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae57f0c2c6fd4d3cace2c1ba55c9ae95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending vocab_size for Qwen/Qwen3-4B-Instruct-2507: 151936 -> 151669\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a366f1aef323459c960267f109baab6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e43ffd385e481d80437ff674216f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a9af9a85354cce87776374ed5494d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a5dfd30f8d4a9c8505ec505a90bd03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62aa3017617433c8df400da7f322db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b7103edd5543698b48f9fd6344869d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at Qwen/Qwen3-4B-Instruct-2507 and are newly initialized because the shapes did not match:\n",
      "- model.embed_tokens.weight: found shape torch.Size([151936, 2560]) in the checkpoint and torch.Size([151669, 2560]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084cbe1fd69548b182619afebcffa02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288d3e6828794f3584429b57c42b207e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/3.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model choice: t5_small_awesome\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd979b6b3bc74d3da271f20258fdc2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a10f61bf9e24fc49dbfcfabbcbe30ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a5f3a4dfcb4a4f90bbc6fcfeb7187f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf14ff1e47040a39d924237df294fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f918904e048f496f85586c0a095ef467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4300f6530684e148dbab8caec9a0e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model choice: llama3_1_8b_lora\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22da483369b4fbbb83563dc03c35dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/733 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model llama3_1_8b_lora failed: HF_TOKEN not set for gated base model: meta-llama/Meta-Llama-3.1-8B. Set HF_TOKEN or disable this model.\n",
      "Summary\n",
      "            model_choice                                           model_id  \\\n",
      "0  qwen_3_4b_text_to_sql  Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...   \n",
      "1       t5_small_awesome             cssupport/t5-small-awesome-text-to-sql   \n",
      "\n",
      "   total  valid_sql_rate  exec_success_rate  exec_acc_all  exec_acc_valid  \\\n",
      "0      5             1.0                0.6           0.6             0.6   \n",
      "1      5             1.0                0.2           0.0             0.0   \n",
      "\n",
      "   exact_match_rate  avg_gen_time_sec  avg_exec_time_sec  invalid_sql  \\\n",
      "0               0.0         12.599303           0.012015            0   \n",
      "1               0.0          1.889598           0.000993            0   \n",
      "\n",
      "   gen_exec_errors  gt_exec_errors  \\\n",
      "0                2               0   \n",
      "1                4               0   \n",
      "\n",
      "                                          output_csv  \n",
      "0  /kaggle/working/Capstone-NLUS-VDD/research_pip...  \n",
      "1  /kaggle/working/Capstone-NLUS-VDD/research_pip...  \n",
      "Combined results saved to: /kaggle/working/Capstone-NLUS-VDD/research_pipeline/benchmark_text_to_sql_all.csv\n"
     ]
    }
   ],
   "source": [
    "model_choices = MODEL_ORDER if RUN_ALL_MODELS else [MODEL_CHOICE]\n",
    "all_results = []\n",
    "summary_rows = []\n",
    "\n",
    "for choice in model_choices:\n",
    "    try:\n",
    "        results_df = run_benchmark_for_model(choice)\n",
    "    except Exception as e:\n",
    "        print(f\"Model {choice} failed: {e}\")\n",
    "        if CONTINUE_ON_ERROR:\n",
    "            continue\n",
    "        raise\n",
    "\n",
    "    out_path = make_output_path(f\"benchmark_text_to_sql_{choice}\")\n",
    "    results_df.to_csv(out_path, index=False)\n",
    "    all_results.append(results_df)\n",
    "\n",
    "    valid_mask = results_df[\"valid_sql\"]\n",
    "    exec_success = results_df[\"gen_error\"].isna()\n",
    "    exec_acc_all = results_df[\"exec_match\"].mean() if not results_df.empty else 0.0\n",
    "    valid_exec_mask = valid_mask & results_df[\"gt_error\"].isna()\n",
    "    exec_acc_valid = results_df.loc[valid_exec_mask, \"exec_match\"].mean() if valid_exec_mask.any() else 0.0\n",
    "    exact_match_rate = results_df.loc[valid_mask, \"exact_match\"].mean() if valid_mask.any() else 0.0\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"model_choice\": choice,\n",
    "        \"model_id\": results_df[\"model_id\"].iloc[0] if not results_df.empty else None,\n",
    "        \"total\": len(results_df),\n",
    "        \"valid_sql_rate\": float(valid_mask.mean()) if not results_df.empty else 0.0,\n",
    "        \"exec_success_rate\": float(exec_success.mean()) if not results_df.empty else 0.0,\n",
    "        \"exec_acc_all\": exec_acc_all,\n",
    "        \"exec_acc_valid\": exec_acc_valid,\n",
    "        \"exact_match_rate\": exact_match_rate,\n",
    "        \"avg_gen_time_sec\": float(results_df[\"gen_time_sec\"].mean()) if not results_df.empty else 0.0,\n",
    "        \"avg_exec_time_sec\": float(results_df[\"exec_time_sec\"].dropna().mean()) if results_df[\"exec_time_sec\"].notna().any() else 0.0,\n",
    "        \"invalid_sql\": int((results_df[\"gen_error\"] == \"INVALID_SQL\").sum()),\n",
    "        \"gen_exec_errors\": int(results_df[\"gen_error\"].notna().sum()),\n",
    "        \"gt_exec_errors\": int(results_df[\"gt_error\"].notna().sum()),\n",
    "        \"output_csv\": str(out_path),\n",
    "    })\n",
    "\n",
    "if not all_results:\n",
    "    raise RuntimeError(\"No model results produced.\")\n",
    "\n",
    "combined_df = pd.concat(all_results, ignore_index=True)\n",
    "combined_df.to_csv(OUTPUT_CSV_ALL, index=False)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "print(\"Summary\")\n",
    "print(summary_df)\n",
    "print(f\"Combined results saved to: {OUTPUT_CSV_ALL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6930cc00-ba37-4fd9-93a9-5bf020e7c97b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T07:08:53.396067Z",
     "iopub.status.busy": "2025-12-27T07:08:53.395303Z",
     "iopub.status.idle": "2025-12-27T07:08:53.421018Z",
     "shell.execute_reply": "2025-12-27T07:08:53.420417Z",
     "shell.execute_reply.started": "2025-12-27T07:08:53.396037Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>gt_sql</th>\n",
       "      <th>gen_sql</th>\n",
       "      <th>valid_sql</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>exec_match</th>\n",
       "      <th>gen_error</th>\n",
       "      <th>gt_error</th>\n",
       "      <th>gen_time_sec</th>\n",
       "      <th>exec_time_sec</th>\n",
       "      <th>model_choice</th>\n",
       "      <th>model_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q1</td>\n",
       "      <td>Find the top 10 electronics items with a price...</td>\n",
       "      <td>SELECT i_item_id, i_item_desc, i_current_price...</td>\n",
       "      <td>SELECT i_item_sk, i_item_name, SUM(cs_ext_sale...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Binder Error: Referenced column \"i_item_name\" ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.591726</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>qwen_3_4b_text_to_sql</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q5</td>\n",
       "      <td>Who are the top 3 customers who spent the most...</td>\n",
       "      <td>SELECT c_last_name, c_first_name, sum(ss_net_p...</td>\n",
       "      <td>SELECT c_customer_sk, SUM(ss_net_paid) as tota...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.406594</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>qwen_3_4b_text_to_sql</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q3</td>\n",
       "      <td>Calculate the total quantity sold and total ne...</td>\n",
       "      <td>SELECT sum(ss_quantity) as total_quantity, sum...</td>\n",
       "      <td>SELECT SUM(ss_quantity), SUM(ss_net_profit) FR...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.953556</td>\n",
       "      <td>0.002932</td>\n",
       "      <td>qwen_3_4b_text_to_sql</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q2</td>\n",
       "      <td>Which are the top 5 states with the highest nu...</td>\n",
       "      <td>SELECT ca_state, COUNT(*) as customer_count FR...</td>\n",
       "      <td>SELECT s_state, COUNT(*) as num_customers FROM...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Binder Error: Referenced column \"s_state\" not ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.569335</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>qwen_3_4b_text_to_sql</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q4</td>\n",
       "      <td>Show me the average sales price for each categ...</td>\n",
       "      <td>SELECT i_category, AVG(ss_sales_price) as avg_...</td>\n",
       "      <td>SELECT i.i_category, AVG(ss_sales_price) as av...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.475305</td>\n",
       "      <td>0.048827</td>\n",
       "      <td>qwen_3_4b_text_to_sql</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>q1</td>\n",
       "      <td>Find the top 10 electronics items with a price...</td>\n",
       "      <td>SELECT i_item_id, i_item_desc, i_current_price...</td>\n",
       "      <td>SELECT cc_name, MAX(cc_tax_percentage) FROM ca...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Parser Error: syntax error at or near \"FROM\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.734055</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>t5_small_awesome</td>\n",
       "      <td>cssupport/t5-small-awesome-text-to-sql</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>q5</td>\n",
       "      <td>Who are the top 3 customers who spent the most...</td>\n",
       "      <td>SELECT c_last_name, c_first_name, sum(ss_net_p...</td>\n",
       "      <td>SELECT cc_call_center_name, MAX(cc_start_date_...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Parser Error: syntax error at end of input</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.076737</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>t5_small_awesome</td>\n",
       "      <td>cssupport/t5-small-awesome-text-to-sql</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>q3</td>\n",
       "      <td>Calculate the total quantity sold and total ne...</td>\n",
       "      <td>SELECT sum(ss_quantity) as total_quantity, sum...</td>\n",
       "      <td>SELECT cc_county, SUM(null) FROM catalog_retur...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Catalog Error: Table with name cc_call_center_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.733557</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>t5_small_awesome</td>\n",
       "      <td>cssupport/t5-small-awesome-text-to-sql</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>q2</td>\n",
       "      <td>Which are the top 5 states with the highest nu...</td>\n",
       "      <td>SELECT ca_state, COUNT(*) as customer_count FR...</td>\n",
       "      <td>SELECT cc_county, MAX(cc_county) FROM call_cen...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.532614</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>t5_small_awesome</td>\n",
       "      <td>cssupport/t5-small-awesome-text-to-sql</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>q4</td>\n",
       "      <td>Show me the average sales price for each categ...</td>\n",
       "      <td>SELECT i_category, AVG(ss_sales_price) as avg_...</td>\n",
       "      <td>SELECT cc_county, AVG(sales) FROM catalog_retu...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Binder Error: Referenced column \"cc_county\" no...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.371027</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>t5_small_awesome</td>\n",
       "      <td>cssupport/t5-small-awesome-text-to-sql</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           question  \\\n",
       "0  q1  Find the top 10 electronics items with a price...   \n",
       "1  q5  Who are the top 3 customers who spent the most...   \n",
       "2  q3  Calculate the total quantity sold and total ne...   \n",
       "3  q2  Which are the top 5 states with the highest nu...   \n",
       "4  q4  Show me the average sales price for each categ...   \n",
       "5  q1  Find the top 10 electronics items with a price...   \n",
       "6  q5  Who are the top 3 customers who spent the most...   \n",
       "7  q3  Calculate the total quantity sold and total ne...   \n",
       "8  q2  Which are the top 5 states with the highest nu...   \n",
       "9  q4  Show me the average sales price for each categ...   \n",
       "\n",
       "                                              gt_sql  \\\n",
       "0  SELECT i_item_id, i_item_desc, i_current_price...   \n",
       "1  SELECT c_last_name, c_first_name, sum(ss_net_p...   \n",
       "2  SELECT sum(ss_quantity) as total_quantity, sum...   \n",
       "3  SELECT ca_state, COUNT(*) as customer_count FR...   \n",
       "4  SELECT i_category, AVG(ss_sales_price) as avg_...   \n",
       "5  SELECT i_item_id, i_item_desc, i_current_price...   \n",
       "6  SELECT c_last_name, c_first_name, sum(ss_net_p...   \n",
       "7  SELECT sum(ss_quantity) as total_quantity, sum...   \n",
       "8  SELECT ca_state, COUNT(*) as customer_count FR...   \n",
       "9  SELECT i_category, AVG(ss_sales_price) as avg_...   \n",
       "\n",
       "                                             gen_sql  valid_sql  exact_match  \\\n",
       "0  SELECT i_item_sk, i_item_name, SUM(cs_ext_sale...       True        False   \n",
       "1  SELECT c_customer_sk, SUM(ss_net_paid) as tota...       True        False   \n",
       "2  SELECT SUM(ss_quantity), SUM(ss_net_profit) FR...       True        False   \n",
       "3  SELECT s_state, COUNT(*) as num_customers FROM...       True        False   \n",
       "4  SELECT i.i_category, AVG(ss_sales_price) as av...       True        False   \n",
       "5  SELECT cc_name, MAX(cc_tax_percentage) FROM ca...       True        False   \n",
       "6  SELECT cc_call_center_name, MAX(cc_start_date_...       True        False   \n",
       "7  SELECT cc_county, SUM(null) FROM catalog_retur...       True        False   \n",
       "8  SELECT cc_county, MAX(cc_county) FROM call_cen...       True        False   \n",
       "9  SELECT cc_county, AVG(sales) FROM catalog_retu...       True        False   \n",
       "\n",
       "   exec_match                                          gen_error  gt_error  \\\n",
       "0       False  Binder Error: Referenced column \"i_item_name\" ...       NaN   \n",
       "1        True                                                NaN       NaN   \n",
       "2        True                                                NaN       NaN   \n",
       "3       False  Binder Error: Referenced column \"s_state\" not ...       NaN   \n",
       "4        True                                                NaN       NaN   \n",
       "5       False       Parser Error: syntax error at or near \"FROM\"       NaN   \n",
       "6       False         Parser Error: syntax error at end of input       NaN   \n",
       "7       False  Catalog Error: Table with name cc_call_center_...       NaN   \n",
       "8       False                                                NaN       NaN   \n",
       "9       False  Binder Error: Referenced column \"cc_county\" no...       NaN   \n",
       "\n",
       "   gen_time_sec  exec_time_sec           model_choice  \\\n",
       "0     15.591726       0.001120  qwen_3_4b_text_to_sql   \n",
       "1     14.406594       0.006500  qwen_3_4b_text_to_sql   \n",
       "2     10.953556       0.002932  qwen_3_4b_text_to_sql   \n",
       "3     10.569335       0.000695  qwen_3_4b_text_to_sql   \n",
       "4     11.475305       0.048827  qwen_3_4b_text_to_sql   \n",
       "5      2.734055       0.000275       t5_small_awesome   \n",
       "6      3.076737       0.000239       t5_small_awesome   \n",
       "7      2.733557       0.000778       t5_small_awesome   \n",
       "8      0.532614       0.003034       t5_small_awesome   \n",
       "9      0.371027       0.000637       t5_small_awesome   \n",
       "\n",
       "                                            model_id  \n",
       "0  Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...  \n",
       "1  Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...  \n",
       "2  Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...  \n",
       "3  Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...  \n",
       "4  Qwen/Qwen3-4B-Instruct-2507 + Ellbendls/Qwen-3...  \n",
       "5             cssupport/t5-small-awesome-text-to-sql  \n",
       "6             cssupport/t5-small-awesome-text-to-sql  \n",
       "7             cssupport/t5-small-awesome-text-to-sql  \n",
       "8             cssupport/t5-small-awesome-text-to-sql  \n",
       "9             cssupport/t5-small-awesome-text-to-sql  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"/kaggle/working/Capstone-NLUS-VDD/research_pipeline/benchmark_text_to_sql_all.csv\")\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03075360-0b79-4fb0-9a88-e75c5f0a4560",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
