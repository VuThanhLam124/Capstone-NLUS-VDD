{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3192d16-62ac-4273-aecb-b29dc37abe69",
      "metadata": {},
      "source": [
        "# TPC-DS Text-to-SQL Execution Benchmark\n",
        "\n",
        "This notebook evaluates text-to-SQL models on a single TPC-DS benchmark and reports execution accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdd9d5dc-c515-4acc-a1d4-677b1b76fd62",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-27T07:04:23.877663Z",
          "iopub.status.busy": "2025-12-27T07:04:23.877436Z",
          "iopub.status.idle": "2025-12-27T07:04:24.647503Z",
          "shell.execute_reply": "2025-12-27T07:04:24.646813Z",
          "shell.execute_reply.started": "2025-12-27T07:04:23.877641Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/VuThanhLam124/Capstone-NLUS-VDD.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e9143d7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-27T07:04:24.648857Z",
          "iopub.status.busy": "2025-12-27T07:04:24.648571Z",
          "iopub.status.idle": "2025-12-27T07:04:24.654177Z",
          "shell.execute_reply": "2025-12-27T07:04:24.653480Z",
          "shell.execute_reply.started": "2025-12-27T07:04:24.648824Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "cd Capstone-NLUS-VDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e87f77ff",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-27T07:04:24.655229Z",
          "iopub.status.busy": "2025-12-27T07:04:24.654991Z",
          "iopub.status.idle": "2025-12-27T07:04:42.729396Z",
          "shell.execute_reply": "2025-12-27T07:04:42.728423Z",
          "shell.execute_reply.started": "2025-12-27T07:04:24.655196Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip -q install sqlglot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6cc9070",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-27T07:04:42.732162Z",
          "iopub.status.busy": "2025-12-27T07:04:42.731861Z",
          "iopub.status.idle": "2025-12-27T07:04:54.010283Z",
          "shell.execute_reply": "2025-12-27T07:04:54.009397Z",
          "shell.execute_reply.started": "2025-12-27T07:04:42.732136Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import gc\n",
        "import math\n",
        "from decimal import Decimal\n",
        "from datetime import date, datetime\n",
        "\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    for p in [start] + list(start.parents):\n",
        "        if (p / \"research_pipeline\").exists():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "REPO_ROOT = find_repo_root(Path.cwd())\n",
        "DB_PATH = REPO_ROOT / \"research_pipeline\" / \"data\" / \"ecommerce_dw.duckdb\"\n",
        "PRIMARY_BENCHMARK = REPO_ROOT / \"research_pipeline\" / \"data\" / \"test_queries_vi_200.json\"\n",
        "FALLBACK_BENCHMARK = REPO_ROOT / \"research_pipeline\" / \"test_queries.json\"\n",
        "OUTPUT_DIR = REPO_ROOT / \"research_pipeline\"\n",
        "RUN_ID = None  # set like \"run1\" or time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "MODEL_CHOICES = {\n",
        "    \"qwen_3_4b_text_to_sql\": {\n",
        "        \"type\": \"lora_causal\",\n",
        "        \"adapter_id\": \"Ellbendls/Qwen-3-4b-Text_to_SQL\",\n",
        "        \"base_id\": \"Qwen/Qwen3-4B-Instruct-2507\",\n",
        "        \"tokenizer_id\": \"Ellbendls/Qwen-3-4b-Text_to_SQL\",\n",
        "        \"allow_vocab_shrink\": True,\n",
        "    },\n",
        "}\n",
        "MODEL_ORDER = [\"qwen_3_4b_text_to_sql\"]\n",
        "RUN_ALL_MODELS = False\n",
        "MODEL_CHOICE = \"qwen_3_4b_text_to_sql\"\n",
        "CONTINUE_ON_ERROR = True\n",
        "\n",
        "MAX_SAMPLES = 50  # set None to run full benchmark\n",
        "SAMPLE_SEED = 42\n",
        "DEFAULT_LIMIT = None  # set to an int to force LIMIT on both GT and generated SQL\n",
        "MAX_TABLES = None  # set to an int to shorten schema prompt\n",
        "MAX_NEW_TOKENS = 256\n",
        "NUM_BEAMS = 1\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "USE_4BIT = torch.cuda.is_available()\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "def make_output_path(stem: str) -> Path:\n",
        "    suffix = f\"_{RUN_ID}\" if RUN_ID else \"\"\n",
        "    return OUTPUT_DIR / f\"{stem}{suffix}.csv\"\n",
        "\n",
        "OUTPUT_CSV_ALL = make_output_path(\"benchmark_text_to_sql_all\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45f6cab2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-27T07:04:54.011768Z",
          "iopub.status.busy": "2025-12-27T07:04:54.011260Z",
          "iopub.status.idle": "2025-12-27T07:05:23.866283Z",
          "shell.execute_reply": "2025-12-27T07:05:23.865467Z",
          "shell.execute_reply.started": "2025-12-27T07:04:54.011739Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "AUTO_SETUP_DB = True\n",
        "SETUP_SCALE_FACTOR = 1\n",
        "FORCE_RECREATE_DB = False\n",
        "\n",
        "def setup_tpcds_db(db_path: Path, scale_factor: int = 1, force_recreate: bool = False) -> None:\n",
        "    db_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    con = duckdb.connect(str(db_path))\n",
        "    try:\n",
        "        con.execute(\"INSTALL tpcds;\")\n",
        "        con.execute(\"LOAD tpcds;\")\n",
        "\n",
        "        tables = [r[0] for r in con.execute(\"SHOW TABLES\").fetchall()]\n",
        "        if tables and not force_recreate:\n",
        "            print(f\"Found {len(tables)} tables. Skip generation.\")\n",
        "            return\n",
        "\n",
        "        if force_recreate and tables:\n",
        "            for t in tables:\n",
        "                con.execute(f\"DROP TABLE {t}\")\n",
        "\n",
        "        print(f\"Generating TPC-DS (sf={scale_factor})...\")\n",
        "        start = time.time()\n",
        "        con.execute(f\"CALL dsdgen(sf={scale_factor});\")\n",
        "        print(f\"Data generation completed in {time.time() - start:.2f}s\")\n",
        "    finally:\n",
        "        con.close()\n",
        "\n",
        "if not DB_PATH.exists():\n",
        "    if AUTO_SETUP_DB:\n",
        "        setup_tpcds_db(DB_PATH, scale_factor=SETUP_SCALE_FACTOR, force_recreate=FORCE_RECREATE_DB)\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"TPC-DS DuckDB not found: {DB_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b2f69a3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-27T07:05:23.867999Z",
          "iopub.status.busy": "2025-12-27T07:05:23.867406Z",
          "iopub.status.idle": "2025-12-27T07:05:23.920300Z",
          "shell.execute_reply": "2025-12-27T07:05:23.919543Z",
          "shell.execute_reply.started": "2025-12-27T07:05:23.867962Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "con = duckdb.connect(str(DB_PATH), read_only=True)\n",
        "\n",
        "def duckdb_schema_prompt(con, *, table_schema: str = \"main\", max_tables: int | None = None) -> str:\n",
        "    rows = con.execute(\n",
        "        \"\"\"\n",
        "        SELECT table_name, column_name, data_type, ordinal_position\n",
        "        FROM information_schema.columns\n",
        "        WHERE table_schema = ?\n",
        "        ORDER BY table_name, ordinal_position\n",
        "        \"\"\",\n",
        "        [table_schema],\n",
        "    ).fetchall()\n",
        "\n",
        "    tables: dict[str, list[tuple[str, str]]] = {}\n",
        "    for table_name, column_name, data_type, _ in rows:\n",
        "        tables.setdefault(str(table_name), []).append((str(column_name), str(data_type)))\n",
        "\n",
        "    table_names = sorted(tables.keys())\n",
        "    if max_tables is not None:\n",
        "        table_names = table_names[:max_tables]\n",
        "\n",
        "    lines: list[str] = []\n",
        "    for t in table_names:\n",
        "        lines.append(f\"TABLE {t} (\")\n",
        "        for col, typ in tables[t]:\n",
        "            lines.append(f\"  {col} {typ}\")\n",
        "        lines.append(\")\")\n",
        "        lines.append(\"\")\n",
        "    return \"\".join(lines).strip()\n",
        "\n",
        "schema_text = duckdb_schema_prompt(con, max_tables=MAX_TABLES)\n",
        "print(f\"Schema tables: {schema_text.count('TABLE ')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "542e7eee",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-27T07:05:23.921742Z",
          "iopub.status.busy": "2025-12-27T07:05:23.921316Z",
          "iopub.status.idle": "2025-12-27T07:05:23.928459Z",
          "shell.execute_reply": "2025-12-27T07:05:23.927702Z",
          "shell.execute_reply.started": "2025-12-27T07:05:23.921705Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if PRIMARY_BENCHMARK.exists():\n",
        "    benchmark_path = PRIMARY_BENCHMARK\n",
        "elif FALLBACK_BENCHMARK.exists():\n",
        "    benchmark_path = FALLBACK_BENCHMARK\n",
        "else:\n",
        "    raise FileNotFoundError(\"No benchmark JSON found.\")\n",
        "\n",
        "raw_items = json.loads(benchmark_path.read_text())\n",
        "items = []\n",
        "for item in raw_items:\n",
        "    question = item.get(\"text\") or item.get(\"question\")\n",
        "    sql = item.get(\"sql\")\n",
        "    if not question or not sql:\n",
        "        continue\n",
        "    items.append({\n",
        "        \"id\": item.get(\"id\", f\"q{len(items)+1}\"),\n",
        "        \"text\": question,\n",
        "        \"sql\": sql,\n",
        "    })\n",
        "\n",
        "if MAX_SAMPLES:\n",
        "    random.seed(SAMPLE_SEED)\n",
        "    items = random.sample(items, min(MAX_SAMPLES, len(items)))\n",
        "\n",
        "print(f\"Benchmark items: {len(items)} from {benchmark_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c23e611a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-27T07:05:23.929803Z",
          "iopub.status.busy": "2025-12-27T07:05:23.929423Z",
          "iopub.status.idle": "2025-12-27T07:05:24.361322Z",
          "shell.execute_reply": "2025-12-27T07:05:24.360691Z",
          "shell.execute_reply.started": "2025-12-27T07:05:23.929771Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import sqlglot\n",
        "except Exception:\n",
        "    sqlglot = None\n",
        "    print(\"sqlglot not installed: SQL normalization will be simple.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c590d27e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-27T07:05:24.363088Z",
          "iopub.status.busy": "2025-12-27T07:05:24.362339Z",
          "iopub.status.idle": "2025-12-27T07:05:24.374649Z",
          "shell.execute_reply": "2025-12-27T07:05:24.373890Z",
          "shell.execute_reply.started": "2025-12-27T07:05:24.363049Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_model_and_tokenizer(spec: dict):\n",
        "    model_type = spec[\"type\"]\n",
        "    quant_config = BitsAndBytesConfig(load_in_4bit=True) if USE_4BIT else None\n",
        "\n",
        "    def resolve_tokenizer_and_config(model_id: str, tokenizer_id: str | None = None, allow_shrink: bool = False):\n",
        "        tokenizer_id = tokenizer_id or model_id\n",
        "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_id, use_fast=True, trust_remote_code=True)\n",
        "        config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
        "        if len(tokenizer) != config.vocab_size and (len(tokenizer) > config.vocab_size or allow_shrink):\n",
        "            print(f\"Extending vocab_size for {model_id}: {config.vocab_size} -> {len(tokenizer)}\")\n",
        "            config.vocab_size = len(tokenizer)\n",
        "        return tokenizer, config\n",
        "\n",
        "    if model_type == \"seq2seq\":\n",
        "        tokenizer, config = resolve_tokenizer_and_config(spec[\"id\"], spec.get(\"tokenizer_id\", spec.get(\"allow_vocab_shrink\", False)), spec.get(\"allow_vocab_shrink\", False))\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            spec[\"id\"],\n",
        "            config=config,\n",
        "            device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
        "            dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "        model_kind = \"seq2seq\"\n",
        "        model_id = spec[\"id\"]\n",
        "    elif model_type == \"lora_causal\":\n",
        "        from peft import PeftConfig, PeftModel\n",
        "        adapter_id = spec[\"adapter_id\"]\n",
        "        peft_config = PeftConfig.from_pretrained(adapter_id)\n",
        "        base_id = spec.get(\"base_id\") or peft_config.base_model_name_or_path\n",
        "        tokenizer, config = resolve_tokenizer_and_config(base_id, spec.get(\"tokenizer_id\", spec.get(\"allow_vocab_shrink\", False)), spec.get(\"allow_vocab_shrink\", False))\n",
        "        model_kwargs = dict(\n",
        "            config=config,\n",
        "            device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
        "            dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
        "            trust_remote_code=True,\n",
        "            ignore_mismatched_sizes=True,\n",
        "        )\n",
        "        if quant_config is not None:\n",
        "            model_kwargs[\"quantization_config\"] = quant_config\n",
        "        model = AutoModelForCausalLM.from_pretrained(base_id, **model_kwargs)\n",
        "        model = PeftModel.from_pretrained(model, adapter_id)\n",
        "        model_kind = \"causal\"\n",
        "        model_id = f\"{base_id} + {adapter_id}\"\n",
        "    else:\n",
        "        tokenizer, config = resolve_tokenizer_and_config(spec[\"id\"], spec.get(\"tokenizer_id\", spec.get(\"allow_vocab_shrink\", False)), spec.get(\"allow_vocab_shrink\", False))\n",
        "        model_kwargs = dict(\n",
        "            config=config,\n",
        "            device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
        "            dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
        "            trust_remote_code=True,\n",
        "            ignore_mismatched_sizes=True,\n",
        "        )\n",
        "        if quant_config is not None:\n",
        "            model_kwargs[\"quantization_config\"] = quant_config\n",
        "        model = AutoModelForCausalLM.from_pretrained(spec[\"id\"], **model_kwargs)\n",
        "        model_kind = \"causal\"\n",
        "        model_id = spec[\"id\"]\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.eval()\n",
        "    return tokenizer, model, model_kind, model_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62033330",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-27T07:05:24.376075Z",
          "iopub.status.busy": "2025-12-27T07:05:24.375762Z",
          "iopub.status.idle": "2025-12-27T07:05:24.392687Z",
          "shell.execute_reply": "2025-12-27T07:05:24.391980Z",
          "shell.execute_reply.started": "2025-12-27T07:05:24.376050Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "_FORBIDDEN_SQL = re.compile(\n",
        "    r\"\\b(INSERT|UPDATE|DELETE|DROP|ALTER|CREATE|COPY|PRAGMA|ATTACH|DETACH|EXPORT|IMPORT|CALL)\\b\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You translate user questions into SQL for DuckDB (TPC-DS). \"\n",
        "    \"Return only SQL, no markdown, no explanations. \"\n",
        "    \"Use only tables and columns from the schema.\"\n",
        ")\n",
        "SEQ2SEQ_PROMPT_TEMPLATE = \"translate to SQL:\\n{question}\\n\\nSCHEMA:\\n{schema}\\n\\nSQL:\"\n",
        "\n",
        "def extract_sql(text: str) -> str:\n",
        "    text = text.strip()\n",
        "    m = re.search(r\"```(?:sql)?\\s*(.*?)```\", text, flags=re.IGNORECASE | re.DOTALL)\n",
        "    if m:\n",
        "        text = m.group(1).strip()\n",
        "    if text.lower().startswith(\"sql:\"):\n",
        "        text = text[4:].strip()\n",
        "    if \";\" in text:\n",
        "        text = text.split(\";\", 1)[0].strip()\n",
        "    return text\n",
        "\n",
        "def is_safe_select(sql: str) -> bool:\n",
        "    s = re.sub(r\"--.*?$\", \"\", sql, flags=re.MULTILINE).strip()\n",
        "    if not s:\n",
        "        return False\n",
        "    if _FORBIDDEN_SQL.search(s):\n",
        "        return False\n",
        "    first = re.split(r\"\\s+\", s, maxsplit=1)[0].upper()\n",
        "    return first in {\"SELECT\", \"WITH\"}\n",
        "\n",
        "def ensure_limit(sql: str, limit: int | None) -> str:\n",
        "    if limit is None:\n",
        "        return sql\n",
        "    s = sql.strip().rstrip(\";\").strip()\n",
        "    if re.search(r\"\\bLIMIT\\b\", s, flags=re.IGNORECASE):\n",
        "        return s\n",
        "    return f\"{s}\\nLIMIT {limit}\"\n",
        "\n",
        "def has_order_by(sql: str) -> bool:\n",
        "    return re.search(r\"\\border\\s+by\\b\", sql, flags=re.IGNORECASE) is not None\n",
        "\n",
        "def normalize_sql(sql: str) -> str:\n",
        "    if sqlglot is not None:\n",
        "        try:\n",
        "            return sqlglot.parse_one(sql, read=\"duckdb\").sql(dialect=\"duckdb\", pretty=False)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return re.sub(r\"\\s+\", \" \", sql.strip()).lower()\n",
        "\n",
        "def build_prompt(question: str, schema_text: str, tokenizer, model_kind: str) -> str:\n",
        "    if model_kind == \"seq2seq\":\n",
        "        return SEQ2SEQ_PROMPT_TEMPLATE.format(question=question, schema=schema_text)\n",
        "    user = f\"SCHEMA:\\n{schema_text}\\n\\nQUESTION:\\n{question}\\n\\nSQL:\"\n",
        "    if getattr(tokenizer, \"chat_template\", None):\n",
        "        return tokenizer.apply_chat_template(\n",
        "            [\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": user},\n",
        "            ],\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "        )\n",
        "    return f\"{SYSTEM_PROMPT}\\n\\n{user}\"\n",
        "\n",
        "def generate_sql(question: str, schema_text: str, tokenizer, model, model_kind: str) -> str:\n",
        "    prompt = build_prompt(question, schema_text, tokenizer, model_kind)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "    pad_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            do_sample=False,\n",
        "            num_beams=NUM_BEAMS,\n",
        "            pad_token_id=pad_id,\n",
        "        )\n",
        "    if model_kind == \"seq2seq\":\n",
        "        text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    else:\n",
        "        gen_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
        "        text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
        "    sql = extract_sql(text)\n",
        "    sql = ensure_limit(sql, DEFAULT_LIMIT)\n",
        "    return sql\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7c725df-7a1e-46a6-b2c4-c62e2a4eaac2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-27T07:05:24.394574Z",
          "iopub.status.busy": "2025-12-27T07:05:24.394337Z",
          "iopub.status.idle": "2025-12-27T07:05:24.407808Z",
          "shell.execute_reply": "2025-12-27T07:05:24.407211Z",
          "shell.execute_reply.started": "2025-12-27T07:05:24.394550Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def normalize_value(v):\n",
        "    if isinstance(v, float):\n",
        "        if math.isnan(v):\n",
        "            return \"nan\"\n",
        "        return round(v, 6)\n",
        "    if isinstance(v, Decimal):\n",
        "        return float(round(v, 6))\n",
        "    if isinstance(v, (datetime, date)):\n",
        "        return v.isoformat()\n",
        "    return v\n",
        "\n",
        "def normalize_rows(rows, keep_order: bool):\n",
        "    if rows is None:\n",
        "        return None\n",
        "    norm = [tuple(normalize_value(x) for x in row) for row in rows]\n",
        "    return norm if keep_order else sorted(norm)\n",
        "\n",
        "def run_sql(con, sql: str):\n",
        "    try:\n",
        "        res = con.execute(sql).fetchall()\n",
        "        return res, None\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b175949c-a01c-4c56-8873-c76c2d893e10",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-27T07:05:24.409301Z",
          "iopub.status.busy": "2025-12-27T07:05:24.408837Z",
          "iopub.status.idle": "2025-12-27T07:05:24.500759Z",
          "shell.execute_reply": "2025-12-27T07:05:24.500032Z",
          "shell.execute_reply.started": "2025-12-27T07:05:24.409277Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "gt_cache = {}\n",
        "for item in items:\n",
        "    qid = item[\"id\"]\n",
        "    gt_sql = ensure_limit(item[\"sql\"], DEFAULT_LIMIT)\n",
        "    gt_res, gt_err = run_sql(con, gt_sql)\n",
        "    gt_cache[qid] = {\n",
        "        \"sql\": gt_sql,\n",
        "        \"res\": gt_res,\n",
        "        \"err\": gt_err,\n",
        "        \"has_order\": has_order_by(gt_sql),\n",
        "        \"norm_sorted\": normalize_rows(gt_res, keep_order=False) if gt_err is None else None,\n",
        "        \"norm_ordered\": normalize_rows(gt_res, keep_order=True) if gt_err is None else None,\n",
        "    }\n",
        "print(f\"Ground-truth cached: {len(gt_cache)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99b73cf8-a9f4-4da2-b54f-165025bd645a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-27T07:05:24.503494Z",
          "iopub.status.busy": "2025-12-27T07:05:24.503212Z",
          "iopub.status.idle": "2025-12-27T07:05:24.511454Z",
          "shell.execute_reply": "2025-12-27T07:05:24.510640Z",
          "shell.execute_reply.started": "2025-12-27T07:05:24.503469Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def run_benchmark_for_model(model_choice: str):\n",
        "    spec = MODEL_CHOICES[model_choice]\n",
        "    print(f\"Loading model choice: {model_choice}\")\n",
        "    tokenizer, model, model_kind, model_id = load_model_and_tokenizer(spec)\n",
        "\n",
        "    results = []\n",
        "    for idx, item in enumerate(items, 1):\n",
        "        qid = item[\"id\"]\n",
        "        question = item[\"text\"]\n",
        "        gt = gt_cache[qid]\n",
        "\n",
        "        start = time.time()\n",
        "        gen_sql = generate_sql(question, schema_text, tokenizer, model, model_kind)\n",
        "        gen_time = time.time() - start\n",
        "\n",
        "        valid_sql = is_safe_select(gen_sql)\n",
        "        if valid_sql:\n",
        "            exec_start = time.time()\n",
        "            gen_res, gen_err = run_sql(con, gen_sql)\n",
        "            exec_time = time.time() - exec_start\n",
        "        else:\n",
        "            gen_res, gen_err, exec_time = None, \"INVALID_SQL\", None\n",
        "\n",
        "        exact_match = False\n",
        "        if valid_sql and gen_err is None:\n",
        "            exact_match = normalize_sql(gen_sql) == normalize_sql(gt[\"sql\"])\n",
        "\n",
        "        exec_match = False\n",
        "        if valid_sql and gen_err is None and gt[\"err\"] is None:\n",
        "            keep_order = gt[\"has_order\"] or has_order_by(gen_sql)\n",
        "            gt_norm = gt[\"norm_ordered\"] if keep_order else gt[\"norm_sorted\"]\n",
        "            gen_norm = normalize_rows(gen_res, keep_order=keep_order)\n",
        "            exec_match = gt_norm == gen_norm\n",
        "\n",
        "        results.append({\n",
        "            \"id\": qid,\n",
        "            \"question\": question,\n",
        "            \"gt_sql\": gt[\"sql\"],\n",
        "            \"gen_sql\": gen_sql,\n",
        "            \"valid_sql\": valid_sql,\n",
        "            \"exact_match\": exact_match,\n",
        "            \"exec_match\": exec_match,\n",
        "            \"gen_error\": gen_err,\n",
        "            \"gt_error\": gt[\"err\"],\n",
        "            \"gen_time_sec\": gen_time,\n",
        "            \"exec_time_sec\": exec_time,\n",
        "            \"model_choice\": model_choice,\n",
        "            \"model_id\": model_id,\n",
        "        })\n",
        "\n",
        "        if idx % 10 == 0:\n",
        "            print(f\"Processed {idx}/{len(items)}\")\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    del model\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a66e3c30-8072-4676-9c28-7d65ba260cf0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-27T07:05:24.512454Z",
          "iopub.status.busy": "2025-12-27T07:05:24.512231Z",
          "iopub.status.idle": "2025-12-27T07:08:08.301302Z",
          "shell.execute_reply": "2025-12-27T07:08:08.300556Z",
          "shell.execute_reply.started": "2025-12-27T07:05:24.512433Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model_choices = MODEL_ORDER if RUN_ALL_MODELS else [MODEL_CHOICE]\n",
        "all_results = []\n",
        "summary_rows = []\n",
        "\n",
        "for choice in model_choices:\n",
        "    try:\n",
        "        results_df = run_benchmark_for_model(choice)\n",
        "    except Exception as e:\n",
        "        print(f\"Model {choice} failed: {e}\")\n",
        "        if CONTINUE_ON_ERROR:\n",
        "            continue\n",
        "        raise\n",
        "\n",
        "    out_path = make_output_path(f\"benchmark_text_to_sql_{choice}\")\n",
        "    results_df.to_csv(out_path, index=False)\n",
        "    all_results.append(results_df)\n",
        "\n",
        "    valid_mask = results_df[\"valid_sql\"]\n",
        "    exec_success = results_df[\"gen_error\"].isna()\n",
        "    exec_acc_all = results_df[\"exec_match\"].mean() if not results_df.empty else 0.0\n",
        "    valid_exec_mask = valid_mask & results_df[\"gt_error\"].isna()\n",
        "    exec_acc_valid = results_df.loc[valid_exec_mask, \"exec_match\"].mean() if valid_exec_mask.any() else 0.0\n",
        "    exact_match_rate = results_df.loc[valid_mask, \"exact_match\"].mean() if valid_mask.any() else 0.0\n",
        "\n",
        "    summary_rows.append({\n",
        "        \"model_choice\": choice,\n",
        "        \"model_id\": results_df[\"model_id\"].iloc[0] if not results_df.empty else None,\n",
        "        \"total\": len(results_df),\n",
        "        \"valid_sql_rate\": float(valid_mask.mean()) if not results_df.empty else 0.0,\n",
        "        \"exec_success_rate\": float(exec_success.mean()) if not results_df.empty else 0.0,\n",
        "        \"exec_acc_all\": exec_acc_all,\n",
        "        \"exec_acc_valid\": exec_acc_valid,\n",
        "        \"exact_match_rate\": exact_match_rate,\n",
        "        \"avg_gen_time_sec\": float(results_df[\"gen_time_sec\"].mean()) if not results_df.empty else 0.0,\n",
        "        \"avg_exec_time_sec\": float(results_df[\"exec_time_sec\"].dropna().mean()) if results_df[\"exec_time_sec\"].notna().any() else 0.0,\n",
        "        \"invalid_sql\": int((results_df[\"gen_error\"] == \"INVALID_SQL\").sum()),\n",
        "        \"gen_exec_errors\": int(results_df[\"gen_error\"].notna().sum()),\n",
        "        \"gt_exec_errors\": int(results_df[\"gt_error\"].notna().sum()),\n",
        "        \"output_csv\": str(out_path),\n",
        "    })\n",
        "\n",
        "if not all_results:\n",
        "    raise RuntimeError(\"No model results produced.\")\n",
        "\n",
        "combined_df = pd.concat(all_results, ignore_index=True)\n",
        "combined_df.to_csv(OUTPUT_CSV_ALL, index=False)\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "print(\"Summary\")\n",
        "print(summary_df)\n",
        "print(f\"Combined results saved to: {OUTPUT_CSV_ALL}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6930cc00-ba37-4fd9-93a9-5bf020e7c97b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-27T07:08:53.396067Z",
          "iopub.status.busy": "2025-12-27T07:08:53.395303Z",
          "iopub.status.idle": "2025-12-27T07:08:53.421018Z",
          "shell.execute_reply": "2025-12-27T07:08:53.420417Z",
          "shell.execute_reply.started": "2025-12-27T07:08:53.396037Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"/kaggle/working/Capstone-NLUS-VDD/research_pipeline/benchmark_text_to_sql_all.csv\")\n",
        "data.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03075360-0b79-4fb0-9a88-e75c5f0a4560",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31236,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}