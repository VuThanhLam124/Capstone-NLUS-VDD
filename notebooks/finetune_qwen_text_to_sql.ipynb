{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen Text-to-SQL Fine-tune (QLoRA)\n",
        "\n",
        "This notebook fine-tunes the Qwen adapter `Ellbendls/Qwen-3-4b-Text_to_SQL` on the local TPC-DS dataset `research_pipeline/data/data_finetune.csv`.\n",
        "It uses QLoRA (4-bit) and saves a new adapter locally for evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0b5fd41",
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/VuThanhLam124/Capstone-NLUS-VDD.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e11a299",
      "metadata": {},
      "outputs": [],
      "source": [
        "cd Capstone-NLUS-VDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74e31a00",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: install deps if missing (Kaggle)\n",
        "!pip -q install -U \"transformers>=4.43\" \"peft>=0.10\" \"bitsandbytes>=0.43\" \"accelerate>=0.30\" \"trl>=0.12\" \"datasets>=2.19\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "import unicodedata\n",
        "\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        ")\n",
        "from peft import PeftModel, prepare_model_for_kbit_training\n",
        "\n",
        "try:\n",
        "    from trl import SFTTrainer\n",
        "    from trl import DataCollatorForCompletionOnlyLM\n",
        "except Exception:\n",
        "    SFTTrainer = None\n",
        "    DataCollatorForCompletionOnlyLM = None\n",
        "\n",
        "print(\"torch\", torch.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_repo_root(start: Path) -> Path:\n",
        "    for p in [start] + list(start.parents):\n",
        "        if (p / \"research_pipeline\").exists():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "\n",
        "REPO_ROOT = find_repo_root(Path.cwd())\n",
        "DATA_PATHS = [\n",
        "    REPO_ROOT / \"research_pipeline\" / \"data\" / \"data_finetune.csv\",\n",
        "    REPO_ROOT / \"data\" / \"data_finetune.csv\",\n",
        "]\n",
        "DB_PATH = REPO_ROOT / \"research_pipeline\" / \"data\" / \"ecommerce_dw.duckdb\"\n",
        "OUTPUT_DIR = REPO_ROOT / \"research_pipeline\" / \"qwen_text_to_sql_lora_v1\"\n",
        "\n",
        "SEED = 42\n",
        "MAX_SAMPLES = None  # set to int for quick debug\n",
        "TRAIN_SPLIT = 0.9\n",
        "MAX_TABLES = 8\n",
        "MAX_SEQ_LEN = 768\n",
        "\n",
        "BASE_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
        "ADAPTER_ID = \"Ellbendls/Qwen-3-4b-Text_to_SQL\"\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "GRAD_ACCUM = 8\n",
        "NUM_EPOCHS = 2\n",
        "LEARNING_RATE = 2e-4\n",
        "WARMUP_RATIO = 0.05\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "\n",
        "def resolve_data_path(paths):\n",
        "    for p in paths:\n",
        "        if p.exists():\n",
        "            return p\n",
        "    raise FileNotFoundError(f\"No data_finetune.csv found in: {paths}\")\n",
        "\n",
        "\n",
        "DATA_PATH = resolve_data_path(DATA_PATHS)\n",
        "print(\"Using data:\", DATA_PATH)\n",
        "print(\"DB path:\", DB_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(DATA_PATH)\n",
        "df = df.dropna(subset=[\"Transcription\", \"SQL Ground Truth\"]).copy()\n",
        "df[\"Transcription\"] = df[\"Transcription\"].astype(str).str.strip()\n",
        "df[\"SQL Ground Truth\"] = df[\"SQL Ground Truth\"].astype(str).str.strip()\n",
        "\n",
        "# Ensure SQL ends with a semicolon for consistency\n",
        "def normalize_sql(sql: str) -> str:\n",
        "    sql = sql.strip()\n",
        "    if not sql.endswith(\";\"):\n",
        "        sql = sql + \";\"\n",
        "    return sql\n",
        "\n",
        "\n",
        "df[\"SQL Ground Truth\"] = df[\"SQL Ground Truth\"].map(normalize_sql)\n",
        "\n",
        "# Optional: drop duplicate SQLs to reduce overfitting\n",
        "df = df.drop_duplicates(subset=[\"SQL Ground Truth\"]).reset_index(drop=True)\n",
        "\n",
        "if MAX_SAMPLES is not None:\n",
        "    df = df.sample(n=min(MAX_SAMPLES, len(df)), random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "print(\"Rows:\", len(df))\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "con = duckdb.connect(str(DB_PATH), read_only=True)\n",
        "schema_map = {}\n",
        "for (table_name,) in con.execute(\"SHOW TABLES\").fetchall():\n",
        "    columns = [r[0] for r in con.execute(f\"DESCRIBE {table_name}\").fetchall()]\n",
        "    schema_map[table_name] = columns\n",
        "con.close()\n",
        "\n",
        "\n",
        "def strip_accents(text: str) -> str:\n",
        "    return \"\".join(ch for ch in unicodedata.normalize(\"NFD\", text) if unicodedata.category(ch) != \"Mn\")\n",
        "\n",
        "\n",
        "def tokenize(text: str) -> list[str]:\n",
        "    text = strip_accents(text.lower())\n",
        "    raw_tokens = re.findall(r\"[a-z0-9_]+\", text)\n",
        "    tokens = []\n",
        "    for tok in raw_tokens:\n",
        "        tokens.extend(tok.split(\"_\"))\n",
        "    return [t for t in tokens if len(t) > 1]\n",
        "\n",
        "\n",
        "SYNONYMS = {\n",
        "    \"khach\": \"customer\",\n",
        "    \"khachhang\": \"customer\",\n",
        "    \"sanpham\": \"item\",\n",
        "    \"hang\": \"item\",\n",
        "    \"danhmuc\": \"category\",\n",
        "    \"bang\": \"state\",\n",
        "    \"tinh\": \"state\",\n",
        "    \"cuahang\": \"store\",\n",
        "    \"doanhthu\": \"revenue\",\n",
        "    \"soluong\": \"quantity\",\n",
        "    \"gia\": \"price\",\n",
        "    \"thang\": \"month\",\n",
        "    \"nam\": \"year\",\n",
        "    \"quy\": \"quarter\",\n",
        "}\n",
        "\n",
        "\n",
        "def expand_tokens(tokens: list[str]) -> set[str]:\n",
        "    expanded = set(tokens)\n",
        "    for tok in list(tokens):\n",
        "        mapped = SYNONYMS.get(tok)\n",
        "        if mapped:\n",
        "            expanded.add(mapped)\n",
        "    return expanded\n",
        "\n",
        "\n",
        "table_tokens = {}\n",
        "for table, cols in schema_map.items():\n",
        "    tokens = set(tokenize(table))\n",
        "    for col in cols:\n",
        "        tokens.update(tokenize(col))\n",
        "    table_tokens[table] = tokens\n",
        "\n",
        "\n",
        "def select_tables_for_question(question: str, max_tables: int = 8) -> list[str]:\n",
        "    q_tokens = expand_tokens(tokenize(question))\n",
        "    scored = []\n",
        "    for table, tokens in table_tokens.items():\n",
        "        score = len(q_tokens & tokens)\n",
        "        scored.append((score, table))\n",
        "    scored.sort(reverse=True)\n",
        "\n",
        "    selected = [t for score, t in scored if score > 0][:max_tables]\n",
        "\n",
        "    def ensure(table: str):\n",
        "        if table in schema_map and table not in selected:\n",
        "            selected.append(table)\n",
        "\n",
        "    if any(tok in q_tokens for tok in {\"year\", \"month\", \"quarter\", \"date\"}):\n",
        "        ensure(\"date_dim\")\n",
        "    if any(tok in q_tokens for tok in {\"customer\"}):\n",
        "        ensure(\"customer\")\n",
        "        ensure(\"customer_address\")\n",
        "    if \"state\" in q_tokens:\n",
        "        ensure(\"customer_address\")\n",
        "        ensure(\"store\")\n",
        "    if \"store\" in q_tokens:\n",
        "        ensure(\"store_sales\")\n",
        "        ensure(\"store\")\n",
        "    if \"web\" in q_tokens:\n",
        "        ensure(\"web_sales\")\n",
        "        ensure(\"web_site\")\n",
        "    if \"catalog\" in q_tokens:\n",
        "        ensure(\"catalog_sales\")\n",
        "        ensure(\"call_center\")\n",
        "    if \"call\" in q_tokens:\n",
        "        ensure(\"call_center\")\n",
        "    if \"inventory\" in q_tokens:\n",
        "        ensure(\"inventory\")\n",
        "    if any(tok in q_tokens for tok in {\"item\", \"product\", \"category\"}):\n",
        "        ensure(\"item\")\n",
        "    if any(tok in q_tokens for tok in {\"sales\", \"revenue\", \"quantity\", \"price\"}):\n",
        "        ensure(\"store_sales\")\n",
        "\n",
        "    return selected[: max_tables or len(selected)]\n",
        "\n",
        "\n",
        "def build_schema_snippet(question: str, max_tables: int = 8) -> str:\n",
        "    tables = select_tables_for_question(question, max_tables=max_tables)\n",
        "    if not tables:\n",
        "        tables = list(schema_map.keys())\n",
        "\n",
        "    lines = []\n",
        "    for table in tables:\n",
        "        cols = schema_map[table]\n",
        "        lines.append(f\"TABLE {table} (\")\n",
        "        for col in cols:\n",
        "            lines.append(f\"  {col}\")\n",
        "        lines.append(\")\")\n",
        "        lines.append(\"\")\n",
        "    return \"\".join(lines).strip()\n",
        "\n",
        "\n",
        "print(f\"Loaded schema for {len(schema_map)} tables\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = (\n",
        "    \"You translate user questions into SQL for DuckDB (TPC-DS). \"\n",
        "    \"Return only SQL, no markdown, no explanations. \"\n",
        "    \"Use only tables and columns from the schema.\"\n",
        ")\n",
        "\n",
        "\n",
        "def format_record(record, tokenizer) -> str:\n",
        "    question = record[\"Transcription\"]\n",
        "    sql = record[\"SQL Ground Truth\"]\n",
        "    schema_text = build_schema_snippet(question, max_tables=MAX_TABLES)\n",
        "    user = f\"\"\"SCHEMA:\n",
        "{schema_text}\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\n",
        "SQL:\"\"\"\n",
        "\n",
        "    if getattr(tokenizer, \"chat_template\", None):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": user},\n",
        "            {\"role\": \"assistant\", \"content\": sql},\n",
        "        ]\n",
        "        return tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "        )\n",
        "\n",
        "    return f\"\"\"{SYSTEM_PROMPT}\n",
        "\n",
        "{user} {sql}\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_ID, use_fast=True, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "def map_fn(record):\n",
        "    return {\"text\": format_record(record, tokenizer)}\n",
        "\n",
        "dataset = dataset.map(map_fn, remove_columns=dataset.column_names)\n",
        "dataset = dataset.shuffle(seed=SEED)\n",
        "\n",
        "train_size = int(len(dataset) * TRAIN_SPLIT)\n",
        "train_dataset = dataset.select(range(train_size))\n",
        "eval_dataset = dataset.select(range(train_size, len(dataset)))\n",
        "\n",
        "print(\"Train size:\", len(train_dataset))\n",
        "print(\"Eval size:\", len(eval_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "config = AutoConfig.from_pretrained(BASE_ID, trust_remote_code=True)\n",
        "if len(tokenizer) != config.vocab_size:\n",
        "    print(f\"Adjust vocab_size {config.vocab_size} -> {len(tokenizer)}\")\n",
        "    config.vocab_size = len(tokenizer)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_ID,\n",
        "    config=config,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    ignore_mismatched_sizes=True,\n",
        ")\n",
        "\n",
        "if len(tokenizer) != base_model.get_input_embeddings().weight.shape[0]:\n",
        "    base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "base_model = prepare_model_for_kbit_training(base_model)\n",
        "model = PeftModel.from_pretrained(base_model, ADAPTER_ID, is_trainable=True)\n",
        "\n",
        "model.print_trainable_parameters()\n",
        "model.config.use_cache = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_dir = str(OUTPUT_DIR)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    fp16=True,\n",
        "    logging_steps=20,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=False,\n",
        "    report_to=\"none\",\n",
        "    optim=\"paged_adamw_8bit\",\n",
        ")\n",
        "\n",
        "response_template = \"<|im_start|>assistant\\n\" if getattr(tokenizer, \"chat_template\", None) else \"SQL:\"\n",
        "\n",
        "data_collator = None\n",
        "if DataCollatorForCompletionOnlyLM is not None:\n",
        "    data_collator = DataCollatorForCompletionOnlyLM(\n",
        "        response_template=response_template,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "trainer_cls = SFTTrainer if SFTTrainer is not None else None\n",
        "\n",
        "if trainer_cls is None:\n",
        "    raise RuntimeError(\"TRL is required for this notebook. Install trl and restart kernel.\")\n",
        "\n",
        "trainer = trainer_cls(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LEN,\n",
        "    packing=False,\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_result = trainer.train()\n",
        "print(train_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "trainer.model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Saved adapter to\", OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "- Use the saved adapter in `notebooks/text_to_sql_tpcds_exec_benchmark.ipynb` by setting `adapter_id` to the output dir.\n",
        "- Re-run the benchmark on `test_queries_vi_200_v2.json` to compare execution accuracy.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "thesis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
